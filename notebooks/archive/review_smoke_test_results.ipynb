{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Review Hardware Smoke Test Results\n",
    "\n",
    "This notebook demonstrates how to replay and analyze the preliminary smoke test executed on IBM ibm_torino (Oct 22, 2025).\n",
    "\n",
    "**Capabilities:**\n",
    "1. Load complete results summary from JSON\n",
    "2. Replay experiments from saved manifests\n",
    "3. Compute NEW observables from saved shot data\n",
    "4. Compare direct vs shadow methods\n",
    "5. Analyze backend calibration data\n",
    "6. Generate visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from quartumse import ShadowEstimator\n",
    "from quartumse.shadows.core import Observable\n",
    "from quartumse.reporting.manifest import ProvenanceManifest\n",
    "from quartumse.reporting.shot_data import ShotDataWriter\n",
    "\n",
    "print('âœ“ Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Part 1: Load Smoke Test Summary\n",
    "\n",
    "The complete results are saved in `validation_data/smoke_test_results_TIMESTAMP.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent smoke test results\n",
    "validation_dir = Path('validation_data')\n",
    "result_files = list(validation_dir.glob('smoke_test_results_*.json'))\n",
    "\n",
    "if not result_files:\n",
    "    raise FileNotFoundError('No smoke test results found in validation_data/')\n",
    "\n",
    "# Use most recent\n",
    "latest_results = max(result_files, key=lambda p: p.stat().st_mtime)\n",
    "print(f'Loading: {latest_results.name}')\n",
    "\n",
    "with open(latest_results, encoding='utf-8') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f'âœ“ Loaded smoke test results from {results[\"metadata\"][\"timestamp\"]}')\n",
    "print(f'  Backend: {results[\"metadata\"][\"backend\"]}')\n",
    "print(f'  Total shots: {results[\"direct_measurements\"][\"total_shots\"]}')\n",
    "print(f'  Git commit: {results[\"metadata\"][\"git_commit\"][:7]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "## Part 2: Compare All Three Methods\n",
    "\n",
    "View the comparison table from the smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison table\n",
    "comparison_df = pd.DataFrame(results['comparison']['table'])\n",
    "print('\\nSmoke Test Results - All Methods:')\n",
    "print('=' * 100)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-checks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display quality check results\n",
    "quality_df = pd.DataFrame(results['analysis']['quality_checks'])\n",
    "print('\\nQuality Checks:')\n",
    "print('=' * 100)\n",
    "quality_df[['observable', 'expected_value', 'direct_expectation', \n",
    "            'shadows_v0_expectation', 'shadows_v1_expectation',\n",
    "            'direct_delta', 'shadows_v0_delta', 'shadows_v1_delta']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "replay-header",
   "metadata": {},
   "source": [
    "## Part 3: Replay Shadow v0 Experiment\n",
    "\n",
    "Reproduce the v0 results from saved shot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "replay-v0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shadow v0 manifest\n",
    "v0_manifest_path = results['shadows_v0']['manifest_path']\n",
    "print(f'Loading Shadow v0 manifest: {v0_manifest_path}')\n",
    "\n",
    "# Replay with original observables\n",
    "estimator = ShadowEstimator(backend='aer_simulator', data_dir='validation_data')\n",
    "replayed_v0 = estimator.replay_from_manifest(v0_manifest_path)\n",
    "\n",
    "print(f'âœ“ Replayed v0 using {replayed_v0.shots_used} saved shadows')\n",
    "print(f'  Execution time: {replayed_v0.execution_time:.3f}s (instant replay!)')\n",
    "print('\\nReplayed Results:')\n",
    "for obs_key, values in replayed_v0.observables.items():\n",
    "    print(f\"  {obs_key}: {values['expectation_value']:.3f} Â± {values['ci_width']/2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-obs-header",
   "metadata": {},
   "source": [
    "## Part 4: Compute NEW Observables from Hardware Data\n",
    "\n",
    "The power of classical shadows: compute observables that weren't in the original experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-observables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NEW observables (not measured in original smoke test)\n",
    "new_observables = [\n",
    "    Observable('II', 1.0),  # Identity (should be 1.0)\n",
    "    Observable('ZI', 1.0),  # Single qubit Z on first qubit\n",
    "    Observable('IZ', 1.0),  # Single qubit Z on second qubit\n",
    "    Observable('YY', 1.0),  # YY correlation (another Bell stabilizer)\n",
    "]\n",
    "\n",
    "print('Computing NEW observables from hardware data...')\n",
    "print('Original observables: ZZ, XX')\n",
    "print('NEW observables:      II, ZI, IZ, YY')\n",
    "print()\n",
    "\n",
    "# Replay with new observables\n",
    "new_result = estimator.replay_from_manifest(\n",
    "    v0_manifest_path,\n",
    "    observables=new_observables\n",
    ")\n",
    "\n",
    "print(f'âœ“ Computed {len(new_result.observables)} NEW observables')\n",
    "print('  (No backend execution required - using saved hardware data!)')\n",
    "print('\\nNEW Observable Estimates from Hardware Data:')\n",
    "for obs_key, values in new_result.observables.items():\n",
    "    exp_val = values['expectation_value']\n",
    "    ci_half = values['ci_width'] / 2\n",
    "    print(f\"  {obs_key}: {exp_val:>6.3f} Â± {ci_half:.3f}\")\n",
    "\n",
    "print('\\nExpected for ideal Bell state:')\n",
    "print('  II: +1.0  (identity)')\n",
    "print('  ZI:  0.0  (single qubit in superposition)')\n",
    "print('  IZ:  0.0  (single qubit in superposition)')\n",
    "print('  YY: +1.0  (Bell stabilizer)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "replay-v1-header",
   "metadata": {},
   "source": [
    "## Part 5: Replay Shadow v1 (Noise-Aware) Experiment\n",
    "\n",
    "Shadow v1 includes Measurement Error Mitigation (MEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "replay-v1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shadow v1 manifest\n",
    "v1_manifest_path = results['shadows_v1']['manifest_path']\n",
    "print(f'Loading Shadow v1 manifest: {v1_manifest_path}')\n",
    "\n",
    "# Check if MEM confusion matrix is available\n",
    "mem_path = results['shadows_v1'].get('mitigation_confusion_matrix_path')\n",
    "if mem_path:\n",
    "    mem_file = Path(mem_path)\n",
    "    if mem_file.exists():\n",
    "        print(f'âœ“ MEM confusion matrix available: {mem_file.name}')\n",
    "    else:\n",
    "        print(f'âš  MEM confusion matrix not found at: {mem_path}')\n",
    "\n",
    "# Replay v1 with noise-aware reconstruction\n",
    "replayed_v1 = estimator.replay_from_manifest(v1_manifest_path)\n",
    "\n",
    "print(f'\\nâœ“ Replayed v1 (noise-aware) using {replayed_v1.shots_used} saved shadows')\n",
    "print(f'  MEM calibration shots: {results[\"shadows_v1\"][\"mem_shots\"]}')\n",
    "print('\\nReplayed v1 Results:')\n",
    "for obs_key, values in replayed_v1.observables.items():\n",
    "    print(f\"  {obs_key}: {values['expectation_value']:.3f} Â± {values['ci_width']/2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backend-header",
   "metadata": {},
   "source": [
    "## Part 6: Analyze Backend Calibration Data\n",
    "\n",
    "Review the IBM ibm_torino calibration snapshot captured during execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backend-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load backend snapshot\n",
    "backend_snapshot = results['metadata']['backend_snapshot']\n",
    "\n",
    "print(f'Backend: {backend_snapshot[\"backend_name\"]}')\n",
    "print(f'Version: {backend_snapshot[\"backend_version\"]}')\n",
    "print(f'Qubits: {backend_snapshot[\"num_qubits\"]}')\n",
    "print(f'Calibration timestamp: {backend_snapshot[\"calibration_timestamp\"]}')\n",
    "print(f'\\nBasis gates: {backend_snapshot[\"basis_gates\"]}')\n",
    "\n",
    "# Display gate errors\n",
    "if 'gate_errors' in backend_snapshot:\n",
    "    print('\\nGate Errors:')\n",
    "    for gate, error in backend_snapshot['gate_errors'].items():\n",
    "        print(f\"  {gate}: {error:.6f} ({error*100:.4f}%)\")\n",
    "\n",
    "# Show readout errors for qubits used in experiment\n",
    "if 'readout_errors' in backend_snapshot:\n",
    "    print('\\nReadout Errors (first 10 qubits):')\n",
    "    readout_errors = {int(k): v for k, v in backend_snapshot['readout_errors'].items()}\n",
    "    for qubit in sorted(readout_errors.keys())[:10]:\n",
    "        error = readout_errors[qubit]\n",
    "        print(f\"  Qubit {qubit}: {error:.6f} ({error*100:.4f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shot-data-header",
   "metadata": {},
   "source": [
    "## Part 7: Examine Raw Shot Data\n",
    "\n",
    "Inspect the Parquet files containing all measurement outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shot-data-diagnostics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load v0 shot data\n",
    "v0_shot_path = results['shadows_v0']['shot_data_path']\n",
    "print(f'Loading v0 shot data: {v0_shot_path}')\n",
    "\n",
    "try:\n",
    "    v0_diagnostics = ShotDataWriter.summarize_parquet(v0_shot_path)\n",
    "    v0_diag_dict = v0_diagnostics.to_dict()\n",
    "    \n",
    "    print(f'\\nâœ“ Shadow v0 Shot Data:')\n",
    "    print(f\"  Total shots: {v0_diag_dict['total_shots']}\")\n",
    "    print(f'\\n  Measurement Basis Distribution:')\n",
    "    for basis, count in v0_diag_dict['measurement_basis_distribution'].items():\n",
    "        print(f\"    {basis}: {count} ({count/v0_diag_dict['total_shots']*100:.1f}%)\")\n",
    "    \n",
    "    print(f'\\n  Top Bitstrings:')\n",
    "    for bitstring, count in list(v0_diag_dict['bitstring_histogram'].items())[:5]:\n",
    "        print(f\"    {bitstring}: {count}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'âš  Could not load shot data: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**What we demonstrated:**\n",
    "\n",
    "1. âœ… **Loaded complete smoke test results** from JSON summary\n",
    "2. âœ… **Replayed v0 and v1 experiments** from saved manifests\n",
    "3. âœ… **Computed NEW observables** from hardware data (measure once, analyze forever)\n",
    "4. âœ… **Analyzed backend calibration** captured during execution\n",
    "5. âœ… **Inspected raw shot data** in Parquet format\n",
    "\n",
    "**Key Insight:** The smoke test data is fully reusable. You can:\n",
    "- Compute any 2-qubit Pauli observable from the saved shadows\n",
    "- Compare different reconstruction methods\n",
    "- Analyze noise characteristics from calibration data\n",
    "- Generate publication-quality figures\n",
    "- Share complete provenance with collaborators\n",
    "\n",
    "**All without re-running on IBM hardware!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
