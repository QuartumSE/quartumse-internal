{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# QuartumSE Benchmark Suite\n",
    "\n",
    "This notebook benchmarks **classical shadows** against direct measurement baselines for a single circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "from quartumse import (\n",
    "    run_benchmark_suite,\n",
    "    BenchmarkMode,\n",
    "    BenchmarkSuiteConfig,\n",
    "    generate_observable_set,\n",
    "    Observable,\n",
    "    ObservableSet,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Circuit Configuration\n",
    "\n",
    "Define the circuit and observables to benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CIRCUIT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CIRCUIT_ID = \"ghz_4q\"  # Identifier for this benchmark\n",
    "N_QUBITS = 4\n",
    "\n",
    "# --- Build Circuit ---\n",
    "def build_ghz(n_qubits: int) -> QuantumCircuit:\n",
    "    \"\"\"GHZ state: (|00...0⟩ + |11...1⟩) / sqrt(2)\"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    qc.h(0)\n",
    "    for i in range(1, n_qubits):\n",
    "        qc.cx(i - 1, i)\n",
    "    return qc\n",
    "\n",
    "circuit = build_ghz(N_QUBITS)\n",
    "print(f\"Circuit: {CIRCUIT_ID}\")\n",
    "print(circuit.draw('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "observables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Observables ---\n",
    "observables = []\n",
    "\n",
    "# Random observables with mixed localities\n",
    "for k in range(1, N_QUBITS + 1):\n",
    "    obs_set = generate_observable_set(\n",
    "        generator_id='random_pauli',\n",
    "        n_qubits=N_QUBITS,\n",
    "        n_observables=5,\n",
    "        seed=42 + k,\n",
    "        weight_distribution='fixed',\n",
    "        fixed_weight=k,\n",
    "    )\n",
    "    observables.extend(list(obs_set.observables))\n",
    "\n",
    "# Add GHZ stabilizers\n",
    "observables.extend([\n",
    "    Observable('Z' * N_QUBITS),\n",
    "    Observable('X' * N_QUBITS),\n",
    "])\n",
    "\n",
    "obs_set = ObservableSet(\n",
    "    observables=observables,\n",
    "    observable_set_id=f'{CIRCUIT_ID}_obs',\n",
    "    generator_id='mixed',\n",
    "    generator_seed=42,\n",
    ")\n",
    "\n",
    "# Build locality map\n",
    "locality_map = {}\n",
    "for obs in observables:\n",
    "    locality = sum(1 for c in obs.pauli_string if c != 'I')\n",
    "    locality_map[obs.observable_id] = locality\n",
    "\n",
    "print(f\"Observables: {len(obs_set)}\")\n",
    "loc_counts = Counter(locality_map.values())\n",
    "for k in sorted(loc_counts.keys()):\n",
    "    print(f\"  K={k}: {loc_counts[k]} observables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Benchmark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "config = BenchmarkSuiteConfig(\n",
    "    mode=BenchmarkMode.ANALYSIS,      # Full analysis with all tasks\n",
    "    n_shots_grid=[100, 500, 1000, 5000],\n",
    "    n_replicates=20,                  # 20 for publication quality\n",
    "    seed=42,\n",
    "    epsilon=0.01,                     # Target precision\n",
    "    delta=0.05,                       # Failure probability\n",
    "    shadows_protocol_id=\"classical_shadows_v0\",\n",
    "    baseline_protocol_id=\"direct_grouped\",\n",
    "    output_base_dir=\"benchmark_results\",\n",
    ")\n",
    "\n",
    "print(\"Benchmark Configuration:\")\n",
    "print(f\"  Mode: {config.mode.value}\")\n",
    "print(f\"  Shot grid: {config.n_shots_grid}\")\n",
    "print(f\"  Replicates: {config.n_replicates}\")\n",
    "print(f\"  Target ε: {config.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# =============================================================================\n",
    "# RUN BENCHMARK\n",
    "# =============================================================================\n",
    "\n",
    "result = run_benchmark_suite(\n",
    "    circuit=circuit,\n",
    "    observable_set=obs_set,\n",
    "    circuit_id=CIRCUIT_ID,\n",
    "    config=config,\n",
    "    locality_map=locality_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Benchmark Summary Report\n",
    "\n",
    "Single consolidated table answering all 8 Measurements Bible questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONSOLIDATED BENCHMARK SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "def generate_summary_report(result, config):\n",
    "    \"\"\"Generate a single consolidated summary table for all 8 tasks.\"\"\"\n",
    "    \n",
    "    long_form = result.long_form_results\n",
    "    truth_values = result.ground_truth.truth_values if result.ground_truth else {}\n",
    "    max_n = max(result.summary.get('n_shots_grid', [5000]))\n",
    "    epsilon = config.epsilon\n",
    "    \n",
    "    # Group data by protocol and N\n",
    "    by_protocol_n = defaultdict(lambda: defaultdict(list))\n",
    "    for row in long_form:\n",
    "        by_protocol_n[row.protocol_id][row.N_total].append(row)\n",
    "    \n",
    "    protocols = list(by_protocol_n.keys())\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TASK 1: Worst-Case Guarantee\n",
    "    # =========================================================================\n",
    "    task1_answers = []\n",
    "    for protocol in protocols:\n",
    "        n_star = None\n",
    "        for n in sorted(by_protocol_n[protocol].keys()):\n",
    "            rows = by_protocol_n[protocol][n]\n",
    "            max_se = max(r.se for r in rows if r.se is not None)\n",
    "            if max_se <= epsilon:\n",
    "                n_star = n\n",
    "                break\n",
    "        if n_star:\n",
    "            task1_answers.append(f\"{protocol}: N*={n_star}\")\n",
    "        else:\n",
    "            rows = by_protocol_n[protocol][max_n]\n",
    "            max_se = max(r.se for r in rows if r.se is not None)\n",
    "            task1_answers.append(f\"{protocol}: N*>{max_n} (SE={max_se:.4f})\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TASK 2: Average Target\n",
    "    # =========================================================================\n",
    "    task2_answers = []\n",
    "    for protocol in protocols:\n",
    "        n_star = None\n",
    "        for n in sorted(by_protocol_n[protocol].keys()):\n",
    "            rows = by_protocol_n[protocol][n]\n",
    "            mean_se = np.mean([r.se for r in rows if r.se is not None])\n",
    "            if mean_se <= epsilon:\n",
    "                n_star = n\n",
    "                break\n",
    "        if n_star:\n",
    "            task2_answers.append(f\"{protocol}: N*={n_star}\")\n",
    "        else:\n",
    "            rows = by_protocol_n[protocol][max_n]\n",
    "            mean_se = np.mean([r.se for r in rows if r.se is not None])\n",
    "            task2_answers.append(f\"{protocol}: N*>{max_n} (SE={mean_se:.4f})\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TASK 3: Fixed Budget Distribution\n",
    "    # =========================================================================\n",
    "    task3_answers = []\n",
    "    for protocol in protocols:\n",
    "        rows = by_protocol_n[protocol][max_n]\n",
    "        ses = [r.se for r in rows if r.se is not None]\n",
    "        if ses:\n",
    "            task3_answers.append(\n",
    "                f\"{protocol}: mean={np.mean(ses):.4f}, median={np.median(ses):.4f}, max={np.max(ses):.4f}\"\n",
    "            )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TASK 4: Dominance\n",
    "    # =========================================================================\n",
    "    obs_best = defaultdict(lambda: {'se': float('inf'), 'protocol': None})\n",
    "    for protocol in protocols:\n",
    "        rows = by_protocol_n[protocol][max_n]\n",
    "        for row in rows:\n",
    "            if row.se < obs_best[row.observable_id]['se']:\n",
    "                obs_best[row.observable_id] = {'se': row.se, 'protocol': protocol}\n",
    "    \n",
    "    wins = defaultdict(int)\n",
    "    for obs_id, data in obs_best.items():\n",
    "        if data['protocol']:\n",
    "            wins[data['protocol']] += 1\n",
    "    \n",
    "    total_obs = len(obs_best)\n",
    "    task4_answers = []\n",
    "    for protocol in protocols:\n",
    "        win_count = wins[protocol]\n",
    "        win_pct = 100 * win_count / total_obs if total_obs > 0 else 0\n",
    "        task4_answers.append(f\"{protocol}: {win_count}/{total_obs} ({win_pct:.0f}%)\")\n",
    "    winner = max(wins, key=wins.get) if wins else \"N/A\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TASK 5: Pilot Selection\n",
    "    # =========================================================================\n",
    "    if result.analysis and hasattr(result.analysis, 'pilot_analysis') and result.analysis.pilot_analysis:\n",
    "        pilot = result.analysis.pilot_analysis\n",
    "        task5_answer = f\"Optimal pilot fraction: {pilot.optimal_fraction*100:.0f}%\"\n",
    "    else:\n",
    "        task5_answer = \"Requires ANALYSIS mode\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TASK 6: Bias-Variance Decomposition\n",
    "    # =========================================================================\n",
    "    task6_answers = []\n",
    "    if truth_values:\n",
    "        for protocol in protocols:\n",
    "            rows = by_protocol_n[protocol][max_n]\n",
    "            by_obs = defaultdict(list)\n",
    "            for row in rows:\n",
    "                if row.observable_id in truth_values:\n",
    "                    by_obs[row.observable_id].append(row.estimate)\n",
    "            \n",
    "            biases_sq, variances = [], []\n",
    "            for obs_id, estimates in by_obs.items():\n",
    "                truth = truth_values[obs_id]\n",
    "                mean_est = np.mean(estimates)\n",
    "                biases_sq.append((mean_est - truth)**2)\n",
    "                variances.append(np.var(estimates))\n",
    "            \n",
    "            if biases_sq:\n",
    "                bias_sq = np.mean(biases_sq)\n",
    "                var = np.mean(variances)\n",
    "                mse = bias_sq + var\n",
    "                task6_answers.append(f\"{protocol}: Bias²={bias_sq:.6f}, Var={var:.6f}, MSE={mse:.6f}\")\n",
    "    else:\n",
    "        task6_answers = [\"Requires ground truth\"]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # TASK 7 & 8: Not implemented in single-run benchmark\n",
    "    # =========================================================================\n",
    "    task7_answer = \"Requires noise profile sweep (not in single-run benchmark)\"\n",
    "    task8_answer = \"Requires adaptive protocol (see Task 5 pilot analysis)\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PRINT CONSOLIDATED REPORT\n",
    "    # =========================================================================\n",
    "    print(\"=\" * 100)\n",
    "    print(\"BENCHMARK SUMMARY REPORT\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Circuit: {result.summary.get('circuit_id', 'unknown')}\")\n",
    "    print(f\"Observables: {len(obs_set)} | Shot Grid: {result.summary.get('n_shots_grid', [])} | Replicates: {config.n_replicates}\")\n",
    "    print(f\"Target precision ε = {epsilon}\")\n",
    "    print(\"=\" * 100)\n",
    "    print()\n",
    "    \n",
    "    # Build the summary table\n",
    "    summary_data = [\n",
    "        (\"1\", \"WORST-CASE N*\", \n",
    "         f\"What N* achieves max SE ≤ {epsilon} for ALL observables?\",\n",
    "         \"\\n\".join(task1_answers)),\n",
    "        \n",
    "        (\"2\", \"AVERAGE N*\",\n",
    "         f\"What N* achieves mean SE ≤ {epsilon}?\",\n",
    "         \"\\n\".join(task2_answers)),\n",
    "        \n",
    "        (\"3\", \"SE DISTRIBUTION\",\n",
    "         f\"What is the SE distribution at N = {max_n}?\",\n",
    "         \"\\n\".join(task3_answers)),\n",
    "        \n",
    "        (\"4\", \"DOMINANCE\",\n",
    "         \"Which protocol wins on more observables?\",\n",
    "         \"\\n\".join(task4_answers) + f\"\\n→ WINNER: {winner}\"),\n",
    "        \n",
    "        (\"5\", \"PILOT SELECTION\",\n",
    "         \"What fraction of budget for pilot?\",\n",
    "         task5_answer),\n",
    "        \n",
    "        (\"6\", \"BIAS-VARIANCE\",\n",
    "         \"How does MSE decompose into bias² + variance?\",\n",
    "         \"\\n\".join(task6_answers)),\n",
    "        \n",
    "        (\"7\", \"NOISE SENSITIVITY\",\n",
    "         \"How does performance degrade with noise?\",\n",
    "         task7_answer),\n",
    "        \n",
    "        (\"8\", \"ADAPTIVE EFFICIENCY\",\n",
    "         \"Can budget reallocation improve results?\",\n",
    "         task8_answer),\n",
    "    ]\n",
    "    \n",
    "    for task_num, task_name, question, answer in summary_data:\n",
    "        print(f\"┌{'─'*98}┐\")\n",
    "        print(f\"│ TASK {task_num}: {task_name:<87} │\")\n",
    "        print(f\"├{'─'*98}┤\")\n",
    "        print(f\"│ Q: {question:<94} │\")\n",
    "        print(f\"├{'─'*98}┤\")\n",
    "        for line in answer.split('\\n'):\n",
    "            print(f\"│ A: {line:<94} │\")\n",
    "        print(f\"└{'─'*98}┘\")\n",
    "        print()\n",
    "    \n",
    "    # Executive Summary\n",
    "    print(\"=\" * 100)\n",
    "    print(\"EXECUTIVE SUMMARY\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    protocol_summaries = result.summary.get('protocol_summaries', {})\n",
    "    if protocol_summaries:\n",
    "        best_by_mean = min(protocol_summaries, key=lambda p: protocol_summaries[p].get('mean_se', float('inf')))\n",
    "        best_by_max = min(protocol_summaries, key=lambda p: protocol_summaries[p].get('max_se', float('inf')))\n",
    "        \n",
    "        print(f\"Best protocol (mean SE): {best_by_mean}\")\n",
    "        print(f\"Best protocol (max SE):  {best_by_max}\")\n",
    "        \n",
    "        if 'classical_shadows_v0' in protocol_summaries and 'direct_grouped' in protocol_summaries:\n",
    "            shadows_se = protocol_summaries['classical_shadows_v0'].get('mean_se', 1)\n",
    "            grouped_se = protocol_summaries['direct_grouped'].get('mean_se', 1)\n",
    "            ratio = shadows_se / grouped_se if grouped_se > 0 else float('inf')\n",
    "            \n",
    "            if ratio < 1:\n",
    "                print(f\"\\nClassical shadows is {1/ratio:.1f}x MORE efficient than direct grouped\")\n",
    "                print(f\"Shot Savings Factor (SSF): {1/(ratio**2):.1f}x\")\n",
    "            else:\n",
    "                print(f\"\\nDirect grouped is {ratio:.1f}x MORE efficient than classical shadows\")\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Full results saved to: {result.output_dir}\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "# Generate the report\n",
    "generate_summary_report(result, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
