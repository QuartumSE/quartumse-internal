{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# QuartumSE Benchmark Suite\n",
    "\n",
    "This notebook benchmarks **classical shadows** against direct measurement baselines for a single circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "from quartumse import (\n",
    "    run_benchmark_suite,\n",
    "    BenchmarkMode,\n",
    "    BenchmarkSuiteConfig,\n",
    "    generate_observable_set,\n",
    "    Observable,\n",
    "    ObservableSet,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Circuit Configuration\n",
    "\n",
    "Define the circuit and observables to benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CIRCUIT CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CIRCUIT_ID = \"ghz_4q\"  # Identifier for this benchmark\n",
    "N_QUBITS = 4\n",
    "\n",
    "# --- Build Circuit ---\n",
    "def build_ghz(n_qubits: int) -> QuantumCircuit:\n",
    "    \"\"\"GHZ state: (|00...0⟩ + |11...1⟩) / sqrt(2)\"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    qc.h(0)\n",
    "    for i in range(1, n_qubits):\n",
    "        qc.cx(i - 1, i)\n",
    "    return qc\n",
    "\n",
    "circuit = build_ghz(N_QUBITS)\n",
    "print(f\"Circuit: {CIRCUIT_ID}\")\n",
    "print(circuit.draw('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "observables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Observables ---\n",
    "observables = []\n",
    "\n",
    "# Random observables with mixed localities\n",
    "for k in range(1, N_QUBITS + 1):\n",
    "    obs_set = generate_observable_set(\n",
    "        generator_id='random_pauli',\n",
    "        n_qubits=N_QUBITS,\n",
    "        n_observables=5,\n",
    "        seed=42 + k,\n",
    "        weight_distribution='fixed',\n",
    "        fixed_weight=k,\n",
    "    )\n",
    "    observables.extend(list(obs_set.observables))\n",
    "\n",
    "# Add GHZ stabilizers\n",
    "observables.extend([\n",
    "    Observable('Z' * N_QUBITS),\n",
    "    Observable('X' * N_QUBITS),\n",
    "])\n",
    "\n",
    "obs_set = ObservableSet(\n",
    "    observables=observables,\n",
    "    observable_set_id=f'{CIRCUIT_ID}_obs',\n",
    "    generator_id='mixed',\n",
    "    generator_seed=42,\n",
    ")\n",
    "\n",
    "# Build locality map\n",
    "locality_map = {}\n",
    "for obs in observables:\n",
    "    locality = sum(1 for c in obs.pauli_string if c != 'I')\n",
    "    locality_map[obs.observable_id] = locality\n",
    "\n",
    "print(f\"Observables: {len(obs_set)}\")\n",
    "loc_counts = Counter(locality_map.values())\n",
    "for k in sorted(loc_counts.keys()):\n",
    "    print(f\"  K={k}: {loc_counts[k]} observables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Benchmark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BENCHMARK CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "config = BenchmarkSuiteConfig(\n",
    "    mode=BenchmarkMode.ANALYSIS,      # Full analysis with all tasks\n",
    "    n_shots_grid=[100, 500, 1000, 5000],\n",
    "    n_replicates=20,                  # 20 for publication quality\n",
    "    seed=42,\n",
    "    epsilon=0.01,                     # Target precision\n",
    "    delta=0.05,                       # Failure probability\n",
    "    shadows_protocol_id=\"classical_shadows_v0\",\n",
    "    baseline_protocol_id=\"direct_grouped\",\n",
    "    output_base_dir=\"benchmark_results\",\n",
    ")\n",
    "\n",
    "print(\"Benchmark Configuration:\")\n",
    "print(f\"  Mode: {config.mode.value}\")\n",
    "print(f\"  Shot grid: {config.n_shots_grid}\")\n",
    "print(f\"  Replicates: {config.n_replicates}\")\n",
    "print(f\"  Target ε: {config.epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# =============================================================================\n",
    "# RUN BENCHMARK\n",
    "# =============================================================================\n",
    "\n",
    "result = run_benchmark_suite(\n",
    "    circuit=circuit,\n",
    "    observable_set=obs_set,\n",
    "    circuit_id=CIRCUIT_ID,\n",
    "    config=config,\n",
    "    locality_map=locality_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Benchmark Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "long_form = result.long_form_results\n",
    "truth_values = result.ground_truth.truth_values if result.ground_truth else {}\n",
    "max_n = max(result.summary.get('n_shots_grid', [5000]))\n",
    "epsilon = config.epsilon\n",
    "\n",
    "# Group data by protocol and N\n",
    "by_protocol_n = defaultdict(lambda: defaultdict(list))\n",
    "for row in long_form:\n",
    "    by_protocol_n[row.protocol_id][row.N_total].append(row)\n",
    "\n",
    "protocols = sorted(by_protocol_n.keys())\n",
    "\n",
    "# Header\n",
    "print(f\"Circuit: {CIRCUIT_ID} | Qubits: {N_QUBITS} | Observables: {len(obs_set)}\")\n",
    "print(f\"Shot grid: {result.summary.get('n_shots_grid', [])} | Replicates: {config.n_replicates} | Target ε = {epsilon}\")\n",
    "print()\n",
    "\n",
    "# Build protocol column headers\n",
    "col_width = 22\n",
    "header = f\"{'Task':<6} {'Question':<45}\"\n",
    "for p in protocols:\n",
    "    short_name = p.replace('classical_shadows_v0', 'shadows').replace('direct_', '')\n",
    "    header += f\" {short_name:>{col_width}}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Task 1: Worst-Case N*\n",
    "row = f\"{'1':<6} {'What N* for max SE ≤ ε (all obs)?':<45}\"\n",
    "for protocol in protocols:\n",
    "    n_star = None\n",
    "    for n in sorted(by_protocol_n[protocol].keys()):\n",
    "        rows = by_protocol_n[protocol][n]\n",
    "        max_se = max(r.se for r in rows if r.se is not None)\n",
    "        if max_se <= epsilon:\n",
    "            n_star = n\n",
    "            break\n",
    "    if n_star:\n",
    "        row += f\" {'N*=' + str(n_star):>{col_width}}\"\n",
    "    else:\n",
    "        rows = by_protocol_n[protocol][max_n]\n",
    "        max_se = max(r.se for r in rows if r.se is not None)\n",
    "        row += f\" {f'N*>{max_n} (SE={max_se:.3f})':>{col_width}}\"\n",
    "print(row)\n",
    "\n",
    "# Task 2: Average N*\n",
    "row = f\"{'2':<6} {'What N* for mean SE ≤ ε?':<45}\"\n",
    "for protocol in protocols:\n",
    "    n_star = None\n",
    "    for n in sorted(by_protocol_n[protocol].keys()):\n",
    "        rows = by_protocol_n[protocol][n]\n",
    "        mean_se = np.mean([r.se for r in rows if r.se is not None])\n",
    "        if mean_se <= epsilon:\n",
    "            n_star = n\n",
    "            break\n",
    "    if n_star:\n",
    "        row += f\" {'N*=' + str(n_star):>{col_width}}\"\n",
    "    else:\n",
    "        rows = by_protocol_n[protocol][max_n]\n",
    "        mean_se = np.mean([r.se for r in rows if r.se is not None])\n",
    "        row += f\" {f'N*>{max_n} (SE={mean_se:.3f})':>{col_width}}\"\n",
    "print(row)\n",
    "\n",
    "# Task 3: SE Distribution (multiple rows)\n",
    "print(f\"{'3':<6} {'SE distribution at N={max_n}?':<45}\")\n",
    "for metric, label in [('mean', 'mean'), ('median', 'median'), ('max', 'max')]:\n",
    "    row = f\"{'':.<6} {'  ' + label:<45}\"\n",
    "    for protocol in protocols:\n",
    "        rows = by_protocol_n[protocol][max_n]\n",
    "        ses = [r.se for r in rows if r.se is not None]\n",
    "        if metric == 'mean':\n",
    "            val = np.mean(ses)\n",
    "        elif metric == 'median':\n",
    "            val = np.median(ses)\n",
    "        else:\n",
    "            val = np.max(ses)\n",
    "        row += f\" {val:>{col_width}.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# Task 4: Dominance\n",
    "obs_best = defaultdict(lambda: {'se': float('inf'), 'protocol': None})\n",
    "for protocol in protocols:\n",
    "    rows = by_protocol_n[protocol][max_n]\n",
    "    for r in rows:\n",
    "        if r.se < obs_best[r.observable_id]['se']:\n",
    "            obs_best[r.observable_id] = {'se': r.se, 'protocol': protocol}\n",
    "wins = defaultdict(int)\n",
    "for obs_id, data in obs_best.items():\n",
    "    if data['protocol']:\n",
    "        wins[data['protocol']] += 1\n",
    "total_obs = len(obs_best)\n",
    "\n",
    "row = f\"{'4':<6} {'Which protocol wins most observables?':<45}\"\n",
    "for protocol in protocols:\n",
    "    win_count = wins[protocol]\n",
    "    win_pct = 100 * win_count / total_obs if total_obs > 0 else 0\n",
    "    row += f\" {f'{win_count}/{total_obs} ({win_pct:.0f}%)':>{col_width}}\"\n",
    "print(row)\n",
    "\n",
    "# Task 5: Pilot Selection\n",
    "row = f\"{'5':<6} {'Optimal pilot fraction?':<45}\"\n",
    "if result.analysis and hasattr(result.analysis, 'pilot_analysis') and result.analysis.pilot_analysis:\n",
    "    pilot_pct = f\"{result.analysis.pilot_analysis.optimal_fraction*100:.0f}%\"\n",
    "    row += f\" {pilot_pct:>{col_width}}\" + \" \" * (col_width + 1) * (len(protocols) - 1)\n",
    "else:\n",
    "    row += f\" {'N/A':>{col_width}}\" + \" \" * (col_width + 1) * (len(protocols) - 1)\n",
    "print(row)\n",
    "\n",
    "# Task 6: Bias-Variance (multiple rows)\n",
    "print(f\"{'6':<6} {'Bias-variance decomposition (MSE)?':<45}\")\n",
    "if truth_values:\n",
    "    for metric, label in [('bias2', 'Bias²'), ('var', 'Variance'), ('mse', 'MSE')]:\n",
    "        row = f\"{'':.<6} {'  ' + label:<45}\"\n",
    "        for protocol in protocols:\n",
    "            rows = by_protocol_n[protocol][max_n]\n",
    "            by_obs = defaultdict(list)\n",
    "            for r in rows:\n",
    "                if r.observable_id in truth_values:\n",
    "                    by_obs[r.observable_id].append(r.estimate)\n",
    "            biases_sq, variances = [], []\n",
    "            for obs_id, estimates in by_obs.items():\n",
    "                truth = truth_values[obs_id]\n",
    "                mean_est = np.mean(estimates)\n",
    "                biases_sq.append((mean_est - truth)**2)\n",
    "                variances.append(np.var(estimates))\n",
    "            if biases_sq:\n",
    "                if metric == 'bias2':\n",
    "                    val = np.mean(biases_sq)\n",
    "                elif metric == 'var':\n",
    "                    val = np.mean(variances)\n",
    "                else:\n",
    "                    val = np.mean(biases_sq) + np.mean(variances)\n",
    "                row += f\" {val:>{col_width}.6f}\"\n",
    "            else:\n",
    "                row += f\" {'N/A':>{col_width}}\"\n",
    "        print(row)\n",
    "else:\n",
    "    print(f\"{'':.<6} {'  (requires ground truth)':<45}\")\n",
    "\n",
    "# Task 7: Noise Sensitivity\n",
    "row = f\"{'7':<6} {'Noise sensitivity?':<45}\"\n",
    "row += f\" {'(requires noise sweep)':>{col_width}}\" + \" \" * (col_width + 1) * (len(protocols) - 1)\n",
    "print(row)\n",
    "\n",
    "# Task 8: Adaptive Efficiency\n",
    "row = f\"{'8':<6} {'Adaptive budget reallocation?':<45}\"\n",
    "row += f\" {'(see Task 5)':>{col_width}}\" + \" \" * (col_width + 1) * (len(protocols) - 1)\n",
    "print(row)\n",
    "\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "# Winner summary\n",
    "winner = max(wins, key=wins.get) if wins else \"N/A\"\n",
    "print(f\"\\nWinner (most observables): {winner}\")\n",
    "print(f\"Results saved to: {result.output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
