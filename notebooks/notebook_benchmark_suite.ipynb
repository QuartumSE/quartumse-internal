{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# QuartumSE Benchmark Suite\n",
    "\n",
    "This notebook demonstrates the **unified benchmark suite** API for running publication-grade benchmarks.\n",
    "\n",
    "## The 8 Tasks (Measurements Bible)\n",
    "\n",
    "| Task | Name | Question |\n",
    "|------|------|----------|\n",
    "| 1 | Worst-Case Guarantee | What N* achieves max SE ≤ ε for all observables? |\n",
    "| 2 | Average Target | What N* achieves mean SE ≤ ε? |\n",
    "| 3 | Fixed Budget | What is the SE distribution at fixed N? |\n",
    "| 4 | Dominance | Which protocol wins on more observables? |\n",
    "| 5 | Pilot Selection | How much budget for pilot vs main run? |\n",
    "| 6 | Bias-Variance | How does MSE decompose into bias² + variance? |\n",
    "| 7 | Noise Sensitivity | How does performance degrade with noise? |\n",
    "| 8 | Adaptive Efficiency | Can we reallocate budget based on pilot? |\n",
    "\n",
    "## Benchmark Modes\n",
    "\n",
    "| Mode | What it runs | Use case |\n",
    "|------|--------------|----------|\n",
    "| `basic` | Tasks 1, 3, 6 + basic report | Quick sanity check |\n",
    "| `complete` | All 8 tasks + complete report | Publication benchmark |\n",
    "| `analysis` | Complete + enhanced analysis | Deep dive with statistics |\n",
    "\n",
    "All results are saved with **unique timestamped directories** - no overwrites!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "from quartumse import (\n",
    "    run_benchmark_suite,\n",
    "    BenchmarkMode,\n",
    "    BenchmarkSuiteConfig,\n",
    "    generate_observable_set,\n",
    "    Observable,\n",
    "    ObservableSet,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Define Circuit and Observables\n",
    "\n",
    "You can use **any Qiskit circuit** and **any set of observables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ┌───┐               \n",
      "q_0: ┤ H ├──■────────────\n",
      "     └───┘┌─┴─┐          \n",
      "q_1: ─────┤ X ├──■───────\n",
      "          └───┘┌─┴─┐     \n",
      "q_2: ──────────┤ X ├──■──\n",
      "               └───┘┌─┴─┐\n",
      "q_3: ───────────────┤ X ├\n",
      "                    └───┘\n"
     ]
    }
   ],
   "source": [
    "# --- Circuit ---\n",
    "N_QUBITS = 4\n",
    "\n",
    "def build_ghz(n_qubits: int) -> QuantumCircuit:\n",
    "    \"\"\"Build GHZ state preparation circuit.\"\"\"\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    qc.h(0)\n",
    "    for i in range(1, n_qubits):\n",
    "        qc.cx(i - 1, i)\n",
    "    return qc\n",
    "\n",
    "circuit = build_ghz(N_QUBITS)\n",
    "print(circuit.draw('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "observables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 22 observables\n",
      "  K=1: 5 observables\n",
      "  K=2: 5 observables\n",
      "  K=3: 5 observables\n",
      "  K=4: 7 observables\n"
     ]
    }
   ],
   "source": [
    "# --- Observables ---\n",
    "# Generate random observables with mixed localities\n",
    "observables = []\n",
    "for k in range(1, N_QUBITS + 1):\n",
    "    obs_set = generate_observable_set(\n",
    "        generator_id='random_pauli',\n",
    "        n_qubits=N_QUBITS,\n",
    "        n_observables=5,\n",
    "        seed=42 + k,\n",
    "        weight_distribution='fixed',\n",
    "        fixed_weight=k,\n",
    "    )\n",
    "    observables.extend(list(obs_set.observables))\n",
    "\n",
    "# Add GHZ stabilizers\n",
    "observables.extend([\n",
    "    Observable('Z' * N_QUBITS),\n",
    "    Observable('X' * N_QUBITS),\n",
    "])\n",
    "\n",
    "obs_set = ObservableSet(\n",
    "    observables=observables,\n",
    "    observable_set_id='benchmark_observables',\n",
    "    generator_id='mixed',\n",
    "    generator_seed=42,\n",
    ")\n",
    "\n",
    "# Build locality map for analysis mode\n",
    "locality_map = {}\n",
    "for obs in observables:\n",
    "    locality = sum(1 for c in obs.pauli_string if c != 'I')\n",
    "    locality_map[obs.observable_id] = locality\n",
    "\n",
    "print(f\"Generated {len(obs_set)} observables\")\n",
    "from collections import Counter\n",
    "loc_counts = Counter(locality_map.values())\n",
    "for k in sorted(loc_counts.keys()):\n",
    "    print(f\"  K={k}: {loc_counts[k]} observables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Benchmark\n",
    "\n",
    "Quick sanity check with Tasks 1, 3, 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "basic-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK SUITE: BASIC\n",
      "======================================================================\n",
      "Run ID: ghz_4q_20260116_123551_37d0b281\n",
      "Output: benchmark_results\\ghz_4q_20260116_123551_37d0b281\n",
      "Mode: basic\n",
      "\n",
      "Step 1: Running base benchmark...\n",
      "  Completed: 990 rows\n",
      "\n",
      "Step 4: Generating reports...\n",
      "  Basic report: benchmark_results\\ghz_4q_20260116_123551_37d0b281\\basic_report.md\n",
      "\n",
      "======================================================================\n",
      "BENCHMARK COMPLETE\n",
      "======================================================================\n",
      "Output directory: benchmark_results\\ghz_4q_20260116_123551_37d0b281\n",
      "Reports generated: ['basic', 'config', 'manifest']\n",
      "\n",
      "CPU times: total: 4min 30s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Basic Benchmark ---\n",
    "config = BenchmarkSuiteConfig(\n",
    "    mode=BenchmarkMode.BASIC,\n",
    "    n_shots_grid=[100, 500, 1000],\n",
    "    n_replicates=5,  # Fewer for quick test\n",
    "    seed=42,\n",
    "    output_base_dir=\"benchmark_results\",\n",
    ")\n",
    "\n",
    "result = run_benchmark_suite(\n",
    "    circuit=circuit,\n",
    "    observable_set=obs_set,\n",
    "    circuit_id=\"ghz_4q\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "basic-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: ghz_4q_20260116_123551_37d0b281\n",
      "Output: benchmark_results\\ghz_4q_20260116_123551_37d0b281\n",
      "Reports: ['basic', 'config', 'manifest']\n",
      "\n",
      "Protocol summaries:\n",
      "  direct_grouped: mean_se=0.0912\n",
      "  direct_optimized: mean_se=0.0836\n",
      "  classical_shadows_v0: mean_se=0.1588\n"
     ]
    }
   ],
   "source": [
    "# --- View Basic Results ---\n",
    "print(f\"Run ID: {result.run_id}\")\n",
    "print(f\"Output: {result.output_dir}\")\n",
    "print(f\"Reports: {list(result.reports.keys())}\")\n",
    "print()\n",
    "print(\"Protocol summaries:\")\n",
    "for protocol, stats in result.summary.get('protocol_summaries', {}).items():\n",
    "    print(f\"  {protocol}: mean_se={stats.get('mean_se', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Complete Benchmark (All 8 Tasks)\n",
    "\n",
    "Publication-grade benchmark with all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complete-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK SUITE: COMPLETE\n",
      "======================================================================\n",
      "Run ID: ghz_4q_20260116_124453_522a4ff6\n",
      "Output: benchmark_results\\ghz_4q_20260116_124453_522a4ff6\n",
      "Mode: complete\n",
      "\n",
      "Step 1: Running base benchmark...\n",
      "  Completed: 2640 rows\n",
      "\n",
      "Step 2: Running all 8 tasks...\n",
      "  Completed: 12 task evaluations\n",
      "\n",
      "Step 4: Generating reports...\n",
      "  Basic report: benchmark_results\\ghz_4q_20260116_124453_522a4ff6\\basic_report.md\n",
      "  Complete report: benchmark_results\\ghz_4q_20260116_124453_522a4ff6\\complete_report.md\n",
      "\n",
      "======================================================================\n",
      "BENCHMARK COMPLETE\n",
      "======================================================================\n",
      "Output directory: benchmark_results\\ghz_4q_20260116_124453_522a4ff6\n",
      "Reports generated: ['basic', 'complete', 'config', 'manifest']\n",
      "\n",
      "CPU times: total: 12min 39s\n",
      "Wall time: 3min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Complete Benchmark ---\n",
    "config = BenchmarkSuiteConfig(\n",
    "    mode=BenchmarkMode.COMPLETE,\n",
    "    n_shots_grid=[100, 500, 1000, 5000],\n",
    "    n_replicates=10,\n",
    "    seed=42,\n",
    "    epsilon=0.01,\n",
    "    delta=0.05,\n",
    "    output_base_dir=\"benchmark_results\",\n",
    ")\n",
    "\n",
    "result = run_benchmark_suite(\n",
    "    circuit=circuit,\n",
    "    observable_set=obs_set,\n",
    "    circuit_id=\"ghz_4q\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "complete-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: ghz_4q_20260116_124453_522a4ff6\n",
      "All tasks completed: 12\n",
      "\n",
      "  task1_classical_shadows_v0\n",
      "  task1_direct_grouped\n",
      "  task1_direct_optimized\n",
      "  task2_classical_shadows_v0\n",
      "  task2_direct_grouped\n",
      "  task2_direct_optimized\n",
      "  task3_classical_shadows_v0\n",
      "  task3_direct_grouped\n",
      "  task3_direct_optimized\n",
      "  task6_classical_shadows_v0\n",
      "  task6_direct_grouped\n",
      "  task6_direct_optimized\n"
     ]
    }
   ],
   "source": [
    "# --- View Complete Results ---\n",
    "print(f\"Run ID: {result.run_id}\")\n",
    "print(f\"All tasks completed: {len(result.all_task_results or {})}\")\n",
    "print()\n",
    "if result.all_task_results:\n",
    "    for task_id in sorted(result.all_task_results.keys()):\n",
    "        print(f\"  {task_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Full Analysis Mode\n",
    "\n",
    "Complete benchmark + enhanced analysis:\n",
    "- N* interpolation (power-law fitting)\n",
    "- Per-observable crossover\n",
    "- Locality correlation\n",
    "- Bootstrap hypothesis testing\n",
    "- Cost-normalized metrics\n",
    "- Multi-pilot fraction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "analysis-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BENCHMARK SUITE: ANALYSIS\n",
      "======================================================================\n",
      "Run ID: ghz_4q_20260116_125630_8c40e800\n",
      "Output: benchmark_results\\ghz_4q_20260116_125630_8c40e800\n",
      "Mode: analysis\n",
      "\n",
      "Step 1: Running base benchmark...\n",
      "  Completed: 5280 rows\n",
      "\n",
      "Step 2: Running all 8 tasks...\n",
      "  Completed: 12 task evaluations\n",
      "\n",
      "Step 3: Running comprehensive analysis...\n",
      "  Comprehensive analysis complete\n",
      "\n",
      "Step 4: Generating reports...\n",
      "  Basic report: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\basic_report.md\n",
      "  Complete report: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\complete_report.md\n",
      "  Analysis report: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\analysis_report.md\n",
      "  Analysis JSON: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\analysis.json\n",
      "\n",
      "======================================================================\n",
      "BENCHMARK COMPLETE\n",
      "======================================================================\n",
      "Output directory: benchmark_results\\ghz_4q_20260116_125630_8c40e800\n",
      "Reports generated: ['basic', 'complete', 'analysis', 'analysis_json', 'config', 'manifest']\n",
      "\n",
      "CPU times: total: 25min 51s\n",
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Full Analysis ---\n",
    "config = BenchmarkSuiteConfig(\n",
    "    mode=BenchmarkMode.ANALYSIS,\n",
    "    n_shots_grid=[100, 500, 1000, 5000],\n",
    "    n_replicates=20,\n",
    "    seed=42,\n",
    "    epsilon=0.01,\n",
    "    delta=0.05,\n",
    "    shadows_protocol_id=\"classical_shadows_v0\",\n",
    "    baseline_protocol_id=\"direct_grouped\",\n",
    "    output_base_dir=\"benchmark_results\",\n",
    ")\n",
    "\n",
    "result = run_benchmark_suite(\n",
    "    circuit=circuit,\n",
    "    observable_set=obs_set,\n",
    "    circuit_id=\"ghz_4q\",\n",
    "    config=config,\n",
    "    locality_map=locality_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "analysis-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: ghz_4q_20260116_125630_8c40e800\n",
      "Output: benchmark_results\\ghz_4q_20260116_125630_8c40e800\n",
      "\n",
      "Analysis Summary:\n",
      "  n_protocols: 3\n",
      "  n_observables: 22\n",
      "  n_shots_evaluated: 4\n",
      "  max_shots: 5000\n",
      "  shadows_mean_se: 0.0725\n",
      "  baseline_mean_se: 0.0405\n",
      "  shadows_vs_baseline_ratio: 1.7883\n",
      "  winner_at_max_n: direct_grouped\n",
      "  shadows_wins_fraction: 0.4545\n",
      "  baseline_wins_fraction: 0.5455\n",
      "  optimal_pilot_fraction: 0.1000\n"
     ]
    }
   ],
   "source": [
    "# --- Analysis Summary ---\n",
    "print(f\"Run ID: {result.run_id}\")\n",
    "print(f\"Output: {result.output_dir}\")\n",
    "print()\n",
    "\n",
    "if result.analysis:\n",
    "    print(\"Analysis Summary:\")\n",
    "    for key, value in result.analysis.summary.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "show-reports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Reports:\n",
      "  basic: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\basic_report.md\n",
      "  complete: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\complete_report.md\n",
      "  analysis: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\analysis_report.md\n",
      "  analysis_json: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\analysis.json\n",
      "  config: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\config.json\n",
      "  manifest: benchmark_results\\ghz_4q_20260116_125630_8c40e800\\manifest.json\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Comprehensive Benchmark Analysis\n",
       "\n",
       "**Run ID:** ghz_4q_20260116_125630_8c40e800\n",
       "**Protocols:** direct_grouped, classical_shadows_v0, direct_optimized\n",
       "**Observables:** 22\n",
       "**Shot Grid:** [100, 500, 1000, 5000]\n",
       "\n",
       "---\n",
       "\n",
       "## Executive Summary\n",
       "\n",
       "- **n_protocols:** 3\n",
       "- **n_observables:** 22\n",
       "- **n_shots_evaluated:** 4\n",
       "- **max_shots:** 5000\n",
       "- **shadows_mean_se:** 0.07249165815911877\n",
       "- **baseline_mean_se:** 0.04053672828641835\n",
       "- **shadows_vs_baseline_ratio:** 1.788295731390014\n",
       "- **winner_at_max_n:** direct_grouped\n",
       "- **shadows_wins_fraction:** 0.45454545454545453\n",
       "- **baseline_wins_fraction:** 0.5454545454545454\n",
       "- **optimal_pilot_fraction:** 0.1\n",
       "\n",
       "---\n",
       "\n",
       "## Task Results\n",
       "\n",
       "### Worst Case\n",
       "\n",
       "\n",
       "**Enhanced Analysis:**\n",
       "\n",
       "### Average Target\n",
       "\n",
       "\n",
       "### Fixed Budget\n",
       "\n",
       "\n",
       "### Dominance\n",
       "\n",
       "- crossover_n: None\n",
       "- always_a_better: False\n",
       "- always_b_better: True\n",
       "- metric_used: mean_se\n",
       "\n",
       "### Pilot Selection\n",
       "\n",
       "- pilot_n: 100\n",
       "- target_n: 5000\n",
       "- selection_accuracy: 0.55\n",
       "- regret: 0.0038921702451488933\n",
       "- criterion_type: truth_based\n",
       "\n",
       "### Bias Variance\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "## Statistical Significance\n",
       "\n",
       "| N | Diff. P-value | K-S P-value | Reject Null | SSF (95% CI) |\n",
       "|---|---------------|-------------|-------------|--------------|\n",
       "| 100 | 0.0000 | 0.0000 | Yes | 0.40 [0.34, 0.48] |\n",
       "| 500 | 0.0000 | 0.0000 | Yes | 0.33 [0.29, 0.38] |\n",
       "| 1000 | 0.0000 | 0.0000 | Yes | 0.32 [0.28, 0.36] |\n",
       "| 5000 | 0.0000 | 0.0000 | Yes | 0.31 [0.27, 0.36] |\n",
       "\n",
       "---\n",
       "\n",
       "## Performance by Locality\n",
       "\n",
       "---\n",
       "\n",
       "## Per-Observable Crossover Analysis\n",
       "\n",
       "- Protocol A (classical_shadows_v0) wins on 45.5% of observables\n",
       "- Protocol B (direct_grouped) wins on 54.5% of observables\n",
       "- Crossover exists for 0.0% of observables\n",
       "\n",
       "---\n",
       "\n",
       "## Multi-Pilot Fraction Analysis\n",
       "\n",
       "| Pilot % | Accuracy | Mean Regret |\n",
       "|---------|----------|-------------|\n",
       "| 2% | 85.0% | 0.0005 |\n",
       "| 5% | 85.0% | 0.0005 |\n",
       "| 10% | 100.0% | 0.0000 |\n",
       "| 20% | 100.0% | 0.0000 |\n",
       "\n",
       "**Optimal pilot fraction:** 10%\n",
       "\n",
       "---\n",
       "\n",
       "## Interpolated N* (Power-Law)\n",
       "\n",
       "- **direct_grouped:** N* = 82803 (RÂ² = 1.000)\n",
       "- **direct_optimized:** N* = 63872 (RÂ² = 1.000)\n",
       "- **classical_shadows_v0:** N* = 446135 (RÂ² = 1.000)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Display Reports ---\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "print(\"Generated Reports:\")\n",
    "for name, path in result.reports.items():\n",
    "    print(f\"  {name}: {path}\")\n",
    "\n",
    "# Display the analysis report\n",
    "if 'analysis' in result.reports:\n",
    "    report_content = result.reports['analysis'].read_text()\n",
    "    display(Markdown(report_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Custom Circuit Example\n",
    "\n",
    "Use any circuit you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Circuit Example ---\n",
    "# Example: Random circuit\n",
    "import numpy as np\n",
    "\n",
    "def build_random_circuit(n_qubits: int, depth: int, seed: int = 42) -> QuantumCircuit:\n",
    "    \"\"\"Build a random circuit.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    \n",
    "    for _ in range(depth):\n",
    "        # Random single-qubit gates\n",
    "        for q in range(n_qubits):\n",
    "            gate = rng.choice(['h', 'x', 'y', 'z', 's', 't'])\n",
    "            getattr(qc, gate)(q)\n",
    "        \n",
    "        # Random CNOTs\n",
    "        for q in range(0, n_qubits - 1, 2):\n",
    "            if rng.random() > 0.5:\n",
    "                qc.cx(q, q + 1)\n",
    "    \n",
    "    return qc\n",
    "\n",
    "custom_circuit = build_random_circuit(4, 3)\n",
    "print(custom_circuit.draw('text'))\n",
    "\n",
    "# You can now run:\n",
    "# result = run_benchmark_suite(custom_circuit, obs_set, circuit_id=\"random_4q_d3\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n\n## 6. Task Summary Report\n\nClear quantitative answers to each of the 8 benchmark questions."
  },
  {
   "cell_type": "code",
   "id": "s3ee1e9erio",
   "source": "# --- TASK SUMMARY REPORT ---\n# Generate clear quantitative answers for each of the 8 tasks\n\nimport numpy as np\nfrom collections import defaultdict\n\ndef generate_task_summary(result):\n    \"\"\"Generate a clear summary answering each task question.\"\"\"\n    \n    print(\"=\" * 80)\n    print(\"BENCHMARK TASK SUMMARY REPORT\")\n    print(\"=\" * 80)\n    print(f\"\\nCircuit: {result.run_id.split('_')[0]}\")\n    print(f\"Observables: {result.summary.get('n_observables', 'N/A')}\")\n    print(f\"Protocols: {', '.join(result.summary.get('protocols', []))}\")\n    print(f\"Shot Grid: {result.summary.get('n_shots_grid', [])}\")\n    print()\n    \n    long_form = result.long_form_results\n    truth_values = result.ground_truth.truth_values if result.ground_truth else {}\n    max_n = max(result.summary.get('n_shots_grid', [5000]))\n    epsilon = 0.01  # Target precision\n    \n    # Group data by protocol and N\n    by_protocol_n = defaultdict(lambda: defaultdict(list))\n    for row in long_form:\n        by_protocol_n[row.protocol_id][row.N_total].append(row)\n    \n    protocols = list(by_protocol_n.keys())\n    \n    # ========== TASK 1: Worst-Case Guarantee ==========\n    print(\"-\" * 80)\n    print(\"TASK 1: WORST-CASE GUARANTEE\")\n    print(\"Question: What N* achieves max SE ≤ ε for ALL observables?\")\n    print(\"-\" * 80)\n    print(f\"Target ε = {epsilon}\")\n    print()\n    \n    for protocol in protocols:\n        n_star = None\n        for n in sorted(by_protocol_n[protocol].keys()):\n            rows = by_protocol_n[protocol][n]\n            max_se = max(r.se for r in rows if r.se is not None)\n            if max_se <= epsilon:\n                n_star = n\n                break\n        \n        if n_star:\n            print(f\"  {protocol}: N* = {n_star} shots\")\n        else:\n            # Get max SE at largest N\n            rows = by_protocol_n[protocol][max_n]\n            max_se = max(r.se for r in rows if r.se is not None)\n            print(f\"  {protocol}: N* > {max_n} (max SE = {max_se:.4f} at N={max_n})\")\n    print()\n    \n    # ========== TASK 2: Average Target ==========\n    print(\"-\" * 80)\n    print(\"TASK 2: AVERAGE TARGET\")\n    print(\"Question: What N* achieves mean SE ≤ ε?\")\n    print(\"-\" * 80)\n    print(f\"Target ε = {epsilon}\")\n    print()\n    \n    for protocol in protocols:\n        n_star = None\n        for n in sorted(by_protocol_n[protocol].keys()):\n            rows = by_protocol_n[protocol][n]\n            mean_se = np.mean([r.se for r in rows if r.se is not None])\n            if mean_se <= epsilon:\n                n_star = n\n                break\n        \n        if n_star:\n            print(f\"  {protocol}: N* = {n_star} shots\")\n        else:\n            rows = by_protocol_n[protocol][max_n]\n            mean_se = np.mean([r.se for r in rows if r.se is not None])\n            print(f\"  {protocol}: N* > {max_n} (mean SE = {mean_se:.4f} at N={max_n})\")\n    print()\n    \n    # ========== TASK 3: Fixed Budget Distribution ==========\n    print(\"-\" * 80)\n    print(\"TASK 3: FIXED BUDGET DISTRIBUTION\")\n    print(f\"Question: What is the SE distribution at N = {max_n}?\")\n    print(\"-\" * 80)\n    print()\n    print(f\"{'Protocol':<25} {'Mean SE':>10} {'Median SE':>10} {'Max SE':>10} {'Std SE':>10}\")\n    print(\"-\" * 70)\n    \n    for protocol in protocols:\n        rows = by_protocol_n[protocol][max_n]\n        ses = [r.se for r in rows if r.se is not None]\n        if ses:\n            print(f\"{protocol:<25} {np.mean(ses):>10.4f} {np.median(ses):>10.4f} \"\n                  f\"{np.max(ses):>10.4f} {np.std(ses):>10.4f}\")\n    print()\n    \n    # ========== TASK 4: Dominance ==========\n    print(\"-\" * 80)\n    print(\"TASK 4: DOMINANCE\")\n    print(\"Question: Which protocol wins on more observables?\")\n    print(\"-\" * 80)\n    print()\n    \n    # Compare at max N\n    obs_winners = defaultdict(lambda: defaultdict(float))\n    for protocol in protocols:\n        rows = by_protocol_n[protocol][max_n]\n        for row in rows:\n            obs_winners[row.observable_id][protocol] = row.se\n    \n    wins = defaultdict(int)\n    for obs_id, protocol_ses in obs_winners.items():\n        if protocol_ses:\n            winner = min(protocol_ses, key=protocol_ses.get)\n            wins[winner] += 1\n    \n    total_obs = len(obs_winners)\n    print(f\"At N = {max_n}:\")\n    for protocol in protocols:\n        win_count = wins[protocol]\n        win_pct = 100 * win_count / total_obs if total_obs > 0 else 0\n        print(f\"  {protocol}: wins on {win_count}/{total_obs} observables ({win_pct:.1f}%)\")\n    \n    if wins:\n        overall_winner = max(wins, key=wins.get)\n        print(f\"\\n  WINNER: {overall_winner}\")\n    print()\n    \n    # ========== TASK 5: Pilot Selection ==========\n    print(\"-\" * 80)\n    print(\"TASK 5: PILOT SELECTION\")\n    print(\"Question: What fraction of budget should be used for pilot?\")\n    print(\"-\" * 80)\n    print()\n    \n    if result.analysis and hasattr(result.analysis, 'pilot_analysis') and result.analysis.pilot_analysis:\n        pilot = result.analysis.pilot_analysis\n        print(f\"{'Pilot %':>10} {'Accuracy':>12} {'Mean Regret':>12}\")\n        print(\"-\" * 40)\n        for frac, res in sorted(pilot.results.items()):\n            print(f\"{frac*100:>9.0f}% {res.selection_accuracy*100:>11.1f}% {res.mean_regret:>12.4f}\")\n        print(f\"\\n  OPTIMAL PILOT: {pilot.optimal_fraction*100:.0f}% of budget\")\n    else:\n        # Estimate based on data\n        print(\"  (Requires analysis mode for detailed pilot study)\")\n        print(\"  Rule of thumb: 5-10% of budget for pilot\")\n    print()\n    \n    # ========== TASK 6: Bias-Variance Decomposition ==========\n    print(\"-\" * 80)\n    print(\"TASK 6: BIAS-VARIANCE DECOMPOSITION\")\n    print(\"Question: How does MSE decompose into bias² + variance?\")\n    print(\"-\" * 80)\n    print()\n    \n    if truth_values:\n        print(f\"{'Protocol':<25} {'Bias²':>12} {'Variance':>12} {'MSE':>12}\")\n        print(\"-\" * 65)\n        \n        for protocol in protocols:\n            rows = by_protocol_n[protocol][max_n]\n            \n            # Group by observable\n            by_obs = defaultdict(list)\n            for row in rows:\n                if row.observable_id in truth_values:\n                    by_obs[row.observable_id].append(row.estimate)\n            \n            biases_sq = []\n            variances = []\n            for obs_id, estimates in by_obs.items():\n                truth = truth_values[obs_id]\n                mean_est = np.mean(estimates)\n                bias = mean_est - truth\n                var = np.var(estimates)\n                biases_sq.append(bias**2)\n                variances.append(var)\n            \n            if biases_sq:\n                mean_bias_sq = np.mean(biases_sq)\n                mean_var = np.mean(variances)\n                mse = mean_bias_sq + mean_var\n                print(f\"{protocol:<25} {mean_bias_sq:>12.6f} {mean_var:>12.6f} {mse:>12.6f}\")\n    else:\n        print(\"  (Requires ground truth for bias-variance decomposition)\")\n    print()\n    \n    # ========== TASK 7: Noise Sensitivity ==========\n    print(\"-\" * 80)\n    print(\"TASK 7: NOISE SENSITIVITY\")\n    print(\"Question: How does performance degrade with noise?\")\n    print(\"-\" * 80)\n    print()\n    print(\"  (Requires running benchmark with multiple noise profiles)\")\n    print(\"  Current run: ideal (noiseless) simulation\")\n    print()\n    \n    # ========== TASK 8: Adaptive Efficiency ==========\n    print(\"-\" * 80)\n    print(\"TASK 8: ADAPTIVE EFFICIENCY\")\n    print(\"Question: Can budget reallocation improve results?\")\n    print(\"-\" * 80)\n    print()\n    print(\"  (Requires two-stage adaptive protocol implementation)\")\n    print(\"  See pilot analysis (Task 5) for related insights\")\n    print()\n    \n    # ========== FINAL SUMMARY ==========\n    print(\"=\" * 80)\n    print(\"EXECUTIVE SUMMARY\")\n    print(\"=\" * 80)\n    print()\n    \n    # Best protocol overall\n    protocol_summaries = result.summary.get('protocol_summaries', {})\n    if protocol_summaries:\n        best_by_mean = min(protocol_summaries, key=lambda p: protocol_summaries[p].get('mean_se', float('inf')))\n        best_by_max = min(protocol_summaries, key=lambda p: protocol_summaries[p].get('max_se', float('inf')))\n        \n        print(f\"Best protocol (mean SE): {best_by_mean}\")\n        print(f\"Best protocol (max SE):  {best_by_max}\")\n        print()\n        \n        # Shot savings factor\n        if 'classical_shadows_v0' in protocol_summaries and 'direct_grouped' in protocol_summaries:\n            shadows_se = protocol_summaries['classical_shadows_v0'].get('mean_se', 1)\n            grouped_se = protocol_summaries['direct_grouped'].get('mean_se', 1)\n            ratio = shadows_se / grouped_se if grouped_se > 0 else float('inf')\n            print(f\"Shadows vs Grouped SE ratio: {ratio:.2f}x\")\n            if ratio < 1:\n                print(f\"  → Classical shadows is {1/ratio:.1f}x more efficient\")\n            else:\n                print(f\"  → Direct grouped is {ratio:.1f}x more efficient\")\n    \n    print()\n    print(\"=\" * 80)\n    print(f\"Full results saved to: {result.output_dir}\")\n    print(\"=\" * 80)\n\n# Generate the summary\ngenerate_task_summary(result)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "x6d6rwzckm",
   "source": "---\n\n## Summary\n\nThe **Task Summary Report** above provides clear quantitative answers to each of the 8 Measurements Bible questions:\n\n| Task | Answer Format |\n|------|---------------|\n| 1 | N* = X shots to achieve max SE ≤ ε |\n| 2 | N* = X shots to achieve mean SE ≤ ε |\n| 3 | Distribution statistics (mean, median, max, std) at fixed N |\n| 4 | Protocol X wins on Y% of observables |\n| 5 | Optimal pilot fraction = X% |\n| 6 | Bias² = X, Variance = Y, MSE = Z |\n| 7 | Performance degradation under noise (requires noise sweep) |\n| 8 | Regret from adaptive reallocation (requires adaptive protocol) |\n\nAll results are saved to unique timestamped directories for reproducibility.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}