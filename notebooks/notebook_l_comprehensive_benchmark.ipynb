{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Comprehensive Benchmark Analysis Suite\n",
    "\n",
    "This notebook runs the **complete enhanced benchmarking suite** implementing all improvements from `Benchmarking_Improvement.md`:\n",
    "\n",
    "## Improvements Implemented\n",
    "\n",
    "| Category | Enhancement | Description |\n",
    "|----------|-------------|-------------|\n",
    "| **Task 1** | N* Interpolation | Power-law fit (SE ∝ N^-0.5) instead of grid search |\n",
    "| **Task 1** | 95th Percentile | More robust than strict max |\n",
    "| **Task 3** | K-S Tests | Statistical test for distribution differences |\n",
    "| **Task 4** | Per-Observable Crossover | Which observables each protocol wins |\n",
    "| **Task 4** | Soft Dominance | \"Wins on 90%+ observables\" criterion |\n",
    "| **Task 5** | Multiple Pilot Fractions | Test 2%, 5%, 10%, 20% |\n",
    "| **New** | Observable Property Analysis | Correlation with locality, commutation |\n",
    "| **New** | Bootstrap Hypothesis Testing | Statistical significance for all claims |\n",
    "| **New** | Cost-Normalized Metrics | Penalize circuit depth overhead |\n",
    "| **New** | Sample Complexity Curves | Visual intuition for scaling |\n",
    "\n",
    "## Output\n",
    "- Detailed per-task analysis with enhancements\n",
    "- Statistical significance testing\n",
    "- Per-observable breakdown\n",
    "- Publication-ready summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "# Core benchmarking\n",
    "from quartumse.benchmarking import run_publication_benchmark\n",
    "from quartumse.observables import Observable, ObservableSet, generate_observable_set\n",
    "from quartumse.protocols import DirectNaiveProtocol, DirectGroupedProtocol, DirectOptimizedProtocol\n",
    "from quartumse.protocols.shadows import ShadowsV0Protocol\n",
    "\n",
    "# Enhanced analysis\n",
    "from quartumse.analysis import (\n",
    "    run_comprehensive_analysis,\n",
    "    interpolate_n_star,\n",
    "    per_observable_crossover,\n",
    "    analyze_by_locality,\n",
    "    compare_protocols_statistically,\n",
    "    bootstrap_ci,\n",
    "    compute_cost_normalized_metrics,\n",
    "    CostModel,\n",
    "    multi_pilot_analysis,\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Qubits: 4\n",
      "  Observables: 30\n",
      "  Shot grid: [100, 500, 1000, 5000]\n",
      "  Replicates: 20\n",
      "  Target epsilon: 0.01\n",
      "  Output: results\\comprehensive_benchmark\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "SEED = 42\n",
    "N_QUBITS = 4\n",
    "N_OBSERVABLES = 30\n",
    "N_SHOTS_GRID = [100, 500, 1000, 5000]\n",
    "N_REPLICATES = 20\n",
    "EPSILON = 0.01\n",
    "DELTA = 0.05\n",
    "\n",
    "# Protocol IDs for comparison\n",
    "SHADOWS_PROTOCOL = \"classical_shadows_v0\"\n",
    "BASELINE_PROTOCOL = \"direct_grouped\"\n",
    "\n",
    "OUTPUT_DIR = Path('results/comprehensive_benchmark')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Qubits: {N_QUBITS}\")\n",
    "print(f\"  Observables: {N_OBSERVABLES}\")\n",
    "print(f\"  Shot grid: {N_SHOTS_GRID}\")\n",
    "print(f\"  Replicates: {N_REPLICATES}\")\n",
    "print(f\"  Target epsilon: {EPSILON}\")\n",
    "print(f\"  Output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circuit-header",
   "metadata": {},
   "source": [
    "## 2. Circuit and Observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ┌───┐               \n",
      "q_0: ┤ H ├──■────────────\n",
      "     └───┘┌─┴─┐          \n",
      "q_1: ─────┤ X ├──■───────\n",
      "          └───┘┌─┴─┐     \n",
      "q_2: ──────────┤ X ├──■──\n",
      "               └───┘┌─┴─┐\n",
      "q_3: ───────────────┤ X ├\n",
      "                    └───┘\n"
     ]
    }
   ],
   "source": [
    "# --- GHZ Circuit ---\n",
    "def build_ghz(n_qubits: int) -> QuantumCircuit:\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    qc.h(0)\n",
    "    for i in range(1, n_qubits):\n",
    "        qc.cx(i - 1, i)\n",
    "    return qc\n",
    "\n",
    "circuit = build_ghz(N_QUBITS)\n",
    "print(circuit.draw('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "observables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 30 observables\n",
      "\n",
      "Locality distribution:\n",
      "  K=1: 7 observables\n",
      "  K=2: 7 observables\n",
      "  K=3: 7 observables\n",
      "  K=4: 9 observables\n"
     ]
    }
   ],
   "source": [
    "# --- Generate observables with mixed localities ---\n",
    "def generate_mixed_locality_observables(n_qubits, n_per_locality, seed):\n",
    "    \"\"\"Generate observables with different localities for comprehensive testing.\"\"\"\n",
    "    all_obs = []\n",
    "    \n",
    "    for k in range(1, n_qubits + 1):\n",
    "        obs_set = generate_observable_set(\n",
    "            generator_id='random_pauli',\n",
    "            n_qubits=n_qubits,\n",
    "            n_observables=n_per_locality,\n",
    "            seed=seed + k,\n",
    "            weight_distribution='fixed',\n",
    "            fixed_weight=k,\n",
    "        )\n",
    "        all_obs.extend(list(obs_set.observables))\n",
    "    \n",
    "    return all_obs\n",
    "\n",
    "# Generate observables\n",
    "n_per_locality = N_OBSERVABLES // N_QUBITS\n",
    "observables = generate_mixed_locality_observables(N_QUBITS, n_per_locality, SEED)\n",
    "\n",
    "# Add GHZ stabilizers\n",
    "ghz_stabilizers = [\n",
    "    Observable('Z' * N_QUBITS),\n",
    "    Observable('X' * N_QUBITS),\n",
    "]\n",
    "observables.extend(ghz_stabilizers)\n",
    "\n",
    "obs_set = ObservableSet(\n",
    "    observables=observables,\n",
    "    observable_set_id='mixed_locality_set',\n",
    "    generator_id='mixed_locality',\n",
    "    generator_seed=SEED,\n",
    ")\n",
    "\n",
    "# Build locality map\n",
    "locality_map = {}\n",
    "for obs in observables:\n",
    "    locality = sum(1 for c in obs.pauli_string if c != 'I')\n",
    "    locality_map[obs.observable_id] = locality\n",
    "\n",
    "print(f\"Generated {len(obs_set)} observables\")\n",
    "print(f\"\\nLocality distribution:\")\n",
    "from collections import Counter\n",
    "loc_counts = Counter(locality_map.values())\n",
    "for k in sorted(loc_counts.keys()):\n",
    "    print(f\"  K={k}: {loc_counts[k]} observables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protocols-header",
   "metadata": {},
   "source": [
    "## 3. Run Base Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protocols",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocols:\n",
      "  - direct_naive\n",
      "  - direct_grouped\n",
      "  - direct_optimized\n",
      "  - classical_shadows_v0\n"
     ]
    }
   ],
   "source": [
    "# --- Protocols ---\n",
    "protocols = [\n",
    "    DirectNaiveProtocol(),\n",
    "    DirectGroupedProtocol(),\n",
    "    DirectOptimizedProtocol(),\n",
    "    ShadowsV0Protocol(),\n",
    "]\n",
    "\n",
    "print(\"Protocols:\")\n",
    "for p in protocols:\n",
    "    print(f\"  - {p.protocol_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running publication benchmark...\n",
      "This will execute 4 protocols × 4 shot budgets × 20 replicates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# --- Run publication benchmark ---\n",
    "print(\"Running publication benchmark...\")\n",
    "print(f\"This will execute {len(protocols)} protocols × {len(N_SHOTS_GRID)} shot budgets × {N_REPLICATES} replicates\")\n",
    "print()\n",
    "\n",
    "results = run_publication_benchmark(\n",
    "    circuit=circuit,\n",
    "    observable_set=obs_set,\n",
    "    protocols=protocols,\n",
    "    n_shots_grid=N_SHOTS_GRID,\n",
    "    n_replicates=N_REPLICATES,\n",
    "    seed=SEED,\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    epsilon=EPSILON,\n",
    "    delta=DELTA,\n",
    ")\n",
    "\n",
    "print(f\"\\nBenchmark completed!\")\n",
    "print(f\"Run ID: {results['summary']['run_id']}\")\n",
    "print(f\"Long-form rows: {results['summary']['n_long_form_rows']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Enhanced Analysis Suite\n",
    "\n",
    "Now we run all the enhanced analyses from `Benchmarking_Improvement.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# --- Run comprehensive analysis ---\n",
    "print(\"Running comprehensive enhanced analysis...\")\n",
    "\n",
    "long_form_rows = results['long_form_results']\n",
    "truth_values = results['ground_truth'].truth_values if results['ground_truth'] else None\n",
    "\n",
    "analysis = run_comprehensive_analysis(\n",
    "    long_form_results=long_form_rows,\n",
    "    truth_values=truth_values,\n",
    "    epsilon=EPSILON,\n",
    "    delta=DELTA,\n",
    "    locality_map=locality_map,\n",
    "    run_id=results['summary']['run_id'],\n",
    "    shadows_protocol_id=SHADOWS_PROTOCOL,\n",
    "    baseline_protocol_id=BASELINE_PROTOCOL,\n",
    ")\n",
    "\n",
    "print(\"Comprehensive analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Executive Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Executive Summary ---\n",
    "summary = analysis.summary\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "for key, value in summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task1-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Task 1: Worst-Case Analysis (Enhanced)\n",
    "\n",
    "**Enhancements:**\n",
    "- N* via power-law interpolation (not grid search)\n",
    "- 95th percentile as alternative to strict max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task1-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Worst-Case ---\n",
    "task1 = analysis.task_analyses.get('task1_worst_case')\n",
    "\n",
    "print(\"TASK 1: WORST-CASE GUARANTEE\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "print(\"Grid-based N* (original):\")\n",
    "for protocol, data in task1.base_results.items():\n",
    "    print(f\"  {protocol}: N* = {data.get('n_star', 'N/A')}\")\n",
    "\n",
    "print()\n",
    "print(\"95th Percentile N* (enhanced):\")\n",
    "for protocol, data in task1.enhanced_results.get('percentile_95_n_star', {}).items():\n",
    "    print(f\"  {protocol}: N* (95th pct) = {data.get('n_star_95th', 'N/A')}\")\n",
    "\n",
    "print()\n",
    "print(\"Interpolated N* (power-law):\")\n",
    "for protocol, data in analysis.interpolated_n_star.items():\n",
    "    n_star = data.get('n_star_interpolated')\n",
    "    r_sq = data.get('r_squared', 0)\n",
    "    if n_star:\n",
    "        print(f\"  {protocol}: N* = {n_star:.0f} (R² = {r_sq:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {protocol}: N* not reached in grid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task3-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Task 3: Fixed Budget Distribution (Enhanced)\n",
    "\n",
    "**Enhancements:**\n",
    "- K-S test for distribution differences\n",
    "- Bootstrap CIs on comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task3-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 3: Fixed Budget Distribution ---\n",
    "print(\"TASK 3: FIXED BUDGET DISTRIBUTION\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "# Get distributions at max N\n",
    "max_n = N_SHOTS_GRID[-1]\n",
    "task3 = analysis.task_analyses.get('task3_distribution')\n",
    "\n",
    "if task3:\n",
    "    distributions = task3.base_results.get('distributions', {})\n",
    "    \n",
    "    print(f\"SE Distribution at N = {max_n}:\")\n",
    "    print()\n",
    "    print(f\"{'Protocol':<25} {'Mean SE':>10} {'Median SE':>10} {'Max SE':>10} {'Std SE':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for protocol, by_n in distributions.items():\n",
    "        if str(max_n) in by_n or max_n in by_n:\n",
    "            stats = by_n.get(str(max_n)) or by_n.get(max_n, {})\n",
    "            print(f\"{protocol:<25} {stats.get('mean', 0):>10.4f} {stats.get('median', 0):>10.4f} \"\n",
    "                  f\"{stats.get('max', 0):>10.4f} {stats.get('std', 0):>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-tests",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Statistical Significance ---\n",
    "print(\"\\nSTATISTICAL SIGNIFICANCE TESTS\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "print(f\"Comparison: {SHADOWS_PROTOCOL} vs {BASELINE_PROTOCOL}\")\n",
    "print()\n",
    "\n",
    "print(f\"{'N':>6} {'Diff P-value':>12} {'K-S P-value':>12} {'Reject H0':>10} {'SSF':>8} {'SSF 95% CI':>20}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for n, comp in sorted(analysis.statistical_comparison.items()):\n",
    "    diff_p = comp.difference_test.p_value\n",
    "    ks_p = comp.ks_test.p_value\n",
    "    reject = \"Yes\" if comp.difference_test.reject_null else \"No\"\n",
    "    \n",
    "    ssf = comp.ssf_ci\n",
    "    if ssf and not np.isnan(ssf.ci_low):\n",
    "        ssf_str = f\"{ssf.estimate:.2f}\"\n",
    "        ssf_ci = f\"[{ssf.ci_low:.2f}, {ssf.ci_high:.2f}]\"\n",
    "    else:\n",
    "        ssf_str = \"N/A\"\n",
    "        ssf_ci = \"N/A\"\n",
    "    \n",
    "    print(f\"{n:>6} {diff_p:>12.4f} {ks_p:>12.4f} {reject:>10} {ssf_str:>8} {ssf_ci:>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Task 4: Dominance Analysis (Enhanced)\n",
    "\n",
    "**Enhancements:**\n",
    "- Per-observable crossover points\n",
    "- Soft dominance (90% threshold)\n",
    "- Performance by locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task4-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 4: Dominance ---\n",
    "print(\"TASK 4: DOMINANCE & CROSSOVER ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "task4 = analysis.task_analyses.get('task4_dominance')\n",
    "if task4:\n",
    "    dom = task4.base_results\n",
    "    print(f\"Shadows wins at N: {dom.get('a_dominates_at', [])}\")\n",
    "    print(f\"Grouped wins at N: {dom.get('b_dominates_at', [])}\")\n",
    "    print(f\"Crossover N: {dom.get('crossover_n', 'None')}\")\n",
    "    print(f\"Always shadows better: {dom.get('always_a_better', False)}\")\n",
    "    print(f\"Always grouped better: {dom.get('always_b_better', False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-obs-crossover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-Observable Crossover ---\n",
    "print(\"\\nPER-OBSERVABLE CROSSOVER ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "crossover = analysis.crossover_analysis\n",
    "if crossover:\n",
    "    summary = crossover.summary\n",
    "    print(f\"Total observables: {summary['n_observables']}\")\n",
    "    print(f\"Shadows always wins: {summary['a_always_wins']} ({summary['a_win_fraction']*100:.1f}%)\")\n",
    "    print(f\"Grouped always wins: {summary['b_always_wins']} ({summary['b_win_fraction']*100:.1f}%)\")\n",
    "    print(f\"Has crossover: {summary['has_crossover']} ({summary['crossover_fraction']*100:.1f}%)\")\n",
    "    \n",
    "    # Soft dominance\n",
    "    print()\n",
    "    soft_90 = crossover.soft_dominance(threshold=0.90)\n",
    "    soft_80 = crossover.soft_dominance(threshold=0.80)\n",
    "    print(f\"Soft dominance (90%): Shadows={soft_90['a_soft_dominates']}, Grouped={soft_90['b_soft_dominates']}\")\n",
    "    print(f\"Soft dominance (80%): Shadows={soft_80['a_soft_dominates']}, Grouped={soft_80['b_soft_dominates']}\")\n",
    "    \n",
    "    # By locality\n",
    "    print()\n",
    "    print(\"Win fraction by locality:\")\n",
    "    by_loc = crossover.by_locality()\n",
    "    print(f\"{'K':>4} {'N Obs':>8} {'Shadows':>10} {'Grouped':>10}\")\n",
    "    print(\"-\" * 35)\n",
    "    for k, stats in by_loc.items():\n",
    "        print(f\"{k:>4} {stats['n_observables']:>8} {stats['a_win_fraction']*100:>9.1f}% {stats['b_win_fraction']*100:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task5-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Task 5: Pilot Selection (Enhanced)\n",
    "\n",
    "**Enhancements:**\n",
    "- Multiple pilot fractions (2%, 5%, 10%, 20%)\n",
    "- Regret analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task5-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 5: Pilot Selection ---\n",
    "print(\"TASK 5: MULTI-PILOT FRACTION ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "pilot = analysis.pilot_analysis\n",
    "if pilot:\n",
    "    print(f\"Target N: {pilot.target_n}\")\n",
    "    print()\n",
    "    print(f\"{'Pilot %':>10} {'Pilot N':>10} {'Accuracy':>10} {'Mean Regret':>12} {'Max Regret':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for frac, result in sorted(pilot.results.items()):\n",
    "        print(f\"{frac*100:>9.0f}% {result.pilot_n:>10} {result.selection_accuracy*100:>9.1f}% \"\n",
    "              f\"{result.mean_regret:>12.4f} {result.max_regret:>12.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Optimal pilot fraction: {pilot.optimal_fraction*100:.0f}%\" if pilot.optimal_fraction else \"Optimal not determined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "locality-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Observable Property Analysis\n",
    "\n",
    "**New analysis:** Correlation between observable locality and protocol performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "locality-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Locality Analysis ---\n",
    "print(\"OBSERVABLE PROPERTY ANALYSIS (LOCALITY)\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "for protocol_id, loc_data in analysis.locality_analysis.items():\n",
    "    print(f\"Protocol: {protocol_id}\")\n",
    "    print(f\"  Locality-SE Correlation: {loc_data.get('locality_correlation', 0):.3f}\")\n",
    "    \n",
    "    reg = loc_data.get('locality_regression', {})\n",
    "    if reg:\n",
    "        print(f\"  Regression: SE = {reg.get('intercept', 0):.4f} + {reg.get('slope', 0):.4f} × K\")\n",
    "        print(f\"  R² = {reg.get('r_squared', 0):.3f}\")\n",
    "    \n",
    "    print(f\"  Performance by locality:\")\n",
    "    by_loc = loc_data.get('by_locality', {})\n",
    "    for k, stats in sorted(by_loc.items(), key=lambda x: int(x[0]) if x[0].isdigit() else 0):\n",
    "        print(f\"    K={k}: mean_se={stats.get('mean_se', 0):.4f}, \"\n",
    "              f\"theoretical_var_factor={stats.get('theoretical_variance_factor', 0)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Cost-Normalized Analysis\n",
    "\n",
    "**New analysis:** Account for circuit depth overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cost-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cost-Normalized Analysis ---\n",
    "print(\"COST-NORMALIZED COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "print()\n",
    "\n",
    "cost = analysis.cost_analysis\n",
    "print(f\"Cost Model: {cost.get('cost_model', {})}\")\n",
    "print()\n",
    "\n",
    "comparison = cost.get('comparison_at_max_n', {})\n",
    "if 'protocols' in comparison:\n",
    "    print(f\"Comparison at N = {comparison.get('n_total')}:\")\n",
    "    print()\n",
    "    print(f\"{'Protocol':<25} {'Raw SE':>10} {'Cost':>10} {'Cost-Norm SE':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for p in comparison['protocols']:\n",
    "        print(f\"{p['protocol_id']:<25} {p['raw_se']:>10.4f} {p['cost']:>10.0f} {p['cost_normalized_se']:>12.4f}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Winner (raw SE): {comparison.get('winner_raw_se')}\")\n",
    "    print(f\"Winner (cost-normalized): {comparison.get('winner_cost_normalized')}\")\n",
    "    print(f\"Ranking changed: {comparison.get('ranking_changed')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-sample-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sample Complexity Curves ---\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group results\n",
    "by_protocol_n = defaultdict(lambda: defaultdict(list))\n",
    "for row in long_form_rows:\n",
    "    by_protocol_n[row.protocol_id][row.N_total].append(row.se)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: SE vs N\n",
    "ax = axes[0]\n",
    "colors = {'direct_naive': 'gray', 'direct_grouped': 'blue', \n",
    "          'direct_optimized': 'green', 'classical_shadows_v0': 'red'}\n",
    "\n",
    "for protocol_id in [BASELINE_PROTOCOL, SHADOWS_PROTOCOL, 'direct_optimized', 'direct_naive']:\n",
    "    if protocol_id not in by_protocol_n:\n",
    "        continue\n",
    "    ns = sorted(by_protocol_n[protocol_id].keys())\n",
    "    mean_ses = [np.mean(by_protocol_n[protocol_id][n]) for n in ns]\n",
    "    std_ses = [np.std(by_protocol_n[protocol_id][n]) for n in ns]\n",
    "    \n",
    "    ax.errorbar(ns, mean_ses, yerr=std_ses, marker='o', label=protocol_id,\n",
    "                color=colors.get(protocol_id, 'black'), capsize=3)\n",
    "\n",
    "ax.axhline(y=EPSILON, color='black', linestyle='--', label=f'Target ε={EPSILON}')\n",
    "ax.set_xlabel('Number of Shots (N)', fontsize=12)\n",
    "ax.set_ylabel('Mean Standard Error', fontsize=12)\n",
    "ax.set_title('Sample Complexity Curves', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: SE by locality\n",
    "ax = axes[1]\n",
    "\n",
    "for protocol_id in [BASELINE_PROTOCOL, SHADOWS_PROTOCOL]:\n",
    "    loc_data = analysis.locality_analysis.get(protocol_id, {})\n",
    "    by_loc = loc_data.get('by_locality', {})\n",
    "    \n",
    "    ks = []\n",
    "    ses = []\n",
    "    for k, stats in sorted(by_loc.items(), key=lambda x: int(x[0]) if str(x[0]).isdigit() else 0):\n",
    "        ks.append(int(k))\n",
    "        ses.append(stats.get('mean_se', 0))\n",
    "    \n",
    "    ax.plot(ks, ses, 'o-', label=protocol_id, color=colors.get(protocol_id, 'black'))\n",
    "\n",
    "ax.set_xlabel('Locality (Pauli Weight K)', fontsize=12)\n",
    "ax.set_ylabel('Mean Standard Error', fontsize=12)\n",
    "ax.set_title(f'Performance by Observable Locality (N={N_SHOTS_GRID[-1]})', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'comprehensive_plots.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlot saved to: {OUTPUT_DIR / 'comprehensive_plots.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-pilot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pilot Selection Plot ---\n",
    "if analysis.pilot_analysis and analysis.pilot_analysis.results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    fracs = []\n",
    "    accs = []\n",
    "    regrets = []\n",
    "    \n",
    "    for frac, result in sorted(analysis.pilot_analysis.results.items()):\n",
    "        fracs.append(frac * 100)\n",
    "        accs.append(result.selection_accuracy * 100)\n",
    "        regrets.append(result.mean_regret)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0].plot(fracs, accs, 'bo-', markersize=10)\n",
    "    axes[0].axhline(y=25, color='gray', linestyle='--', label='Random (25%)')\n",
    "    axes[0].set_xlabel('Pilot Fraction (%)', fontsize=12)\n",
    "    axes[0].set_ylabel('Selection Accuracy (%)', fontsize=12)\n",
    "    axes[0].set_title('Pilot Selection Accuracy', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regret plot\n",
    "    axes[1].plot(fracs, regrets, 'ro-', markersize=10)\n",
    "    axes[1].set_xlabel('Pilot Fraction (%)', fontsize=12)\n",
    "    axes[1].set_ylabel('Mean Regret', fontsize=12)\n",
    "    axes[1].set_title('Selection Regret', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'pilot_analysis.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "report-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Generate Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Markdown Report ---\n",
    "report_content = analysis.generate_report()\n",
    "\n",
    "# Add timestamp and config\n",
    "header = f\"\"\"<!-- Generated: {datetime.now().isoformat()} -->\n",
    "<!-- Config: {N_QUBITS} qubits, {len(obs_set)} observables, {N_REPLICATES} replicates -->\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "full_report = header + report_content\n",
    "\n",
    "# Save report\n",
    "report_path = OUTPUT_DIR / 'comprehensive_report.md'\n",
    "report_path.write_text(full_report, encoding='utf-8')\n",
    "print(f\"Report saved to: {report_path}\")\n",
    "\n",
    "# Save JSON\n",
    "json_path = OUTPUT_DIR / 'comprehensive_analysis.json'\n",
    "analysis.save(json_path)\n",
    "print(f\"JSON saved to: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display Report ---\n",
    "display(Markdown(report_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusions",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE BENCHMARK CONCLUSIONS\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# Key findings\n",
    "s = analysis.summary\n",
    "\n",
    "print(\"1. OVERALL WINNER:\")\n",
    "print(f\"   At N={N_SHOTS_GRID[-1]}: {s.get('winner_at_max_n', 'Unknown')}\")\n",
    "print(f\"   Shadows/Baseline SE ratio: {s.get('shadows_vs_baseline_ratio', 0):.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"2. PER-OBSERVABLE ANALYSIS:\")\n",
    "print(f\"   Shadows wins on {s.get('shadows_wins_fraction', 0)*100:.1f}% of observables\")\n",
    "print(f\"   Baseline wins on {s.get('baseline_wins_fraction', 0)*100:.1f}% of observables\")\n",
    "print()\n",
    "\n",
    "print(\"3. STATISTICAL SIGNIFICANCE:\")\n",
    "if analysis.statistical_comparison:\n",
    "    max_n_comp = analysis.statistical_comparison.get(N_SHOTS_GRID[-1])\n",
    "    if max_n_comp:\n",
    "        print(f\"   Difference p-value: {max_n_comp.difference_test.p_value:.4f}\")\n",
    "        print(f\"   Reject null (α=0.05): {max_n_comp.difference_test.reject_null}\")\n",
    "print()\n",
    "\n",
    "print(\"4. LOCALITY EFFECT:\")\n",
    "if analysis.crossover_analysis:\n",
    "    by_loc = analysis.crossover_analysis.by_locality()\n",
    "    for k, stats in sorted(by_loc.items()):\n",
    "        winner = \"Shadows\" if stats['a_win_fraction'] > stats['b_win_fraction'] else \"Baseline\"\n",
    "        print(f\"   K={k}: {winner} wins ({max(stats['a_win_fraction'], stats['b_win_fraction'])*100:.0f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"5. PILOT SELECTION:\")\n",
    "if analysis.pilot_analysis and analysis.pilot_analysis.optimal_fraction:\n",
    "    opt_frac = analysis.pilot_analysis.optimal_fraction\n",
    "    opt_result = analysis.pilot_analysis.results.get(opt_frac)\n",
    "    if opt_result:\n",
    "        print(f\"   Optimal pilot: {opt_frac*100:.0f}% of budget\")\n",
    "        print(f\"   Selection accuracy: {opt_result.selection_accuracy*100:.1f}%\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Full report: {OUTPUT_DIR / 'comprehensive_report.md'}\")\n",
    "print(f\"JSON data: {OUTPUT_DIR / 'comprehensive_analysis.json'}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
