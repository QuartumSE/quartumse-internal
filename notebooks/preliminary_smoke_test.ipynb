{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Smoke Test (Phase A) Notebook\n",
    "\n",
    "This notebook reproduces the two-qubit Bell-state smoke test described in the experimental plan.\n",
    "It gives you an interactive way to validate the IBM hardware path before running larger studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Install the Quartumse package in the same environment as this notebook (this repository already provides it).\n",
    "2. Make sure Qiskit is installed and up to date.\n",
    "3. Set the `QISKIT_IBM_TOKEN` environment variable to a valid IBM Quantum Platform API token **before** you run any hardware cells.\n",
    "4. Ensure you have runtime minutes available on the `ibm_torino` backend.\n",
    "\n",
    "> ⚠️ The notebook will raise an error if the IBM token is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Set the IBM token in-notebook\n",
    "\n",
    "If you did not export `QISKIT_IBM_TOKEN` when launching the notebook server you can do it here.\n",
    "Remove the comment marker and paste your token string between quotes.\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ[\"QISKIT_IBM_TOKEN\"] = \"paste-your-token\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load environment variables from .env file (if it exists)\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()\n    print('✓ Environment variables loaded from .env file')\nexcept ImportError:\n    print('⚠️ python-dotenv not installed. Install with: pip install python-dotenv')\n    print('   Alternatively, set environment variables manually before starting Jupyter.')\nexcept Exception as e:\n    print(f'⚠️ Could not load .env file: {e}')\n\n# Verify IBM token is available\nimport os\nif not os.environ.get('QISKIT_IBM_TOKEN') and not os.environ.get('QISKIT_RUNTIME_API_TOKEN'):\n    raise EnvironmentError(\n        'QISKIT_IBM_TOKEN or QISKIT_RUNTIME_API_TOKEN is not set.\\n'\n        'Options:\\n'\n        '  1. Create a .env file with: QISKIT_RUNTIME_API_TOKEN=your-token-here\\n'\n        '  2. Set environment variable before starting Jupyter\\n'\n        '  3. Uncomment and use the manual token cell below\\n'\n        'Get your token from: https://quantum.ibm.com'\n    )\n\n# Show which token variable was found\ntoken_var = 'QISKIT_IBM_TOKEN' if os.environ.get('QISKIT_IBM_TOKEN') else 'QISKIT_RUNTIME_API_TOKEN'\nprint(f'✓ IBM token detected from {token_var} (value not shown for security)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helpers\n",
    "\n",
    "The helpers mirror the standalone smoke-test script so results line up with the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nfrom qiskit import QuantumCircuit, transpile\n\nfrom quartumse.connectors import resolve_backend, is_ibm_runtime_backend, create_runtime_sampler\nfrom quartumse.shadows.core import Observable\nfrom quartumse import ShadowEstimator\nfrom quartumse.shadows import ShadowConfig\nfrom quartumse.shadows.config import ShadowVersion\nfrom quartumse.reporting.manifest import MitigationConfig"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bell_circuit():\n",
    "    qc = QuantumCircuit(2)\n",
    "    qc.h(0)\n",
    "    qc.cx(0, 1)\n",
    "    return qc\n",
    "\n",
    "\n",
    "def measure_in_basis(qc, pauli_str):\n",
    "    circuit = qc.copy()\n",
    "    for i, axis in enumerate(pauli_str):\n",
    "        if axis == 'X':\n",
    "            circuit.h(i)\n",
    "        elif axis == 'Y':\n",
    "            circuit.sdg(i)\n",
    "            circuit.h(i)\n",
    "    circuit.measure_all()\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def parity_from_counts(counts, pauli_str, shots):\n",
    "    def bit_value(bitstring, index):\n",
    "        return int(bitstring[-(index + 1)])\n",
    "\n",
    "    expectation = 0.0\n",
    "    for bitstring, ct in counts.items():\n",
    "        weight = ct / shots\n",
    "        parity = 1.0\n",
    "        for idx, axis in enumerate(pauli_str):\n",
    "            if axis != 'I':\n",
    "                parity *= 1 - 2 * bit_value(bitstring, idx)\n",
    "        expectation += weight * parity\n",
    "    return expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to IBM hardware\n",
    "\n",
    "This resolves the IBM backend descriptor and prepares the output directory used by the smoke test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend, snapshot = resolve_backend('ibm:ibm_torino')\n",
    "print('Connected to backend:', backend.name)\n",
    "print('Backend version snapshot:', snapshot)\n",
    "Path('validation_data').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run direct measurement baselines\n",
    "\n",
    "These replicate the 250-shot ZZ/XX parity checks. The results feed into the comparison table at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "observables = [Observable('ZZ', 1.0), Observable('XX', 1.0)]\ndirect_shots = {\n    'ZZ': 250,\n    'XX': 250,\n}\n\ndirect_results = {}\nqc = bell_circuit()\n\n# Check if we need to use IBM Runtime Sampler primitives\nsampler = None\nif is_ibm_runtime_backend(backend):\n    sampler = create_runtime_sampler(backend)\n\nfor obs in observables:\n    pauli = obs.pauli_string\n    shots = direct_shots[pauli]\n    circuit = measure_in_basis(qc, pauli)\n    compiled = transpile(circuit, backend)\n    \n    if sampler is not None:\n        job = sampler.run([compiled], shots=shots)\n        result = job.result()\n        counts = result[0].data.meas.get_counts()\n    else:\n        job = backend.run(compiled, shots=shots)\n        counts = job.result().get_counts()\n    \n    expectation = parity_from_counts(counts, pauli, shots)\n    direct_results[pauli] = {\n        'expectation': float(expectation),\n        'shots': shots,\n        'counts': counts,\n    }\n    print(f\"[Direct] {pauli}: expectation={expectation:.3f}, shots={shots}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical shadows v0 baseline\n",
    "\n",
    "Uses the default (noise-agnostic) shadow workflow with 500 random measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_v0 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V0_BASELINE,\n",
    "        shadow_size=500,\n",
    "        random_seed=42,\n",
    "    ),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v0 = shadow_v0.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v0 manifest saved to:', result_v0.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical shadows v1 with measurement error mitigation\n",
    "\n",
    "This run matches the plan's MEM-assisted configuration using 4×128 calibration shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_shots = 128\n",
    "shadow_v1 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V1_NOISE_AWARE,\n",
    "        shadow_size=200,\n",
    "        random_seed=43,\n",
    "        apply_inverse_channel=True,\n",
    "    ),\n",
    "    mitigation_config=MitigationConfig(techniques=[], parameters={'mem_shots': mem_shots}),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v1 = shadow_v1.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v1 manifest saved to:', result_v1.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results\n",
    "\n",
    "The cell below prints expectation values and (when available) the 95% confidence intervals pulled from the estimator outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ci(ci):\n",
    "    if ci is None:\n",
    "        return 'N/A'\n",
    "    return f\"[{ci[0]:.3f}, {ci[1]:.3f}]\"\n",
    "\n",
    "summary_rows = []\n",
    "for obs in observables:\n",
    "    pauli = obs.pauli_string\n",
    "    direct = direct_results[pauli]['expectation']\n",
    "    v0_stats = result_v0.observables[pauli]\n",
    "    v1_stats = result_v1.observables[pauli]\n",
    "\n",
    "    summary_rows.append({\n",
    "        'Observable': pauli,\n",
    "        'Direct expectation': f\"{direct:.3f}\",\n",
    "        'Shadows v0 expectation': f\"{v0_stats['expectation_value']:.3f}\",\n",
    "        'Shadows v0 CI95': format_ci(v0_stats.get('ci_95')),\n",
    "        'Shadows v1 expectation': f\"{v1_stats['expectation_value']:.3f}\",\n",
    "        'Shadows v1 CI95': format_ci(v1_stats.get('ci_95')),\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation artifacts\n",
    "\n",
    "The smoke test writes manifest files and raw shot data under `validation_data/`. Use the cell below to confirm that captures occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_root = Path('validation_data')\n",
    "for path in sorted(validation_root.rglob('*')):\n",
    "    if path.is_file():\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ctyf53gf3nq",
   "source": "import json\nfrom datetime import datetime\n\n# Compile all results into a single dictionary\nsmoke_test_results = {\n    'metadata': {\n        'test_name': 'preliminary_smoke_test',\n        'timestamp': datetime.utcnow().isoformat() + 'Z',\n        'backend': backend.name,\n        'backend_snapshot': {\n            'backend_name': snapshot.backend_name,\n            'backend_version': snapshot.backend_version,\n            'num_qubits': snapshot.num_qubits,\n            'calibration_timestamp': snapshot.calibration_timestamp.isoformat() if snapshot.calibration_timestamp else None,\n        },\n        'circuit': {\n            'type': 'Bell state',\n            'num_qubits': 2,\n        },\n        'observables': [str(obs) for obs in observables],\n    },\n    'direct_measurements': {\n        'method': 'Direct Pauli measurement with basis rotations',\n        'total_shots': sum(direct_results[k]['shots'] for k in direct_results),\n        'results': {\n            pauli: {\n                'expectation': direct_results[pauli]['expectation'],\n                'shots': direct_results[pauli]['shots'],\n                'counts': dict(direct_results[pauli]['counts'])  # Convert to regular dict\n            }\n            for pauli in direct_results\n        }\n    },\n    'shadows_v0': {\n        'method': 'Classical Shadows v0 (baseline)',\n        'experiment_id': result_v0.experiment_id,\n        'manifest_path': result_v0.manifest_path,\n        'shot_data_path': result_v0.shot_data_path,\n        'shadow_size': 500,\n        'execution_time': result_v0.execution_time,\n        'results': {\n            pauli: {\n                'expectation_value': result_v0.observables[pauli]['expectation_value'],\n                'variance': result_v0.observables[pauli]['variance'],\n                'ci_95': result_v0.observables[pauli].get('ci_95'),\n                'ci_width': result_v0.observables[pauli]['ci_width'],\n            }\n            for pauli in [str(obs) for obs in observables]\n        }\n    },\n    'shadows_v1': {\n        'method': 'Classical Shadows v1 (noise-aware with MEM)',\n        'experiment_id': result_v1.experiment_id,\n        'manifest_path': result_v1.manifest_path,\n        'shot_data_path': result_v1.shot_data_path,\n        'shadow_size': 200,\n        'mem_shots': mem_shots,\n        'execution_time': result_v1.execution_time,\n        'results': {\n            pauli: {\n                'expectation_value': result_v1.observables[pauli]['expectation_value'],\n                'variance': result_v1.observables[pauli]['variance'],\n                'ci_95': result_v1.observables[pauli].get('ci_95'),\n                'ci_width': result_v1.observables[pauli]['ci_width'],\n            }\n            for pauli in [str(obs) for obs in observables]\n        }\n    },\n    'comparison': {\n        'description': 'Side-by-side comparison of all three methods',\n        'expected_values': {\n            'ZZ': 1.0,  # Perfect correlation for Bell state\n            'XX': 1.0,  # Perfect correlation for Bell state\n        },\n        'table': summary_rows,\n    },\n    'analysis': {\n        'notes': [\n            'Bell state should show ZZ = XX = +1 (perfect correlation)',\n            'Noise and finite sampling cause deviations from ideal',\n            'v1 (noise-aware) should show reduced error on noisy hardware',\n            'All methods should give consistent results within confidence intervals',\n        ]\n    }\n}\n\n# Save to JSON file with timestamp\nresults_filename = f\"smoke_test_results_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.json\"\nresults_path = Path('validation_data') / results_filename\n\nwith open(results_path, 'w') as f:\n    json.dump(smoke_test_results, f, indent=2)\n\nprint(f\"✓ Complete smoke test results saved to: {results_path}\")\nprint(f\"  File size: {results_path.stat().st_size / 1024:.1f} KB\")\nprint(f\"\\nResults include:\")\nprint(f\"  - Metadata (backend, timestamp, circuit)\")\nprint(f\"  - Direct measurement results and counts\")\nprint(f\"  - Shadow v0 results (with manifest/shot data paths)\")\nprint(f\"  - Shadow v1 results (with MEM)\")\nprint(f\"  - Comparison table\")\nprint(f\"  - Analysis notes\")\nprint(f\"\\nTo load later:\")\nprint(f\"  import json\")\nprint(f\"  with open('{results_path}', 'r') as f:\")\nprint(f\"      results = json.load(f)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rxe5squzfwb",
   "source": "## Summary: What Gets Saved\n\n**When you run this notebook, the following data is automatically saved to `validation_data/`:**\n\n### 1. Shadow Estimator Manifests (JSON)\n- **Location**: `validation_data/manifests/{experiment_id}.json`\n- **Contains**: Complete provenance including circuit, backend calibration, mitigation config, results\n- **Files**: 2 manifests (one for v0, one for v1)\n- **Size**: ~10 KB each\n\n### 2. Shot Data (Parquet)\n- **Location**: `validation_data/shots/{experiment_id}.parquet`\n- **Contains**: All raw measurement outcomes (basis, bitstring) for replay\n- **Files**: 2 parquet files (one for v0, one for v1)\n- **Size**: ~50-100 KB each\n\n### 3. Smoke Test Summary (JSON)\n- **Location**: `validation_data/smoke_test_results_{timestamp}.json`\n- **Contains**: All results from all three methods (direct, v0, v1) plus comparison\n- **Files**: 1 summary file per notebook run\n- **Size**: ~5-10 KB\n\n### 4. Direct Measurement Raw Counts\n- **Location**: Included in smoke test summary JSON\n- **Contains**: Full count dictionaries for ZZ and XX measurements\n\n**Total storage per smoke test run**: ~200-300 KB\n\n**To review later**:\n```python\n# Load smoke test summary\nimport json\nwith open('validation_data/smoke_test_results_20251022_143000.json', 'r') as f:\n    results = json.load(f)\n\n# Access any result\nprint(results['shadows_v1']['results']['ZZ']['expectation_value'])\n\n# Load full provenance manifests\nfrom quartumse.reporting.manifest import ProvenanceManifest\nmanifest = ProvenanceManifest.from_json(results['shadows_v1']['manifest_path'])\n\n# Replay with new observables\nfrom quartumse import ShadowEstimator\nestimator = ShadowEstimator(backend='...', data_dir='validation_data')\nnew_result = estimator.replay_from_manifest(results['shadows_v1']['manifest_path'])\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "bhkn2kcetbu",
   "source": "## Save Complete Results for Later Review\n\nExport all results (direct, v0, v1, comparison) to JSON for later analysis.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}