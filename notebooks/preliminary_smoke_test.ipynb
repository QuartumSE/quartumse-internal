{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Smoke Test (Phase A) Notebook\n",
    "\n",
    "This notebook reproduces the two-qubit Bell-state smoke test described in the experimental plan.\n",
    "It gives you an interactive way to validate the IBM hardware path before running larger studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Install the Quartumse package in the same environment as this notebook (this repository already provides it).\n",
    "2. Make sure Qiskit is installed and up to date.\n",
    "3. Set the `QISKIT_IBM_TOKEN` environment variable to a valid IBM Quantum Platform API token **before** you run any hardware cells.\n",
    "4. Ensure you have runtime minutes available on the `ibm_torino` backend.\n",
    "\n",
    "> \u26a0\ufe0f The notebook will raise an error if the IBM token is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Set the IBM token in-notebook\n",
    "\n",
    "If you did not export `QISKIT_IBM_TOKEN` when launching the notebook server you can do it here.\n",
    "Remove the comment marker and paste your token string between quotes.\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ[\"QISKIT_IBM_TOKEN\"] = \"paste-your-token\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.environ.get('QISKIT_IBM_TOKEN'):\n",
    "    raise EnvironmentError('QISKIT_IBM_TOKEN is not set. Export your IBM Quantum token before continuing.')\n",
    "print('IBM token detected (value not shown).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helpers\n",
    "\n",
    "The helpers mirror the standalone smoke-test script so results line up with the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "\n",
    "from quartumse.connectors import resolve_backend\n",
    "from quartumse.shadows.core import Observable\n",
    "from quartumse import ShadowEstimator\n",
    "from quartumse.shadows import ShadowConfig\n",
    "from quartumse.shadows.config import ShadowVersion\n",
    "from quartumse.reporting.manifest import MitigationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bell_circuit():\n",
    "    qc = QuantumCircuit(2)\n",
    "    qc.h(0)\n",
    "    qc.cx(0, 1)\n",
    "    return qc\n",
    "\n",
    "\n",
    "def measure_in_basis(qc, pauli_str):\n",
    "    circuit = qc.copy()\n",
    "    for i, axis in enumerate(pauli_str):\n",
    "        if axis == 'X':\n",
    "            circuit.h(i)\n",
    "        elif axis == 'Y':\n",
    "            circuit.sdg(i)\n",
    "            circuit.h(i)\n",
    "    circuit.measure_all()\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def parity_from_counts(counts, pauli_str, shots):\n",
    "    def bit_value(bitstring, index):\n",
    "        return int(bitstring[-(index + 1)])\n",
    "\n",
    "    expectation = 0.0\n",
    "    for bitstring, ct in counts.items():\n",
    "        weight = ct / shots\n",
    "        parity = 1.0\n",
    "        for idx, axis in enumerate(pauli_str):\n",
    "            if axis != 'I':\n",
    "                parity *= 1 - 2 * bit_value(bitstring, idx)\n",
    "        expectation += weight * parity\n",
    "    return expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to IBM hardware\n",
    "\n",
    "This resolves the IBM backend descriptor and prepares the output directory used by the smoke test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend, snapshot = resolve_backend('ibm:ibm_torino')\n",
    "print('Connected to backend:', backend.name)\n",
    "print('Backend version snapshot:', snapshot)\n",
    "Path('validation_data').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run direct measurement baselines\n",
    "\n",
    "These replicate the 250-shot ZZ/XX parity checks. The results feed into the comparison table at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables = [Observable('ZZ', 1.0), Observable('XX', 1.0)]\n",
    "direct_shots = {\n",
    "    'ZZ': 250,\n",
    "    'XX': 250,\n",
    "}\n",
    "\n",
    "direct_results = {}\n",
    "qc = bell_circuit()\n",
    "\n",
    "for obs in observables:\n",
    "    pauli = obs.pauli_string\n",
    "    shots = direct_shots[pauli]\n",
    "    circuit = measure_in_basis(qc, pauli)\n",
    "    compiled = transpile(circuit, backend)\n",
    "    job = backend.run(compiled, shots=shots)\n",
    "    counts = job.result().get_counts()\n",
    "    expectation = parity_from_counts(counts, pauli, shots)\n",
    "    direct_results[pauli] = {\n",
    "        'expectation': float(expectation),\n",
    "        'shots': shots,\n",
    "        'counts': counts,\n",
    "    }\n",
    "    print(f\"[Direct] {pauli}: expectation={expectation:.3f}, shots={shots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical shadows v0 baseline\n",
    "\n",
    "Uses the default (noise-agnostic) shadow workflow with 500 random measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_v0 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V0_BASELINE,\n",
    "        shadow_size=500,\n",
    "        random_seed=42,\n",
    "    ),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v0 = shadow_v0.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v0 manifest saved to:', result_v0.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical shadows v1 with measurement error mitigation\n",
    "\n",
    "This run matches the plan's MEM-assisted configuration using 4\u00d7128 calibration shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_shots = 128\n",
    "shadow_v1 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V1_NOISE_AWARE,\n",
    "        shadow_size=200,\n",
    "        random_seed=43,\n",
    "        apply_inverse_channel=True,\n",
    "    ),\n",
    "    mitigation_config=MitigationConfig(techniques=[], parameters={'mem_shots': mem_shots}),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v1 = shadow_v1.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v1 manifest saved to:', result_v1.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results\n",
    "\n",
    "The cell below prints expectation values and (when available) the 95% confidence intervals pulled from the estimator outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ci(ci):\n",
    "    if ci is None:\n",
    "        return 'N/A'\n",
    "    return f\"[{ci[0]:.3f}, {ci[1]:.3f}]\"\n",
    "\n",
    "summary_rows = []\n",
    "for obs in observables:\n",
    "    pauli = obs.pauli_string\n",
    "    direct = direct_results[pauli]['expectation']\n",
    "    v0_stats = result_v0.observables[pauli]\n",
    "    v1_stats = result_v1.observables[pauli]\n",
    "\n",
    "    summary_rows.append({\n",
    "        'Observable': pauli,\n",
    "        'Direct expectation': f\"{direct:.3f}\",\n",
    "        'Shadows v0 expectation': f\"{v0_stats['expectation_value']:.3f}\",\n",
    "        'Shadows v0 CI95': format_ci(v0_stats.get('ci_95')),\n",
    "        'Shadows v1 expectation': f\"{v1_stats['expectation_value']:.3f}\",\n",
    "        'Shadows v1 CI95': format_ci(v1_stats.get('ci_95')),\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation artifacts\n",
    "\n",
    "The smoke test writes manifest files and raw shot data under `validation_data/`. Use the cell below to confirm that captures occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_root = Path('validation_data')\n",
    "for path in sorted(validation_root.rglob('*')):\n",
    "    if path.is_file():\n",
    "        print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
