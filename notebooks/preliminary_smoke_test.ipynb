{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Smoke Test (Phase A) Notebook\n",
    "\n",
    "This notebook reproduces the two-qubit Bell-state smoke test described in the experimental plan.\n",
    "It gives you an interactive way to validate the IBM hardware path before running larger studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Install the Quartumse package in the same environment as this notebook (this repository already provides it).\n",
    "2. Make sure Qiskit is installed and up to date.\n",
    "3. Set the `QISKIT_IBM_TOKEN` environment variable to a valid IBM Quantum Platform API token **before** you run any hardware cells.\n",
    "4. Ensure you have runtime minutes available on the `ibm_torino` backend.\n",
    "\n",
    "> ⚠️ The notebook will raise an error if the IBM token is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Set the IBM token in-notebook\n",
    "\n",
    "If you did not export `QISKIT_IBM_TOKEN` when launching the notebook server you can do it here.\n",
    "Remove the comment marker and paste your token string between quotes.\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ[\"QISKIT_IBM_TOKEN\"] = \"paste-your-token\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load environment variables from .env file (if it exists)\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()\n    print('✓ Environment variables loaded from .env file')\nexcept ImportError:\n    print('⚠️ python-dotenv not installed. Install with: pip install python-dotenv')\n    print('   Alternatively, set environment variables manually before starting Jupyter.')\nexcept Exception as e:\n    print(f'⚠️ Could not load .env file: {e}')\n\n# Verify IBM token is available\nimport os\nif not os.environ.get('QISKIT_IBM_TOKEN') and not os.environ.get('QISKIT_RUNTIME_API_TOKEN'):\n    raise EnvironmentError(\n        'QISKIT_IBM_TOKEN or QISKIT_RUNTIME_API_TOKEN is not set.\\n'\n        'Options:\\n'\n        '  1. Create a .env file with: QISKIT_RUNTIME_API_TOKEN=your-token-here\\n'\n        '  2. Set environment variable before starting Jupyter\\n'\n        '  3. Uncomment and use the manual token cell below\\n'\n        'Get your token from: https://quantum.ibm.com'\n    )\n\n# Show which token variable was found\ntoken_var = 'QISKIT_IBM_TOKEN' if os.environ.get('QISKIT_IBM_TOKEN') else 'QISKIT_RUNTIME_API_TOKEN'\nprint(f'✓ IBM token detected from {token_var} (value not shown for security)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and helpers\n",
    "\n",
    "The helpers mirror the standalone smoke-test script so results line up with the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport numpy as np\nfrom qiskit import QuantumCircuit, transpile\n\nfrom quartumse.connectors import resolve_backend, is_ibm_runtime_backend, create_runtime_sampler\nfrom quartumse.shadows.core import Observable\nfrom quartumse import ShadowEstimator\nfrom quartumse.shadows import ShadowConfig\nfrom quartumse.shadows.config import ShadowVersion\nfrom quartumse.reporting.manifest import MitigationConfig"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bell_circuit():\n",
    "    qc = QuantumCircuit(2)\n",
    "    qc.h(0)\n",
    "    qc.cx(0, 1)\n",
    "    return qc\n",
    "\n",
    "\n",
    "def measure_in_basis(qc, pauli_str):\n",
    "    circuit = qc.copy()\n",
    "    for i, axis in enumerate(pauli_str):\n",
    "        if axis == 'X':\n",
    "            circuit.h(i)\n",
    "        elif axis == 'Y':\n",
    "            circuit.sdg(i)\n",
    "            circuit.h(i)\n",
    "    circuit.measure_all()\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def parity_from_counts(counts, pauli_str, shots):\n",
    "    def bit_value(bitstring, index):\n",
    "        return int(bitstring[-(index + 1)])\n",
    "\n",
    "    expectation = 0.0\n",
    "    for bitstring, ct in counts.items():\n",
    "        weight = ct / shots\n",
    "        parity = 1.0\n",
    "        for idx, axis in enumerate(pauli_str):\n",
    "            if axis != 'I':\n",
    "                parity *= 1 - 2 * bit_value(bitstring, idx)\n",
    "        expectation += weight * parity\n",
    "    return expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to IBM hardware\n",
    "\n",
    "This resolves the IBM backend descriptor and prepares the output directory used by the smoke test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend, snapshot = resolve_backend('ibm:ibm_torino')\n",
    "print('Connected to backend:', backend.name)\n",
    "print('Backend version snapshot:', snapshot)\n",
    "Path('validation_data').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run direct measurement baselines\n",
    "\n",
    "These replicate the 250-shot ZZ/XX parity checks. The results feed into the comparison table at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "observables = [Observable('ZZ', 1.0), Observable('XX', 1.0)]\ndirect_shots = {\n    'ZZ': 250,\n    'XX': 250,\n}\n\ndirect_results = {}\nqc = bell_circuit()\n\n# Check if we need to use IBM Runtime Sampler primitives\nsampler = None\nif is_ibm_runtime_backend(backend):\n    sampler = create_runtime_sampler(backend)\n\nfor obs in observables:\n    pauli = obs.pauli_string\n    shots = direct_shots[pauli]\n    circuit = measure_in_basis(qc, pauli)\n    compiled = transpile(circuit, backend)\n    \n    if sampler is not None:\n        job = sampler.run([compiled], shots=shots)\n        result = job.result()\n        counts = result[0].data.meas.get_counts()\n    else:\n        job = backend.run(compiled, shots=shots)\n        counts = job.result().get_counts()\n    \n    expectation = parity_from_counts(counts, pauli, shots)\n    direct_results[pauli] = {\n        'expectation': float(expectation),\n        'shots': shots,\n        'counts': counts,\n    }\n    print(f\"[Direct] {pauli}: expectation={expectation:.3f}, shots={shots}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical shadows v0 baseline\n",
    "\n",
    "Uses the default (noise-agnostic) shadow workflow with 500 random measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_v0 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V0_BASELINE,\n",
    "        shadow_size=500,\n",
    "        random_seed=42,\n",
    "    ),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v0 = shadow_v0.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v0 manifest saved to:', result_v0.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical shadows v1 with measurement error mitigation\n",
    "\n",
    "This run matches the plan's MEM-assisted configuration using 4×128 calibration shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_shots = 128\n",
    "shadow_v1 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V1_NOISE_AWARE,\n",
    "        shadow_size=200,\n",
    "        random_seed=43,\n",
    "        apply_inverse_channel=True,\n",
    "    ),\n",
    "    mitigation_config=MitigationConfig(techniques=[], parameters={'mem_shots': mem_shots}),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v1 = shadow_v1.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v1 manifest saved to:', result_v1.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results\n",
    "\n",
    "The cell below prints expectation values and (when available) the 95% confidence intervals pulled from the estimator outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ci(ci):\n",
    "    if ci is None:\n",
    "        return 'N/A'\n",
    "    return f\"[{ci[0]:.3f}, {ci[1]:.3f}]\"\n",
    "\n",
    "summary_rows = []\n",
    "for obs in observables:\n",
    "    pauli = obs.pauli_string\n",
    "    direct = direct_results[pauli]['expectation']\n",
    "    v0_stats = result_v0.observables[pauli]\n",
    "    v1_stats = result_v1.observables[pauli]\n",
    "\n",
    "    summary_rows.append({\n",
    "        'Observable': pauli,\n",
    "        'Direct expectation': f\"{direct:.3f}\",\n",
    "        'Shadows v0 expectation': f\"{v0_stats['expectation_value']:.3f}\",\n",
    "        'Shadows v0 CI95': format_ci(v0_stats.get('ci_95')),\n",
    "        'Shadows v1 expectation': f\"{v1_stats['expectation_value']:.3f}\",\n",
    "        'Shadows v1 CI95': format_ci(v1_stats.get('ci_95')),\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation artifacts\n",
    "\n",
    "The smoke test writes manifest files and raw shot data under `validation_data/`. Use the cell below to confirm that captures occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_root = Path('validation_data')\n",
    "for path in sorted(validation_root.rglob('*')):\n",
    "    if path.is_file():\n",
    "        print(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}