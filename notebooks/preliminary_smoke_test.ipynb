{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9355a09",
   "metadata": {},
   "source": [
    "# IBM Smoke Test (Phase A) Notebook\n",
    "\n",
    "This notebook reproduces the two-qubit Bell-state smoke test described in the experimental plan.\n",
    "It gives you an interactive way to validate the IBM hardware path before running larger studies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9ec99",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Install the Quartumse package in the same environment as this notebook (this repository already provides it).\n",
    "2. Make sure Qiskit is installed and up to date.\n",
    "3. Set the `QISKIT_IBM_TOKEN` environment variable to a valid IBM Quantum Platform API token **before** you run any hardware cells.\n",
    "4. Ensure you have runtime minutes available on the `ibm_torino` backend.\n",
    "\n",
    "> ⚠️ The notebook will raise an error if the IBM token is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01032e55",
   "metadata": {},
   "source": [
    "### Option: Set the IBM token in-notebook\n",
    "\n",
    "If you did not export `QISKIT_IBM_TOKEN` when launching the notebook server you can do it here.\n",
    "Remove the comment marker and paste your token string between quotes.\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ[\"QISKIT_IBM_TOKEN\"] = \"paste-your-token\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file (if it exists)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print('✓ Environment variables loaded from .env file')\n",
    "except ImportError:\n",
    "    print('⚠️ python-dotenv not installed. Install with: pip install python-dotenv')\n",
    "    print('   Alternatively, set environment variables manually before starting Jupyter.')\n",
    "except Exception as e:\n",
    "    print(f'⚠️ Could not load .env file: {e}')\n",
    "\n",
    "# Verify IBM token is available\n",
    "import os\n",
    "if not os.environ.get('QISKIT_IBM_TOKEN') and not os.environ.get('QISKIT_RUNTIME_API_TOKEN'):\n",
    "    raise EnvironmentError(\n",
    "        'QISKIT_IBM_TOKEN or QISKIT_RUNTIME_API_TOKEN is not set.\\n'\n",
    "        'Options:\\n'\n",
    "        '  1. Create a .env file with: QISKIT_RUNTIME_API_TOKEN=your-token-here\\n'\n",
    "        '  2. Set environment variable before starting Jupyter\\n'\n",
    "        '  3. Uncomment and use the manual token cell below\\n'\n",
    "        'Get your token from: https://quantum.ibm.com'\n",
    "    )\n",
    "\n",
    "# Show which token variable was found\n",
    "token_var = 'QISKIT_IBM_TOKEN' if os.environ.get('QISKIT_IBM_TOKEN') else 'QISKIT_RUNTIME_API_TOKEN'\n",
    "print(f'✓ IBM token detected from {token_var} (value not shown for security)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f6de",
   "metadata": {},
   "source": [
    "## Imports and helpers\n",
    "\n",
    "The helpers mirror the standalone smoke-test script so results line up with the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "\n",
    "from quartumse.connectors import resolve_backend, is_ibm_runtime_backend, create_runtime_sampler\n",
    "from quartumse.shadows.core import Observable\n",
    "from quartumse import ShadowEstimator\n",
    "from quartumse.shadows import ShadowConfig\n",
    "from quartumse.shadows.config import ShadowVersion\n",
    "from quartumse.reporting.manifest import MitigationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6105e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSPILE_OPTIONS = {\n",
    "    'optimization_level': 1,\n",
    "    'seed_transpiler': 314159,\n",
    "}\n",
    "\n",
    "\n",
    "def bell_circuit():\n",
    "    qc = QuantumCircuit(2)\n",
    "    qc.h(0)\n",
    "    qc.cx(0, 1)\n",
    "    return qc\n",
    "\n",
    "\n",
    "def measure_in_basis(qc, pauli_str):\n",
    "    circuit = qc.copy()\n",
    "    for i, axis in enumerate(pauli_str):\n",
    "        if axis == 'X':\n",
    "            circuit.h(i)\n",
    "        elif axis == 'Y':\n",
    "            circuit.sdg(i)\n",
    "            circuit.h(i)\n",
    "    circuit.measure_all()\n",
    "    return circuit\n",
    "\n",
    "\n",
    "def parity_from_counts(counts, pauli_str, shots):\n",
    "    def bit_value(bitstring, index):\n",
    "        return int(bitstring[-(index + 1)])\n",
    "\n",
    "    expectation = 0.0\n",
    "    for bitstring, ct in counts.items():\n",
    "        weight = ct / shots\n",
    "        parity = 1.0\n",
    "        for idx, axis in enumerate(pauli_str):\n",
    "            if axis != 'I':\n",
    "                parity *= 1 - 2 * bit_value(bitstring, idx)\n",
    "        expectation += weight * parity\n",
    "    return expectation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13145f7e",
   "metadata": {},
   "source": [
    "## Connect to IBM hardware\n",
    "\n",
    "This resolves the IBM backend descriptor and prepares the output directory used by the smoke test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3431b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend, snapshot = resolve_backend('ibm:ibm_torino')\n",
    "print('Connected to backend:', backend.name)\n",
    "print('Backend version snapshot:', snapshot)\n",
    "Path('validation_data').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee14168",
   "metadata": {},
   "source": [
    "## Run direct measurement baselines\n",
    "\n",
    "These replicate the 250-shot ZZ/XX parity checks. The results feed into the comparison table at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03700a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "observables = [Observable('ZZ', 1.0), Observable('XX', 1.0)]\n",
    "direct_shots = {\n",
    "    'ZZ': 250,\n",
    "    'XX': 250,\n",
    "}\n",
    "\n",
    "direct_results = {}\n",
    "qc = bell_circuit()\n",
    "\n",
    "sampler = None\n",
    "execution_mode = 'backend.run'\n",
    "if is_ibm_runtime_backend(backend):\n",
    "    sampler = create_runtime_sampler(backend)\n",
    "    if sampler is not None:\n",
    "        execution_mode = 'runtime_sampler'\n",
    "\n",
    "for obs in observables:\n",
    "    pauli = obs.pauli_string\n",
    "    shots = direct_shots[pauli]\n",
    "    circuit = measure_in_basis(qc, pauli)\n",
    "    compiled = transpile(circuit, backend, **TRANSPILE_OPTIONS)\n",
    "\n",
    "    if sampler is not None:\n",
    "        job = sampler.run([compiled], shots=shots)\n",
    "        result = job.result()\n",
    "        counts = result[0].data.meas.get_counts()\n",
    "    else:\n",
    "        job = backend.run(compiled, shots=shots)\n",
    "        counts = job.result().get_counts()\n",
    "\n",
    "    expectation = parity_from_counts(counts, pauli, shots)\n",
    "\n",
    "    try:\n",
    "        compiled_qasm = compiled.qasm()\n",
    "    except Exception:\n",
    "        compiled_qasm = None\n",
    "\n",
    "    direct_results[pauli] = {\n",
    "        'expectation': float(expectation),\n",
    "        'shots': shots,\n",
    "        'counts': dict(counts),\n",
    "        'compiled_circuit_qasm': compiled_qasm,\n",
    "        'execution_mode': execution_mode,\n",
    "        'transpile_options': dict(TRANSPILE_OPTIONS),\n",
    "    }\n",
    "    print(f\"[Direct] {pauli}: expectation={expectation:.3f}, shots={shots}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d70995-5e4d-498b-84f2-9448bf07f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Retrieve the completed job results manually\n",
    "  from qiskit_ibm_runtime import QiskitRuntimeService\n",
    "  import os\n",
    "\n",
    "  token = os.environ.get('QISKIT_IBM_TOKEN') or os.environ.get('QISKIT_RUNTIME_API_TOKEN')\n",
    "  service = QiskitRuntimeService(token=token)\n",
    "\n",
    "  # Get the most recent completed jobs\n",
    "  jobs = service.jobs(limit=5, backend_name='ibm_torino')\n",
    "  for job in jobs:\n",
    "      status = job.status()\n",
    "      print(f\"Job {job.job_id()}: {status}\")\n",
    "\n",
    "      # Check if status is string 'DONE' or has .name attribute\n",
    "      status_str = status if isinstance(status, str) else status.name\n",
    "\n",
    "      if status_str == 'DONE':\n",
    "          print(f\"  ✓ Job completed successfully\")\n",
    "          try:\n",
    "              result = job.result()\n",
    "              print(f\"  Result type: {type(result)}\")\n",
    "\n",
    "              # Extract counts from SamplerV2 result\n",
    "              counts = result[0].data.meas.get_counts()\n",
    "              print(f\"  Counts: {counts}\")\n",
    "              print(f\"  Total shots: {sum(counts.values())}\")\n",
    "\n",
    "              # Calculate which observable this was (ZZ or XX)\n",
    "              # ZZ should show mostly '00' and '11'\n",
    "              # XX should show more mixed results\n",
    "              if '00' in counts and '11' in counts:\n",
    "                  correlation = (counts.get('00', 0) + counts.get('11', 0)) / sum(counts.values())\n",
    "                  print(f\"  Correlation (00+11): {correlation:.3f}\")\n",
    "\n",
    "          except Exception as e:\n",
    "              print(f\"  Error extracting result: {e}\")\n",
    "              import traceback\n",
    "              traceback.print_exc()\n",
    "\n",
    "          print()  # Blank line between jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0095ac6",
   "metadata": {},
   "source": [
    "## Classical shadows v0 baseline\n",
    "\n",
    "Uses the default (noise-agnostic) shadow workflow with 500 random measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b42359",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_v0 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V0_BASELINE,\n",
    "        shadow_size=500,\n",
    "        random_seed=42,\n",
    "    ),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v0 = shadow_v0.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v0 manifest saved to:', result_v0.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56823117",
   "metadata": {},
   "source": [
    "## Classical shadows v1 with measurement error mitigation\n",
    "\n",
    "This run matches the plan's MEM-assisted configuration using 4×128 calibration shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95e0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_shots = 128\n",
    "shadow_v1 = ShadowEstimator(\n",
    "    backend='ibm:ibm_torino',\n",
    "    shadow_config=ShadowConfig(\n",
    "        version=ShadowVersion.V1_NOISE_AWARE,\n",
    "        shadow_size=200,\n",
    "        random_seed=43,\n",
    "        apply_inverse_channel=True,\n",
    "    ),\n",
    "    mitigation_config=MitigationConfig(techniques=[], parameters={'mem_shots': mem_shots}),\n",
    "    data_dir='validation_data',\n",
    ")\n",
    "\n",
    "result_v1 = shadow_v1.estimate(qc, observables, save_manifest=True)\n",
    "print('Shadow v1 manifest saved to:', result_v1.manifest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d212e4",
   "metadata": {},
   "source": [
    "## Compare the results\n",
    "\n",
    "The cell below prints expectation values and (when available) the 95% confidence intervals pulled from the estimator outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ci(ci):\n",
    "  if ci is None:\n",
    "      return 'N/A'\n",
    "  return f\"[{ci[0]:.3f}, {ci[1]:.3f}]\"\n",
    "\n",
    "summary_rows = []\n",
    "for obs in observables:\n",
    "  obs_key = str(obs)  # Use full string representation as key\n",
    "  pauli = obs.pauli_string  # Use for display\n",
    "  direct = direct_results[pauli]['expectation']\n",
    "  v0_stats = result_v0.observables[obs_key]  # Fixed: use obs_key\n",
    "  v1_stats = result_v1.observables[obs_key]  # Fixed: use obs_key\n",
    "\n",
    "  summary_rows.append({\n",
    "      'Observable': pauli,\n",
    "      'Direct expectation': f\"{direct:.3f}\",\n",
    "      'Shadows v0 expectation': f\"{v0_stats['expectation_value']:.3f}\",\n",
    "      'Shadows v0 CI95': format_ci(v0_stats.get('ci_95')),\n",
    "      'Shadows v1 expectation': f\"{v1_stats['expectation_value']:.3f}\",\n",
    "      'Shadows v1 CI95': format_ci(v1_stats.get('ci_95')),\n",
    "  })\n",
    "\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf764100-ee3d-4091-9926-2ef654725894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's in result_v0\n",
    "print(\"Cell 12 result_v0 status:\")\n",
    "print(f\"  Type: {type(result_v0)}\")\n",
    "print(f\"  Has observables? {hasattr(result_v0, 'observables')}\")\n",
    "\n",
    "if hasattr(result_v0, 'observables'):\n",
    "  print(f\"  Observables keys: {list(result_v0.observables.keys())}\")\n",
    "  print(f\"  Observables data: {result_v0.observables}\")\n",
    "else:\n",
    "  print(\"  No observables attribute!\")\n",
    "\n",
    "# Check what observables were requested\n",
    "print(\"\\nRequested observables:\")\n",
    "for obs in observables:\n",
    "  print(f\"  Observable: {obs}\")\n",
    "  print(f\"  pauli_string: {obs.pauli_string}\")\n",
    "  print(f\"  str(obs): {str(obs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab07a0d",
   "metadata": {},
   "source": [
    "## Validation artifacts\n",
    "\n",
    "The smoke test writes manifest files and raw shot data under `validation_data/`. Use the cell below to confirm that captures occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7676fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_entries = [\n",
    "    ('Shadow v0 manifest', Path(result_v0.manifest_path)),\n",
    "    ('Shadow v0 shot data', Path(result_v0.shot_data_path)),\n",
    "    ('Shadow v1 manifest', Path(result_v1.manifest_path)),\n",
    "    ('Shadow v1 shot data', Path(result_v1.shot_data_path)),\n",
    "]\n",
    "\n",
    "if result_v1.mitigation_confusion_matrix_path:\n",
    "    validation_entries.append((\n",
    "        'Shadow v1 MEM confusion matrix',\n",
    "        Path(result_v1.mitigation_confusion_matrix_path),\n",
    "    ))\n",
    "\n",
    "def _candidate_paths(raw_path):\n",
    "    path_obj = Path(raw_path)\n",
    "    yield path_obj\n",
    "    if not path_obj.is_absolute():\n",
    "        yield Path('validation_data') / path_obj\n",
    "        yield Path(result_v0.manifest_path).parent / path_obj\n",
    "        yield Path(result_v1.manifest_path).parent / path_obj\n",
    "\n",
    "for label, raw_path in validation_entries:\n",
    "    candidates = list(_candidate_paths(raw_path))\n",
    "    resolved = next((p for p in candidates if p.exists()), None)\n",
    "    if resolved is None:\n",
    "        raise FileNotFoundError(f\"Expected artifact missing: {label} -> {raw_path}\")\n",
    "    size_kb = resolved.stat().st_size / 1024\n",
    "    print(f\"\\u2713 {label}: {resolved} ({size_kb:.1f} KB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ctyf53gf3nq",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import platform\n",
    "import subprocess\n",
    "from datetime import datetime, UTC\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import qiskit\n",
    "import quartumse\n",
    "\n",
    "# Custom JSON encoder to handle numpy types\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "  def default(self, obj):\n",
    "      if isinstance(obj, np.integer):\n",
    "          return int(obj)\n",
    "      if isinstance(obj, np.floating):\n",
    "          return float(obj)\n",
    "      if isinstance(obj, np.ndarray):\n",
    "          return obj.tolist()\n",
    "      if isinstance(obj, np.bool_):\n",
    "          return bool(obj)\n",
    "      return super().default(obj)\n",
    "\n",
    "EXPECTED_VALUES = {\n",
    "  'ZZ': 1.0,\n",
    "  'XX': 1.0,\n",
    "}\n",
    "CATASTROPHIC_DELTA = 0.8\n",
    "WARNING_DELTA = 0.4\n",
    "\n",
    "quality_checks = []\n",
    "for obs in observables:\n",
    "  pauli = obs.pauli_string\n",
    "  obs_key = str(obs)\n",
    "  expected = EXPECTED_VALUES.get(pauli, 0.0)\n",
    "  direct = direct_results[pauli]['expectation']\n",
    "  v0_stats = result_v0.observables[obs_key]\n",
    "  v1_stats = result_v1.observables[obs_key]\n",
    "\n",
    "  direct_delta = abs(direct - expected)\n",
    "  v0_delta = abs(v0_stats['expectation_value'] - expected)\n",
    "  v1_delta = abs(v1_stats['expectation_value'] - expected)\n",
    "\n",
    "  if direct_delta > CATASTROPHIC_DELTA:\n",
    "      raise AssertionError(\n",
    "          f\"Direct measurement for {pauli} deviates from expectation by {direct_delta:.3f} (> {CATASTROPHIC_DELTA})\"\n",
    "      )\n",
    "  if v0_delta > CATASTROPHIC_DELTA:\n",
    "      raise AssertionError(\n",
    "          f\"Shadows v0 result for {pauli} deviates from expectation by {v0_delta:.3f} (> {CATASTROPHIC_DELTA})\"\n",
    "      )\n",
    "  if v1_delta > CATASTROPHIC_DELTA:\n",
    "      raise AssertionError(\n",
    "          f\"Shadows v1 result for {pauli} deviates from expectation by {v1_delta:.3f} (> {CATASTROPHIC_DELTA})\"\n",
    "      )\n",
    "\n",
    "  v0_ci = v0_stats.get('ci_95')\n",
    "  v1_ci = v1_stats.get('ci_95')\n",
    "  v0_ci_contains_expected = None if v0_ci is None else (v0_ci[0] <= expected <= v0_ci[1])\n",
    "  v1_ci_contains_expected = None if v1_ci is None else (v1_ci[0] <= expected <= v1_ci[1])\n",
    "\n",
    "  quality_checks.append({\n",
    "      'observable': pauli,\n",
    "      'expected_value': expected,\n",
    "      'direct_expectation': direct,\n",
    "      'shadows_v0_expectation': v0_stats['expectation_value'],\n",
    "      'shadows_v1_expectation': v1_stats['expectation_value'],\n",
    "      'direct_delta': direct_delta,\n",
    "      'shadows_v0_delta': v0_delta,\n",
    "      'shadows_v1_delta': v1_delta,\n",
    "      'direct_within_warning_delta': direct_delta <= WARNING_DELTA,\n",
    "      'shadows_v0_within_warning_delta': v0_delta <= max(WARNING_DELTA, v0_stats.get('ci_width', 0.0)),\n",
    "      'shadows_v1_within_warning_delta': v1_delta <= max(WARNING_DELTA, v1_stats.get('ci_width', 0.0)),\n",
    "      'shadows_v0_ci_95': v0_ci,\n",
    "      'shadows_v1_ci_95': v1_ci,\n",
    "      'shadows_v0_ci_contains_expected': v0_ci_contains_expected,\n",
    "      'shadows_v1_ci_contains_expected': v1_ci_contains_expected,\n",
    "      'direct_vs_v0_delta': abs(direct - v0_stats['expectation_value']),\n",
    "      'direct_vs_v1_delta': abs(direct - v1_stats['expectation_value']),\n",
    "  })\n",
    "\n",
    "  status_flags = []\n",
    "  status_flags.append('direct OK' if direct_delta <= WARNING_DELTA else 'direct ⚠')\n",
    "  status_flags.append('v0 OK' if v0_delta <= WARNING_DELTA else 'v0 ⚠')\n",
    "  status_flags.append('v1 OK' if v1_delta <= WARNING_DELTA else 'v1 ⚠')\n",
    "  print(f\"Quality check {pauli}: \" + ', '.join(status_flags))\n",
    "\n",
    "def _git_output(args):\n",
    "  try:\n",
    "      return subprocess.check_output(['git', *args], text=True).strip()\n",
    "  except Exception:\n",
    "      return None\n",
    "\n",
    "git_commit = _git_output(['rev-parse', 'HEAD'])\n",
    "git_status = _git_output(['status', '--short'])\n",
    "if git_status == '':\n",
    "  git_status = 'clean'\n",
    "\n",
    "backend_snapshot_payload = getattr(snapshot, 'model_dump', None)\n",
    "if callable(backend_snapshot_payload):\n",
    "  backend_snapshot_dict = snapshot.model_dump(mode='json')\n",
    "else:\n",
    "  backend_snapshot_dict = {\n",
    "      'backend_name': snapshot.backend_name,\n",
    "      'backend_version': snapshot.backend_version,\n",
    "      'num_qubits': snapshot.num_qubits,\n",
    "      'calibration_timestamp': str(getattr(snapshot, 'calibration_timestamp', None)),\n",
    "  }\n",
    "\n",
    "smoke_test_results = {\n",
    "  'metadata': {\n",
    "      'test_name': 'preliminary_smoke_test',\n",
    "      'timestamp': datetime.now(UTC).isoformat(),\n",
    "      'backend': backend.name,\n",
    "      'backend_snapshot': backend_snapshot_dict,\n",
    "      'runtime_sampler_used': sampler is not None,\n",
    "      'runtime_execution_mode': execution_mode,\n",
    "      'git_commit': git_commit,\n",
    "      'git_status': git_status,\n",
    "      'software_versions': {\n",
    "          'quartumse': quartumse.__version__,\n",
    "          'qiskit': qiskit.__version__,\n",
    "          'python': platform.python_version(),\n",
    "      },\n",
    "      'transpile_options': TRANSPILE_OPTIONS,\n",
    "      'circuit': {\n",
    "          'type': 'Bell state',\n",
    "          'num_qubits': 2,\n",
    "      },\n",
    "      'observables': [str(obs) for obs in observables],\n",
    "  },\n",
    "  'direct_measurements': {\n",
    "      'method': 'Direct Pauli measurement with basis rotations',\n",
    "      'total_shots': sum(direct_results[k]['shots'] for k in direct_results),\n",
    "      'results': direct_results,\n",
    "  },\n",
    "  'shadows_v0': {\n",
    "      'method': 'Classical Shadows v0 (baseline)',\n",
    "      'experiment_id': result_v0.experiment_id,\n",
    "      'manifest_path': result_v0.manifest_path,\n",
    "      'shot_data_path': result_v0.shot_data_path,\n",
    "      'shadow_size': 500,\n",
    "      'execution_time': result_v0.execution_time,\n",
    "      'results': {\n",
    "          str(obs): {\n",
    "              'expectation_value': result_v0.observables[str(obs)]['expectation_value'],\n",
    "              'variance': result_v0.observables[str(obs)]['variance'],\n",
    "              'ci_95': result_v0.observables[str(obs)].get('ci_95'),\n",
    "              'ci_width': result_v0.observables[str(obs)]['ci_width'],\n",
    "          }\n",
    "          for obs in observables\n",
    "      },\n",
    "  },\n",
    "  'shadows_v1': {\n",
    "      'method': 'Classical Shadows v1 (noise-aware with MEM)',\n",
    "      'experiment_id': result_v1.experiment_id,\n",
    "      'manifest_path': result_v1.manifest_path,\n",
    "      'shot_data_path': result_v1.shot_data_path,\n",
    "      'shadow_size': 200,\n",
    "      'mem_shots': mem_shots,\n",
    "      'execution_time': result_v1.execution_time,\n",
    "      'mitigation_confusion_matrix_path': result_v1.mitigation_confusion_matrix_path,\n",
    "      'results': {\n",
    "          str(obs): {\n",
    "              'expectation_value': result_v1.observables[str(obs)]['expectation_value'],\n",
    "              'variance': result_v1.observables[str(obs)]['variance'],\n",
    "              'ci_95': result_v1.observables[str(obs)].get('ci_95'),\n",
    "              'ci_width': result_v1.observables[str(obs)]['ci_width'],\n",
    "          }\n",
    "          for obs in observables\n",
    "      },\n",
    "  },\n",
    "  'comparison': {\n",
    "      'description': 'Side-by-side comparison of all three methods',\n",
    "      'expected_values': EXPECTED_VALUES,\n",
    "      'table': summary_rows,\n",
    "  },\n",
    "  'analysis': {\n",
    "      'notes': [\n",
    "          'Bell state should show ZZ = XX = +1 (perfect correlation)',\n",
    "          'Noise and finite sampling cause deviations from ideal',\n",
    "          'v1 (noise-aware) should show reduced error on noisy hardware',\n",
    "          'All methods should give consistent results within confidence intervals',\n",
    "      ],\n",
    "      'quality_checks': quality_checks,\n",
    "      'thresholds': {\n",
    "          'catastrophic_delta': CATASTROPHIC_DELTA,\n",
    "          'warning_delta': WARNING_DELTA,\n",
    "      },\n",
    "  },\n",
    "}\n",
    "\n",
    "results_filename = f\"smoke_test_results_{datetime.now(UTC).strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "results_path = Path('validation_data') / results_filename\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "  json.dump(smoke_test_results, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "if not results_path.exists():\n",
    "  raise FileNotFoundError(f'Failed to create results JSON at {results_path}')\n",
    "\n",
    "print(f\"✓ Complete smoke test results saved to: {results_path}\")\n",
    "print(f\"  File size: {results_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"  Git commit: {git_commit}\")\n",
    "print(f\"  Runtime sampler used: {sampler is not None}\")\n",
    "print('Quality checks recorded for analysis JSON.')\n",
    "print(f\"  Thresholds -> catastrophic: {CATASTROPHIC_DELTA}, warning: {WARNING_DELTA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rxe5squzfwb",
   "metadata": {},
   "source": [
    "## Summary: What Gets Saved\n",
    "\n",
    "**When you run this notebook, the following data is automatically saved to `validation_data/`:**\n",
    "\n",
    "### 1. Shadow Estimator Manifests (JSON)\n",
    "- **Location**: `validation_data/manifests/{experiment_id}.json`\n",
    "- **Contains**: Complete provenance including circuit, backend calibration, mitigation config, software versions, results\n",
    "- **Files**: 2 manifests (one for v0, one for v1)\n",
    "- **Size**: ~10 KB each\n",
    "\n",
    "### 2. Shot Data (Parquet)\n",
    "- **Location**: `validation_data/shots/{experiment_id}.parquet`\n",
    "- **Contains**: All raw measurement outcomes (basis, bitstring) for replay\n",
    "- **Files**: 2 parquet files (one for v0, one for v1)\n",
    "- **Size**: ~50-100 KB each\n",
    "\n",
    "### 3. MEM Calibration Artifacts\n",
    "- **Location**: `validation_data/mem/{experiment_id}.npz`\n",
    "- **Contains**: Confusion matrix required for noise-aware replays\n",
    "- **Files**: 1 NPZ file when MEM is enabled\n",
    "- **Size**: ~5-20 KB\n",
    "\n",
    "### 4. Smoke Test Summary (JSON)\n",
    "- **Location**: `validation_data/smoke_test_results_{timestamp}.json`\n",
    "- **Contains**: All results from all three methods (direct, v0, v1), git metadata, transpilation options, quality checks\n",
    "- **Files**: 1 summary file per notebook run\n",
    "- **Size**: ~5-15 KB\n",
    "\n",
    "### 5. Direct Measurement Raw Counts\n",
    "- **Location**: Included in smoke test summary JSON\n",
    "- **Contains**: Full count dictionaries, compiled circuit QASM, execution mode\n",
    "\n",
    "**Total storage per smoke test run**: ~220-330 KB\n",
    "\n",
    "**To review later**:\n",
    "```python\n",
    "# Load smoke test summary\n",
    "import json\n",
    "from quartumse.shadows.core import Observable\n",
    "from quartumse import ShadowEstimator\n",
    "\n",
    "with open('validation_data/smoke_test_results_YYYYMMDD_HHMMSS.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Access any result\n",
    "print(results['shadows_v1']['results']['ZZ']['expectation_value'])\n",
    "\n",
    "# Load full provenance manifests\n",
    "from quartumse.reporting.manifest import ProvenanceManifest\n",
    "manifest = ProvenanceManifest.from_json(results['shadows_v1']['manifest_path'])\n",
    "\n",
    "# Replay with new observables (handles noise-aware MEM replays)\n",
    "estimator = ShadowEstimator(backend='aer_simulator', data_dir='validation_data')\n",
    "new_result = estimator.replay_from_manifest(\n",
    "    results['shadows_v1']['manifest_path'],\n",
    "    observables=[Observable('YY'), Observable('XZ')],\n",
    ")\n",
    "print(new_result.observables['YY']['expectation_value'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bhkn2kcetbu",
   "metadata": {},
   "source": [
    "## Save Complete Results for Later Review\n",
    "\n",
    "Export all results (direct, v0, v1, comparison) to JSON for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2e8d56-621b-47ad-ace1-dc401b1454f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}