{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"QuartumSE Documentation \u00b6 Welcome to the QuartumSE documentation site. This MkDocs-powered hub follows the same narrative as the repository: Install & verify the SDK locally. Run baseline experiments on Aer or IBM Runtime. Automate campaigns with reporting and provenance capture. Dig into architecture and theory as you extend the platform. Use the navigation to jump directly to tutorials, how-to guides, explanation notes, and operational runbooks. Each section is kept intentionally focused so new contributors can follow a linear path from installation to production-grade experiments. Looking for API details? Head to the Reference section. Need the strategic context? See the Roadmap and Project Bible .","title":"Home"},{"location":"#quartumse-documentation","text":"Welcome to the QuartumSE documentation site. This MkDocs-powered hub follows the same narrative as the repository: Install & verify the SDK locally. Run baseline experiments on Aer or IBM Runtime. Automate campaigns with reporting and provenance capture. Dig into architecture and theory as you extend the platform. Use the navigation to jump directly to tutorials, how-to guides, explanation notes, and operational runbooks. Each section is kept intentionally focused so new contributors can follow a linear path from installation to production-grade experiments. Looking for API details? Head to the Reference section. Need the strategic context? See the Roadmap and Project Bible .","title":"QuartumSE Documentation"},{"location":"archive/","text":"Archive \u00b6 Historical documentation and experiment starter scripts have been removed as of October 2025 to streamline the repository and improve navigation for new users. What was removed \u00b6 Documentation Archives \u00b6 docs/archive/bootstrap_summary_20251020.md - Bootstrap phase summary docs/archive/status_report_20251022.md - Phase 1 status report docs/archive/strategic_analysis_20251021.md - Strategic analysis Experiment Starter Scripts \u00b6 experiments/archive/benchmarking/B_T01_rb_starter.py - Randomized benchmarking starter experiments/archive/chemistry/C_T01_h2_vqe_starter.py - H\u2082 VQE starter experiments/archive/metrology/M_T01_ghz_phase_starter.py - GHZ phase metrology starter experiments/archive/optimization/O_T01_maxcut_starter.py - MaxCut optimization starter Archived Notebooks \u00b6 notebooks/archive/preliminary_smoke_test.ipynb - Early smoke test (superseded by notebooks/comprehensive_test_suite.ipynb ) notebooks/archive/review_smoke_test_results.ipynb - Smoke test analysis notebooks/archive/s_t01_ghz_classical_shadows.ipynb - Old S-T01 notebook Accessing archived content \u00b6 All removed files remain accessible in the Git history. To retrieve archived content: View list of archived files: git show fa5e756 --name-only --diff-filter = D Restore a specific archived file to your working directory: # Example: restore the bootstrap summary git show fa5e756:docs/archive/bootstrap_summary_20251020.md > bootstrap_summary.md Check out the repository state before archive removal: git checkout fa5e756^ # One commit before removal Browse archived files on GitHub: Visit the commit directly: fa5e756 Current documentation structure \u00b6 For up-to-date documentation, see: Documentation index - Complete navigation guide Tutorials - Getting started guides How-to guides - Task-oriented walkthroughs Explanation - Deep dives into architecture and theory Strategy - Project roadmap and planning Superseding resources \u00b6 Content from archived materials has been integrated into current documentation: Bootstrap & status reports \u2192 Phase 1 Task Checklist Strategic analysis \u2192 Project Bible and Roadmap Starter experiments \u2192 Shadow experiments directory in repository root ( experiments/shadows/ ) Old notebooks \u2192 Current notebooks in repository root ( notebooks/ ) with comprehensive test suite If you need specific information from archived files that isn't covered in current docs, please open a discussion or file an issue .","title":"Archive"},{"location":"archive/#archive","text":"Historical documentation and experiment starter scripts have been removed as of October 2025 to streamline the repository and improve navigation for new users.","title":"Archive"},{"location":"archive/#what-was-removed","text":"","title":"What was removed"},{"location":"archive/#documentation-archives","text":"docs/archive/bootstrap_summary_20251020.md - Bootstrap phase summary docs/archive/status_report_20251022.md - Phase 1 status report docs/archive/strategic_analysis_20251021.md - Strategic analysis","title":"Documentation Archives"},{"location":"archive/#experiment-starter-scripts","text":"experiments/archive/benchmarking/B_T01_rb_starter.py - Randomized benchmarking starter experiments/archive/chemistry/C_T01_h2_vqe_starter.py - H\u2082 VQE starter experiments/archive/metrology/M_T01_ghz_phase_starter.py - GHZ phase metrology starter experiments/archive/optimization/O_T01_maxcut_starter.py - MaxCut optimization starter","title":"Experiment Starter Scripts"},{"location":"archive/#archived-notebooks","text":"notebooks/archive/preliminary_smoke_test.ipynb - Early smoke test (superseded by notebooks/comprehensive_test_suite.ipynb ) notebooks/archive/review_smoke_test_results.ipynb - Smoke test analysis notebooks/archive/s_t01_ghz_classical_shadows.ipynb - Old S-T01 notebook","title":"Archived Notebooks"},{"location":"archive/#accessing-archived-content","text":"All removed files remain accessible in the Git history. To retrieve archived content: View list of archived files: git show fa5e756 --name-only --diff-filter = D Restore a specific archived file to your working directory: # Example: restore the bootstrap summary git show fa5e756:docs/archive/bootstrap_summary_20251020.md > bootstrap_summary.md Check out the repository state before archive removal: git checkout fa5e756^ # One commit before removal Browse archived files on GitHub: Visit the commit directly: fa5e756","title":"Accessing archived content"},{"location":"archive/#current-documentation-structure","text":"For up-to-date documentation, see: Documentation index - Complete navigation guide Tutorials - Getting started guides How-to guides - Task-oriented walkthroughs Explanation - Deep dives into architecture and theory Strategy - Project roadmap and planning","title":"Current documentation structure"},{"location":"archive/#superseding-resources","text":"Content from archived materials has been integrated into current documentation: Bootstrap & status reports \u2192 Phase 1 Task Checklist Strategic analysis \u2192 Project Bible and Roadmap Starter experiments \u2192 Shadow experiments directory in repository root ( experiments/shadows/ ) Old notebooks \u2192 Current notebooks in repository root ( notebooks/ ) with comprehensive test suite If you need specific information from archived files that isn't covered in current docs, please open a discussion or file an issue .","title":"Superseding resources"},{"location":"explanation/architecture/","text":"Architecture \u00b6 An overview of the QuartumSE system architecture will be provided here.","title":"Architecture"},{"location":"explanation/architecture/#architecture","text":"An overview of the QuartumSE system architecture will be provided here.","title":"Architecture"},{"location":"explanation/manifest-schema/","text":"QuartumSE Data Storage Conventions \u00b6 This document explains the data directory structure and when to use each directory. By default, ShadowEstimator writes manifests and Parquet files under ./data unless you override data_dir . Directory Overview \u00b6 QuartumSE/ \u251c\u2500\u2500 data/ # Production experiment data \u251c\u2500\u2500 validation_data/ # Phase 1 validation & smoke tests \u251c\u2500\u2500 demo_data/ # Notebook demos & tutorials \u251c\u2500\u2500 notebook_data/ # Interactive notebook experimentation \u2514\u2500\u2500 experiments/validation/archived_runs/ # Archived validation results When to Use Each Directory \u00b6 data/ - Production Experiments \u00b6 Use for: - Final, production-quality experiments - Data intended for publication or reports - Long-term archival - Experiments in workstreams C/O/B/M (Chemistry, Optimization, Benchmarking, Metrology) Examples: estimator = ShadowEstimator ( backend = \"ibm:ibm_torino\" , data_dir = \"data\" ) Retention: Keep indefinitely, archive carefully validation_data/ - Phase 1 Validation & Testing \u00b6 Use for: - Hardware validation experiments (S-T01, S-T02, etc.) - Smoke tests on IBM hardware - SSR verification experiments - Phase 1 exit criteria validation - Shadow experiment scripts ( experiments/shadows/*/run_*.py ) Scripts/notebooks that use this: - experiments/shadows/preliminary_test/run_smoke_test.py - experiments/validation/hardware_validation.py - experiments/shadows/extended_ghz/run_ghz_extended.py - experiments/shadows/parallel_bell_pairs/run_bell_pairs.py - Hardware sections of notebooks/comprehensive_test_suite.ipynb Examples: estimator = ShadowEstimator ( backend = \"ibm:ibm_torino\" , data_dir = \"validation_data\" ) Retention: - Keep during Phase 1 validation period - Archive successful runs to experiments/validation/archived_runs/ - Move final validation results to data/ for publication demo_data/ - Demos & Tutorials \u00b6 Use for: - Notebook demonstrations - Quickstart examples - Tutorial walkthroughs - Non-critical testing Notebooks that use this: - notebooks/quickstart_shot_persistence.ipynb - notebooks/comprehensive_test_suite.ipynb - notebooks/noise_aware_shadows_demo.ipynb Examples: estimator = ShadowEstimator ( backend = AerSimulator (), data_dir = \"demo_data\" ) Retention: Ephemeral - safe to delete at any time notebook_data/ - Interactive Notebooks \u00b6 Use for: - Jupyter notebook experimentation - Development and debugging - Exploratory data analysis - One-off interactive tests Examples: estimator = ShadowEstimator ( backend = \"ibm:ibm_torino\" , data_dir = \"notebook_data\" ) Retention: Ephemeral - safe to delete at any time Directory Structure \u00b6 All data directories follow the same subdirectory structure: {data_dir}/ \u251c\u2500\u2500 manifests/ # Provenance manifests (JSON) \u2502 \u2514\u2500\u2500 {experiment_id}.json \u251c\u2500\u2500 shots/ # Raw measurement data (Parquet) \u2502 \u2514\u2500\u2500 {experiment_id}.parquet \u251c\u2500\u2500 reports/ # Generated reports (HTML/PDF) \u2502 \u2514\u2500\u2500 {experiment_id}_report.html \u2514\u2500\u2500 calibrations/ # MEM confusion matrices (optional) \u2514\u2500\u2500 {experiment_id}_confusion.json See data/README.md in the repository for detailed schema documentation. Git Tracking \u00b6 All data directories are git-ignored except for: - \u2705 README.md files (documentation) - \u2705 .gitkeep files (preserve empty directories) - \u274c JSON manifests (ignored) - \u274c Parquet shot data (ignored) - \u274c HTML/PDF reports (ignored) Reason: Experimental data files are large and change frequently. Only code and documentation are version-controlled. Quick Commands \u00b6 Create all directories: \u00b6 mkdir -p data/ { manifests,shots,reports,calibrations } mkdir -p validation_data/ { manifests,shots,reports,calibrations } mkdir -p demo_data mkdir -p notebook_data Clean validation data: \u00b6 rm -rf validation_data/ { manifests,shots,reports,calibrations } /* Clean demo/notebook data: \u00b6 rm -rf demo_data/* notebook_data/* List all experiments by directory: \u00b6 ls -lh data/manifests/ # Production ls -lh validation_data/manifests/ # Validation ls -lh demo_data/manifests/ # Demos Check total data usage: \u00b6 du -sh data/ validation_data/ demo_data/ notebook_data/ Storage Estimates \u00b6 Phase 1 (6 validation experiments): \u00b6 validation_data/ : ~1.6 MB data/ : ~0 MB (not used yet) Phase 2+ (Production workloads): \u00b6 data/ : ~50-100 MB per workstream (C/O/B/M) validation_data/ : Archived after Phase 1 Per Experiment: \u00b6 Manifest: ~10 KB Shot data (500 shots, 3 qubits): ~50-100 KB Report: ~50 KB Total per experiment : ~100-200 KB Best Practices \u00b6 Always specify data_dir explicitly in scripts: # Good estimator = ShadowEstimator ( ... , data_dir = \"validation_data\" ) # Bad (default may change) estimator = ShadowEstimator ( ... ) Use appropriate directory for purpose : Production \u2192 data/ Validation/testing \u2192 validation_data/ Demos \u2192 demo_data/ Notebooks \u2192 notebook_data/ Archive validation results after Phase 1: cp validation_data/manifests/final_validation.json \\ experiments/validation/archived_runs/results_final.txt Clean temporary directories regularly : # Weekly cleanup rm -rf demo_data/* notebook_data/* Check storage usage before long experiment runs: df -h . # Check available disk space du -sh validation_data/ # Check current usage FAQ \u00b6 Q: Which directory should I use for the preliminary smoke test? A: validation_data/ - it's part of Phase 1 validation. Q: Can I move experiments between directories? A: Yes! Manifests and shot data are self-contained. Just copy/move the files: mv validation_data/manifests/ { id } .json data/manifests/ mv validation_data/shots/ { id } .parquet data/shots/ Q: What if I accidentally delete a data directory? A: All directories auto-create subdirectories when needed. Just re-run your experiment or manually recreate: mkdir -p validation_data/ { manifests,shots,reports,calibrations } Q: How do I back up my validation data? A: Copy the entire directory: cp -r validation_data/ validation_data_backup_ $( date +%Y%m%d ) Q: Why are Parquet files ignored by git? A: They're binary files that can be large (MBs) and change frequently. Git is optimized for text files (code, docs). Last Updated: 2025-10-22 QuartumSE Version: 0.1.0","title":"Manifest Schema"},{"location":"explanation/manifest-schema/#quartumse-data-storage-conventions","text":"This document explains the data directory structure and when to use each directory. By default, ShadowEstimator writes manifests and Parquet files under ./data unless you override data_dir .","title":"QuartumSE Data Storage Conventions"},{"location":"explanation/manifest-schema/#directory-overview","text":"QuartumSE/ \u251c\u2500\u2500 data/ # Production experiment data \u251c\u2500\u2500 validation_data/ # Phase 1 validation & smoke tests \u251c\u2500\u2500 demo_data/ # Notebook demos & tutorials \u251c\u2500\u2500 notebook_data/ # Interactive notebook experimentation \u2514\u2500\u2500 experiments/validation/archived_runs/ # Archived validation results","title":"Directory Overview"},{"location":"explanation/manifest-schema/#when-to-use-each-directory","text":"","title":"When to Use Each Directory"},{"location":"explanation/manifest-schema/#data-production-experiments","text":"Use for: - Final, production-quality experiments - Data intended for publication or reports - Long-term archival - Experiments in workstreams C/O/B/M (Chemistry, Optimization, Benchmarking, Metrology) Examples: estimator = ShadowEstimator ( backend = \"ibm:ibm_torino\" , data_dir = \"data\" ) Retention: Keep indefinitely, archive carefully","title":"data/ - Production Experiments"},{"location":"explanation/manifest-schema/#validation_data-phase-1-validation-testing","text":"Use for: - Hardware validation experiments (S-T01, S-T02, etc.) - Smoke tests on IBM hardware - SSR verification experiments - Phase 1 exit criteria validation - Shadow experiment scripts ( experiments/shadows/*/run_*.py ) Scripts/notebooks that use this: - experiments/shadows/preliminary_test/run_smoke_test.py - experiments/validation/hardware_validation.py - experiments/shadows/extended_ghz/run_ghz_extended.py - experiments/shadows/parallel_bell_pairs/run_bell_pairs.py - Hardware sections of notebooks/comprehensive_test_suite.ipynb Examples: estimator = ShadowEstimator ( backend = \"ibm:ibm_torino\" , data_dir = \"validation_data\" ) Retention: - Keep during Phase 1 validation period - Archive successful runs to experiments/validation/archived_runs/ - Move final validation results to data/ for publication","title":"validation_data/ - Phase 1 Validation &amp; Testing"},{"location":"explanation/manifest-schema/#demo_data-demos-tutorials","text":"Use for: - Notebook demonstrations - Quickstart examples - Tutorial walkthroughs - Non-critical testing Notebooks that use this: - notebooks/quickstart_shot_persistence.ipynb - notebooks/comprehensive_test_suite.ipynb - notebooks/noise_aware_shadows_demo.ipynb Examples: estimator = ShadowEstimator ( backend = AerSimulator (), data_dir = \"demo_data\" ) Retention: Ephemeral - safe to delete at any time","title":"demo_data/ - Demos &amp; Tutorials"},{"location":"explanation/manifest-schema/#notebook_data-interactive-notebooks","text":"Use for: - Jupyter notebook experimentation - Development and debugging - Exploratory data analysis - One-off interactive tests Examples: estimator = ShadowEstimator ( backend = \"ibm:ibm_torino\" , data_dir = \"notebook_data\" ) Retention: Ephemeral - safe to delete at any time","title":"notebook_data/ - Interactive Notebooks"},{"location":"explanation/manifest-schema/#directory-structure","text":"All data directories follow the same subdirectory structure: {data_dir}/ \u251c\u2500\u2500 manifests/ # Provenance manifests (JSON) \u2502 \u2514\u2500\u2500 {experiment_id}.json \u251c\u2500\u2500 shots/ # Raw measurement data (Parquet) \u2502 \u2514\u2500\u2500 {experiment_id}.parquet \u251c\u2500\u2500 reports/ # Generated reports (HTML/PDF) \u2502 \u2514\u2500\u2500 {experiment_id}_report.html \u2514\u2500\u2500 calibrations/ # MEM confusion matrices (optional) \u2514\u2500\u2500 {experiment_id}_confusion.json See data/README.md in the repository for detailed schema documentation.","title":"Directory Structure"},{"location":"explanation/manifest-schema/#git-tracking","text":"All data directories are git-ignored except for: - \u2705 README.md files (documentation) - \u2705 .gitkeep files (preserve empty directories) - \u274c JSON manifests (ignored) - \u274c Parquet shot data (ignored) - \u274c HTML/PDF reports (ignored) Reason: Experimental data files are large and change frequently. Only code and documentation are version-controlled.","title":"Git Tracking"},{"location":"explanation/manifest-schema/#quick-commands","text":"","title":"Quick Commands"},{"location":"explanation/manifest-schema/#create-all-directories","text":"mkdir -p data/ { manifests,shots,reports,calibrations } mkdir -p validation_data/ { manifests,shots,reports,calibrations } mkdir -p demo_data mkdir -p notebook_data","title":"Create all directories:"},{"location":"explanation/manifest-schema/#clean-validation-data","text":"rm -rf validation_data/ { manifests,shots,reports,calibrations } /*","title":"Clean validation data:"},{"location":"explanation/manifest-schema/#clean-demonotebook-data","text":"rm -rf demo_data/* notebook_data/*","title":"Clean demo/notebook data:"},{"location":"explanation/manifest-schema/#list-all-experiments-by-directory","text":"ls -lh data/manifests/ # Production ls -lh validation_data/manifests/ # Validation ls -lh demo_data/manifests/ # Demos","title":"List all experiments by directory:"},{"location":"explanation/manifest-schema/#check-total-data-usage","text":"du -sh data/ validation_data/ demo_data/ notebook_data/","title":"Check total data usage:"},{"location":"explanation/manifest-schema/#storage-estimates","text":"","title":"Storage Estimates"},{"location":"explanation/manifest-schema/#phase-1-6-validation-experiments","text":"validation_data/ : ~1.6 MB data/ : ~0 MB (not used yet)","title":"Phase 1 (6 validation experiments):"},{"location":"explanation/manifest-schema/#phase-2-production-workloads","text":"data/ : ~50-100 MB per workstream (C/O/B/M) validation_data/ : Archived after Phase 1","title":"Phase 2+ (Production workloads):"},{"location":"explanation/manifest-schema/#per-experiment","text":"Manifest: ~10 KB Shot data (500 shots, 3 qubits): ~50-100 KB Report: ~50 KB Total per experiment : ~100-200 KB","title":"Per Experiment:"},{"location":"explanation/manifest-schema/#best-practices","text":"Always specify data_dir explicitly in scripts: # Good estimator = ShadowEstimator ( ... , data_dir = \"validation_data\" ) # Bad (default may change) estimator = ShadowEstimator ( ... ) Use appropriate directory for purpose : Production \u2192 data/ Validation/testing \u2192 validation_data/ Demos \u2192 demo_data/ Notebooks \u2192 notebook_data/ Archive validation results after Phase 1: cp validation_data/manifests/final_validation.json \\ experiments/validation/archived_runs/results_final.txt Clean temporary directories regularly : # Weekly cleanup rm -rf demo_data/* notebook_data/* Check storage usage before long experiment runs: df -h . # Check available disk space du -sh validation_data/ # Check current usage","title":"Best Practices"},{"location":"explanation/manifest-schema/#faq","text":"Q: Which directory should I use for the preliminary smoke test? A: validation_data/ - it's part of Phase 1 validation. Q: Can I move experiments between directories? A: Yes! Manifests and shot data are self-contained. Just copy/move the files: mv validation_data/manifests/ { id } .json data/manifests/ mv validation_data/shots/ { id } .parquet data/shots/ Q: What if I accidentally delete a data directory? A: All directories auto-create subdirectories when needed. Just re-run your experiment or manually recreate: mkdir -p validation_data/ { manifests,shots,reports,calibrations } Q: How do I back up my validation data? A: Copy the entire directory: cp -r validation_data/ validation_data_backup_ $( date +%Y%m%d ) Q: Why are Parquet files ignored by git? A: They're binary files that can be large (MBs) and change frequently. Git is optimized for text files (code, docs). Last Updated: 2025-10-22 QuartumSE Version: 0.1.0","title":"FAQ"},{"location":"explanation/shadows-theory/","text":"Shadows Theory \u00b6 Background theory and rationale for QuartumSE shadow experiments will be described on this page.","title":"Shadows Theory"},{"location":"explanation/shadows-theory/#shadows-theory","text":"Background theory and rationale for QuartumSE shadow experiments will be described on this page.","title":"Shadows Theory"},{"location":"how-to/generate-report/","text":"Generate Experiment Reports \u00b6 QuartumSE can generate self-contained HTML reports from saved manifests, providing human-readable experiment summaries with full provenance details. Overview \u00b6 Report contents: - Experiment metadata (ID, timestamp, versions) - Circuit visualization and fingerprint - Backend calibration snapshot - Observable estimates with confidence intervals - Shot data diagnostics (basis distribution, bitstring histogram) - Mitigation configuration (MEM, ZNE settings) - Resource usage (shots, execution time) Formats: - HTML (default) - Self-contained, viewable in any browser - PDF (future) - Requires weasyprint (not yet implemented) Quick Start \u00b6 Generate HTML Report (CLI) \u00b6 Unix/macOS: quartumse report data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json \\ --output reports/experiment_report.html Windows: quartumse report data / manifests / a3f2b1c4 - 5678 - 90ab-cdef - 1234567890ab . json ` - -output reports / experiment_report . html Output: Generating report from data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json... Report saved to reports/experiment_report.html Open in browser: Unix/macOS: open reports/experiment_report.html Windows: Start-Process reports / experiment_report . html Linux: xdg-open reports/experiment_report.html Using Python API \u00b6 Basic Report Generation \u00b6 from quartumse.reporting import ReportGenerator # Load manifest and generate report report = ReportGenerator . from_manifest_file ( \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" ) # Save as HTML report . to_html ( \"reports/experiment_report.html\" ) print ( \"Report generated successfully!\" ) Customizing Report Content \u00b6 from quartumse.reporting import ReportGenerator from pathlib import Path manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" # Create report with custom title report = ReportGenerator . from_manifest_file ( manifest_path ) report . title = \"GHZ State Validation - Phase 1\" report . description = \"Classical shadows v0 baseline on 3-qubit GHZ state\" # Add custom metadata report . add_metadata ( \"Experiment Group\" , \"Phase 1 Validation\" ) report . add_metadata ( \"Researcher\" , \"QuartumSE Team\" ) report . add_metadata ( \"Status\" , \"PASSED \u2713\" ) # Generate output output_path = Path ( \"reports/ghz_phase1_report.html\" ) output_path . parent . mkdir ( parents = True , exist_ok = True ) report . to_html ( str ( output_path )) print ( f \"Custom report saved: { output_path } \" ) Batch Report Generation \u00b6 Generate reports for all experiments in a directory: Unix/macOS: for manifest in data/manifests/*.json ; do base = $( basename \" $manifest \" .json ) quartumse report \" $manifest \" --output \"reports/ ${ base } _report.html\" echo \"Generated: reports/ ${ base } _report.html\" done Windows (PowerShell): Get-ChildItem data / manifests /*. json | ForEach -Object { $base = $_ . BaseName quartumse report $_ . FullName - -output \"reports/${base}_report.html\" Write-Host \"Generated: reports/${base}_report.html\" } Python script: from pathlib import Path from quartumse.reporting import ReportGenerator manifest_dir = Path ( \"data/manifests\" ) report_dir = Path ( \"reports\" ) report_dir . mkdir ( parents = True , exist_ok = True ) for manifest_path in sorted ( manifest_dir . glob ( \"*.json\" )): try : report = ReportGenerator . from_manifest_file ( str ( manifest_path )) output_name = f \" { manifest_path . stem } _report.html\" output_path = report_dir / output_name report . to_html ( str ( output_path )) print ( f \"\u2713 Generated: { output_name } \" ) except Exception as e : print ( f \"\u2717 Failed: { manifest_path . name } - { e } \" ) print ( f \" \\n Total reports generated: { len ( list ( report_dir . glob ( '*.html' ))) } \" ) Report Structure \u00b6 Section 1: Experiment Overview \u00b6 Experiment ID : Unique UUID for traceability Created : ISO timestamp Backend : Device name and qubit count Shadow Version : v0 (baseline), v1 (noise-aware), etc. Shadow Size : Number of random measurements Execution Time : Wall-clock time for quantum execution Section 2: Circuit Details \u00b6 Num Qubits : Circuit width Depth : Gate depth Gate Counts : Breakdown by gate type (H, CNOT, etc.) Circuit Hash : SHA-256 fingerprint for reproducibility Circuit Visualization : (if available) Section 3: Observable Results \u00b6 Table with columns: - Observable : Pauli string (e.g., ZII , ZZZ ) - Expectation Value : Estimated mean - Variance : Estimation variance - 95% CI : Confidence interval bounds - CI Width : Interval width (precision measure) Section 4: Shot Data Diagnostics \u00b6 Measurement Basis Distribution: - Histogram showing X/Y/Z measurement frequencies per qubit - Should be uniform (~33% each) for random Clifford shadows Top Bitstrings: - Most frequent measurement outcomes - Useful for debugging unexpected distributions Qubit Marginals: - Per-qubit P(|0\u27e9) probabilities - Should be ~0.5 for maximally entangled states Section 5: Mitigation Configuration \u00b6 Techniques : MEM, ZNE, etc. MEM Confusion Matrix Path : (if applicable) MEM Shots per State : Calibration budget Qubits Calibrated : Indices of calibrated qubits Section 6: Backend Calibration \u00b6 Calibration Timestamp : When device properties were captured T1 Times : (if available) Decay times per qubit T2 Times : (if available) Dephasing times per qubit Readout Errors : (if available) Measurement error rates Properties Hash : SHA-256 of full calibration JSON Section 7: Provenance \u00b6 QuartumSE Version : Package version used Qiskit Version : Qiskit version used Python Version : Python interpreter version Random Seed : (if set) For reproducibility Tags : User-defined searchable tags Report Output Locations \u00b6 Default Behavior \u00b6 When using the experiment API ( estimator.estimate(save_manifest=True) ), no report is auto-generated. Use the CLI or Python API to generate reports from saved manifests. If you ran experiments with a custom --data-dir , substitute that path for the default data/ directory referenced below. Recommended Structure \u00b6 quartumse/ \u251c\u2500\u2500 data/ \u2502 \u251c\u2500\u2500 manifests/ # JSON manifests \u2502 \u2502 \u251c\u2500\u2500 a3f2b1c4-....json \u2502 \u2502 \u2514\u2500\u2500 b5e8f9d2-....json \u2502 \u251c\u2500\u2500 shots/ # Parquet shot data \u2502 \u2502 \u251c\u2500\u2500 a3f2b1c4-....parquet \u2502 \u2502 \u2514\u2500\u2500 b5e8f9d2-....parquet \u2502 \u2514\u2500\u2500 mem/ # MEM confusion matrices \u2502 \u251c\u2500\u2500 a3f2b1c4-....npz \u2502 \u2514\u2500\u2500 b5e8f9d2-....npz \u2514\u2500\u2500 reports/ # HTML reports (gitignored) \u251c\u2500\u2500 a3f2b1c4-...._report.html \u2514\u2500\u2500 b5e8f9d2-...._report.html Advanced: Programmatic Report Analysis \u00b6 Extract key metrics from reports for comparison: import json from pathlib import Path import pandas as pd def extract_metrics_from_manifest ( manifest_path ): \"\"\"Extract key metrics from manifest JSON.\"\"\" with open ( manifest_path , 'r' ) as f : manifest = json . load ( f ) metrics = { 'experiment_id' : manifest [ 'experiment_id' ], 'backend' : manifest [ 'backend' ][ 'backend_name' ], 'shadow_size' : manifest [ 'shadows' ][ 'shadow_size' ], 'shadow_version' : manifest [ 'shadows' ][ 'version' ], 'num_qubits' : manifest [ 'circuit' ][ 'num_qubits' ], 'execution_time' : manifest . get ( 'resource_usage' , {}) . get ( 'quantum_execution_seconds' , None ), } # Extract observable results for obs_str , obs_data in manifest . get ( 'results_summary' , {}) . items (): metrics [ f 'obs_ { obs_str } _expectation' ] = obs_data . get ( 'expectation_value' ) metrics [ f 'obs_ { obs_str } _ci_width' ] = obs_data . get ( 'ci_width' ) return metrics # Process all manifests manifest_dir = Path ( \"data/manifests\" ) all_metrics = [ extract_metrics_from_manifest ( p ) for p in manifest_dir . glob ( \"*.json\" )] # Convert to DataFrame for analysis df = pd . DataFrame ( all_metrics ) # Compare by backend print ( \" \\n Metrics by Backend:\" ) print ( df . groupby ( 'backend' )[ 'shadow_size' ] . agg ([ 'mean' , 'min' , 'max' , 'count' ])) # Compare by shadow version print ( \" \\n Metrics by Shadow Version:\" ) print ( df . groupby ( 'shadow_version' )[ 'execution_time' ] . agg ([ 'mean' , 'median' , 'std' ])) PDF Reports (Future) \u00b6 PDF generation is planned for Phase 2. It will require the weasyprint package: Installation (when implemented): pip install quartumse [ reporting ] Usage (planned API): from quartumse.reporting import ReportGenerator report = ReportGenerator . from_manifest_file ( \"data/manifests/experiment.json\" ) report . to_pdf ( \"reports/experiment_report.pdf\" ) # Not yet implemented Current workaround: 1. Generate HTML report 2. Open in browser 3. Print to PDF using browser's print dialog Troubleshooting \u00b6 \"Manifest file not found\" - Check path is correct (absolute or relative) - Verify experiment completed and saved manifest - Use ls data/manifests/ to list available manifests \"cannot import name 'generate_html_report'\" - Use ReportGenerator class instead (new API) - Old function name removed in favor of class-based API - Update code: ReportGenerator.from_manifest_file(path).to_html(output) \"Shot data file not found\" - Report includes shot diagnostics if shot data exists - Gracefully degrades if shot data missing (shows warning) - Check manifest['shot_data_path'] points to valid Parquet file \"Template not found\" - Report templates bundled with package - If error persists, reinstall: pip install --force-reinstall quartumse \"Report HTML not rendering properly\" - Open in modern browser (Chrome, Firefox, Safari, Edge) - Check for JavaScript errors in browser console - Verify HTML file wasn't corrupted during generation \"Report generation slow (>5 seconds)\" - Check shot data file size (should be <10MB for typical experiments) - Verify no network I/O (all files should be local) - Report generation should complete in <1 second normally Related \u00b6 Run S-T01 GHZ - Generate manifests worth reporting Replay from Manifest - Re-analyze before reporting Manifest Schema - Full manifest specification","title":"Generate Report"},{"location":"how-to/generate-report/#generate-experiment-reports","text":"QuartumSE can generate self-contained HTML reports from saved manifests, providing human-readable experiment summaries with full provenance details.","title":"Generate Experiment Reports"},{"location":"how-to/generate-report/#overview","text":"Report contents: - Experiment metadata (ID, timestamp, versions) - Circuit visualization and fingerprint - Backend calibration snapshot - Observable estimates with confidence intervals - Shot data diagnostics (basis distribution, bitstring histogram) - Mitigation configuration (MEM, ZNE settings) - Resource usage (shots, execution time) Formats: - HTML (default) - Self-contained, viewable in any browser - PDF (future) - Requires weasyprint (not yet implemented)","title":"Overview"},{"location":"how-to/generate-report/#quick-start","text":"","title":"Quick Start"},{"location":"how-to/generate-report/#generate-html-report-cli","text":"Unix/macOS: quartumse report data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json \\ --output reports/experiment_report.html Windows: quartumse report data / manifests / a3f2b1c4 - 5678 - 90ab-cdef - 1234567890ab . json ` - -output reports / experiment_report . html Output: Generating report from data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json... Report saved to reports/experiment_report.html Open in browser: Unix/macOS: open reports/experiment_report.html Windows: Start-Process reports / experiment_report . html Linux: xdg-open reports/experiment_report.html","title":"Generate HTML Report (CLI)"},{"location":"how-to/generate-report/#using-python-api","text":"","title":"Using Python API"},{"location":"how-to/generate-report/#basic-report-generation","text":"from quartumse.reporting import ReportGenerator # Load manifest and generate report report = ReportGenerator . from_manifest_file ( \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" ) # Save as HTML report . to_html ( \"reports/experiment_report.html\" ) print ( \"Report generated successfully!\" )","title":"Basic Report Generation"},{"location":"how-to/generate-report/#customizing-report-content","text":"from quartumse.reporting import ReportGenerator from pathlib import Path manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" # Create report with custom title report = ReportGenerator . from_manifest_file ( manifest_path ) report . title = \"GHZ State Validation - Phase 1\" report . description = \"Classical shadows v0 baseline on 3-qubit GHZ state\" # Add custom metadata report . add_metadata ( \"Experiment Group\" , \"Phase 1 Validation\" ) report . add_metadata ( \"Researcher\" , \"QuartumSE Team\" ) report . add_metadata ( \"Status\" , \"PASSED \u2713\" ) # Generate output output_path = Path ( \"reports/ghz_phase1_report.html\" ) output_path . parent . mkdir ( parents = True , exist_ok = True ) report . to_html ( str ( output_path )) print ( f \"Custom report saved: { output_path } \" )","title":"Customizing Report Content"},{"location":"how-to/generate-report/#batch-report-generation","text":"Generate reports for all experiments in a directory: Unix/macOS: for manifest in data/manifests/*.json ; do base = $( basename \" $manifest \" .json ) quartumse report \" $manifest \" --output \"reports/ ${ base } _report.html\" echo \"Generated: reports/ ${ base } _report.html\" done Windows (PowerShell): Get-ChildItem data / manifests /*. json | ForEach -Object { $base = $_ . BaseName quartumse report $_ . FullName - -output \"reports/${base}_report.html\" Write-Host \"Generated: reports/${base}_report.html\" } Python script: from pathlib import Path from quartumse.reporting import ReportGenerator manifest_dir = Path ( \"data/manifests\" ) report_dir = Path ( \"reports\" ) report_dir . mkdir ( parents = True , exist_ok = True ) for manifest_path in sorted ( manifest_dir . glob ( \"*.json\" )): try : report = ReportGenerator . from_manifest_file ( str ( manifest_path )) output_name = f \" { manifest_path . stem } _report.html\" output_path = report_dir / output_name report . to_html ( str ( output_path )) print ( f \"\u2713 Generated: { output_name } \" ) except Exception as e : print ( f \"\u2717 Failed: { manifest_path . name } - { e } \" ) print ( f \" \\n Total reports generated: { len ( list ( report_dir . glob ( '*.html' ))) } \" )","title":"Batch Report Generation"},{"location":"how-to/generate-report/#report-structure","text":"","title":"Report Structure"},{"location":"how-to/generate-report/#section-1-experiment-overview","text":"Experiment ID : Unique UUID for traceability Created : ISO timestamp Backend : Device name and qubit count Shadow Version : v0 (baseline), v1 (noise-aware), etc. Shadow Size : Number of random measurements Execution Time : Wall-clock time for quantum execution","title":"Section 1: Experiment Overview"},{"location":"how-to/generate-report/#section-2-circuit-details","text":"Num Qubits : Circuit width Depth : Gate depth Gate Counts : Breakdown by gate type (H, CNOT, etc.) Circuit Hash : SHA-256 fingerprint for reproducibility Circuit Visualization : (if available)","title":"Section 2: Circuit Details"},{"location":"how-to/generate-report/#section-3-observable-results","text":"Table with columns: - Observable : Pauli string (e.g., ZII , ZZZ ) - Expectation Value : Estimated mean - Variance : Estimation variance - 95% CI : Confidence interval bounds - CI Width : Interval width (precision measure)","title":"Section 3: Observable Results"},{"location":"how-to/generate-report/#section-4-shot-data-diagnostics","text":"Measurement Basis Distribution: - Histogram showing X/Y/Z measurement frequencies per qubit - Should be uniform (~33% each) for random Clifford shadows Top Bitstrings: - Most frequent measurement outcomes - Useful for debugging unexpected distributions Qubit Marginals: - Per-qubit P(|0\u27e9) probabilities - Should be ~0.5 for maximally entangled states","title":"Section 4: Shot Data Diagnostics"},{"location":"how-to/generate-report/#section-5-mitigation-configuration","text":"Techniques : MEM, ZNE, etc. MEM Confusion Matrix Path : (if applicable) MEM Shots per State : Calibration budget Qubits Calibrated : Indices of calibrated qubits","title":"Section 5: Mitigation Configuration"},{"location":"how-to/generate-report/#section-6-backend-calibration","text":"Calibration Timestamp : When device properties were captured T1 Times : (if available) Decay times per qubit T2 Times : (if available) Dephasing times per qubit Readout Errors : (if available) Measurement error rates Properties Hash : SHA-256 of full calibration JSON","title":"Section 6: Backend Calibration"},{"location":"how-to/generate-report/#section-7-provenance","text":"QuartumSE Version : Package version used Qiskit Version : Qiskit version used Python Version : Python interpreter version Random Seed : (if set) For reproducibility Tags : User-defined searchable tags","title":"Section 7: Provenance"},{"location":"how-to/generate-report/#report-output-locations","text":"","title":"Report Output Locations"},{"location":"how-to/generate-report/#default-behavior","text":"When using the experiment API ( estimator.estimate(save_manifest=True) ), no report is auto-generated. Use the CLI or Python API to generate reports from saved manifests. If you ran experiments with a custom --data-dir , substitute that path for the default data/ directory referenced below.","title":"Default Behavior"},{"location":"how-to/generate-report/#recommended-structure","text":"quartumse/ \u251c\u2500\u2500 data/ \u2502 \u251c\u2500\u2500 manifests/ # JSON manifests \u2502 \u2502 \u251c\u2500\u2500 a3f2b1c4-....json \u2502 \u2502 \u2514\u2500\u2500 b5e8f9d2-....json \u2502 \u251c\u2500\u2500 shots/ # Parquet shot data \u2502 \u2502 \u251c\u2500\u2500 a3f2b1c4-....parquet \u2502 \u2502 \u2514\u2500\u2500 b5e8f9d2-....parquet \u2502 \u2514\u2500\u2500 mem/ # MEM confusion matrices \u2502 \u251c\u2500\u2500 a3f2b1c4-....npz \u2502 \u2514\u2500\u2500 b5e8f9d2-....npz \u2514\u2500\u2500 reports/ # HTML reports (gitignored) \u251c\u2500\u2500 a3f2b1c4-...._report.html \u2514\u2500\u2500 b5e8f9d2-...._report.html","title":"Recommended Structure"},{"location":"how-to/generate-report/#advanced-programmatic-report-analysis","text":"Extract key metrics from reports for comparison: import json from pathlib import Path import pandas as pd def extract_metrics_from_manifest ( manifest_path ): \"\"\"Extract key metrics from manifest JSON.\"\"\" with open ( manifest_path , 'r' ) as f : manifest = json . load ( f ) metrics = { 'experiment_id' : manifest [ 'experiment_id' ], 'backend' : manifest [ 'backend' ][ 'backend_name' ], 'shadow_size' : manifest [ 'shadows' ][ 'shadow_size' ], 'shadow_version' : manifest [ 'shadows' ][ 'version' ], 'num_qubits' : manifest [ 'circuit' ][ 'num_qubits' ], 'execution_time' : manifest . get ( 'resource_usage' , {}) . get ( 'quantum_execution_seconds' , None ), } # Extract observable results for obs_str , obs_data in manifest . get ( 'results_summary' , {}) . items (): metrics [ f 'obs_ { obs_str } _expectation' ] = obs_data . get ( 'expectation_value' ) metrics [ f 'obs_ { obs_str } _ci_width' ] = obs_data . get ( 'ci_width' ) return metrics # Process all manifests manifest_dir = Path ( \"data/manifests\" ) all_metrics = [ extract_metrics_from_manifest ( p ) for p in manifest_dir . glob ( \"*.json\" )] # Convert to DataFrame for analysis df = pd . DataFrame ( all_metrics ) # Compare by backend print ( \" \\n Metrics by Backend:\" ) print ( df . groupby ( 'backend' )[ 'shadow_size' ] . agg ([ 'mean' , 'min' , 'max' , 'count' ])) # Compare by shadow version print ( \" \\n Metrics by Shadow Version:\" ) print ( df . groupby ( 'shadow_version' )[ 'execution_time' ] . agg ([ 'mean' , 'median' , 'std' ]))","title":"Advanced: Programmatic Report Analysis"},{"location":"how-to/generate-report/#pdf-reports-future","text":"PDF generation is planned for Phase 2. It will require the weasyprint package: Installation (when implemented): pip install quartumse [ reporting ] Usage (planned API): from quartumse.reporting import ReportGenerator report = ReportGenerator . from_manifest_file ( \"data/manifests/experiment.json\" ) report . to_pdf ( \"reports/experiment_report.pdf\" ) # Not yet implemented Current workaround: 1. Generate HTML report 2. Open in browser 3. Print to PDF using browser's print dialog","title":"PDF Reports (Future)"},{"location":"how-to/generate-report/#troubleshooting","text":"\"Manifest file not found\" - Check path is correct (absolute or relative) - Verify experiment completed and saved manifest - Use ls data/manifests/ to list available manifests \"cannot import name 'generate_html_report'\" - Use ReportGenerator class instead (new API) - Old function name removed in favor of class-based API - Update code: ReportGenerator.from_manifest_file(path).to_html(output) \"Shot data file not found\" - Report includes shot diagnostics if shot data exists - Gracefully degrades if shot data missing (shows warning) - Check manifest['shot_data_path'] points to valid Parquet file \"Template not found\" - Report templates bundled with package - If error persists, reinstall: pip install --force-reinstall quartumse \"Report HTML not rendering properly\" - Open in modern browser (Chrome, Firefox, Safari, Edge) - Check for JavaScript errors in browser console - Verify HTML file wasn't corrupted during generation \"Report generation slow (>5 seconds)\" - Check shot data file size (should be <10MB for typical experiments) - Verify no network I/O (all files should be local) - Report generation should complete in <1 second normally","title":"Troubleshooting"},{"location":"how-to/generate-report/#related","text":"Run S-T01 GHZ - Generate manifests worth reporting Replay from Manifest - Re-analyze before reporting Manifest Schema - Full manifest specification","title":"Related"},{"location":"how-to/replay-from-manifest/","text":"Replay from Manifest \u00b6 QuartumSE's \"measure once, ask later\" capability lets you compute new observable estimates from saved measurement data without re-executing on quantum hardware. Overview \u00b6 Why replay? - Zero hardware cost - No additional quantum shots needed - Instant results - Compute new observables in seconds - Reproducibility - Same data, different analysis - Exploration - Test hypotheses without waiting for hardware queue What's replayable: - Shadow measurement outcomes (bitstrings + bases) - MEM confusion matrices (for v1 noise-aware shadows) - Backend calibration snapshots - Circuit fingerprints and random seeds - Output directory is configurable; if you ran experiments with --data-dir=/path/to/artifacts the manifest and shot data live underneath that directory instead of the default data/ tree. Quick Start \u00b6 Basic Replay (Python API) \u00b6 from quartumse import ShadowEstimator from quartumse.shadows.core import Observable # Original experiment saved manifest to: data/manifests/a3f2b1c4...json # Create estimator (backend not used during replay) estimator = ShadowEstimator ( backend = \"aer_simulator\" ) # Define NEW observables (different from original run) new_observables = [ Observable ( \"XX\" , coefficient = 1.0 ), Observable ( \"YY\" , coefficient = 1.0 ), Observable ( \"ZZ\" , coefficient = 1.0 ), ] # Replay from saved manifest result = estimator . replay_from_manifest ( manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" , observables = new_observables ) # Access results (same structure as estimate()) for obs_str , data in result . observables . items (): exp_val = data [ 'expectation_value' ] variance = data [ 'variance' ] ci = data [ 'ci_95' ] ci_width = data [ 'ci_width' ] print ( f \" { obs_str } :\" ) print ( f \" Value: { exp_val : .4f } \u00b1 { np . sqrt ( variance ) : .4f } \" ) print ( f \" 95% CI: [ { ci [ 0 ] : .3f } , { ci [ 1 ] : .3f } ] (width: { ci_width : .3f } )\" ) Output: 1.0*XX: Value: 0.9844 \u00b1 0.0312 95% CI: [0.923, 1.046] (width: 0.123) 1.0*YY: Value: 0.9922 \u00b1 0.0289 95% CI: [0.936, 1.048] (width: 0.113) 1.0*ZZ: Value: 1.0039 \u00b1 0.0156 95% CI: [0.973, 1.035] (width: 0.061) Finding Manifests \u00b6 List saved experiments \u00b6 Unix/macOS: ls -lt data/manifests/ | head -10 Windows: Get-ChildItem data / manifests / | Sort-Object LastWriteTime -Descending | Select-Object -First 10 Inspect manifest metadata \u00b6 import json manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" with open ( manifest_path , 'r' ) as f : manifest = json . load ( f ) print ( f \"Experiment ID: { manifest [ 'experiment_id' ] } \" ) print ( f \"Created: { manifest [ 'created_at' ] } \" ) print ( f \"Backend: { manifest [ 'backend' ][ 'backend_name' ] } \" ) print ( f \"Shadow size: { manifest [ 'shadows' ][ 'shadow_size' ] } \" ) print ( f \"Circuit qubits: { manifest [ 'circuit' ][ 'num_qubits' ] } \" ) print ( f \"Original observables: { [ obs [ 'pauli' ] for obs in manifest [ 'observables' ]] } \" ) Output: Experiment ID: a3f2b1c4-5678-90ab-cdef-1234567890ab Created: 2025-10-29T14:32:15.789012 Backend: aer_simulator Shadow size: 256 Circuit qubits: 3 Original observables: ['ZII', 'ZZI', 'ZZZ'] Complete Replay Example \u00b6 Step 1: Run Original Experiment \u00b6 from qiskit import QuantumCircuit from qiskit_aer import AerSimulator from quartumse import ShadowEstimator from quartumse.shadows import ShadowConfig from quartumse.shadows.core import Observable # Create GHZ state circuit = QuantumCircuit ( 3 ) circuit . h ( 0 ) circuit . cx ( 0 , 1 ) circuit . cx ( 0 , 2 ) # Original observables (Z-type stabilizers) original_obs = [ Observable ( \"ZII\" ), Observable ( \"ZZI\" ), Observable ( \"ZZZ\" ), ] # Run experiment with manifest saving config = ShadowConfig ( shadow_size = 256 , random_seed = 42 ) estimator = ShadowEstimator ( backend = AerSimulator (), shadow_config = config ) result = estimator . estimate ( circuit = circuit , observables = original_obs , save_manifest = True # Important! ) print ( f \"Manifest saved: { result . manifest_path } \" ) print ( f \"Shot data saved: { result . shot_data_path } \" ) Step 2: Replay with New Observables (Days/Weeks Later) \u00b6 # No hardware needed! Can run offline with saved data from quartumse import ShadowEstimator from quartumse.shadows.core import Observable # Define new observables (X/Y type for comparison) new_observables = [ Observable ( \"XII\" ), # X on qubit 0 Observable ( \"XXI\" ), # X on qubits 0-1 Observable ( \"XXX\" ), # X on all qubits ] # Replay (backend argument ignored during replay) estimator = ShadowEstimator ( backend = \"aer_simulator\" ) replayed = estimator . replay_from_manifest ( manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" , observables = new_observables ) # Compare with analytical expectations for GHZ ghz_x_expectations = { \"1.0*XII\" : 0.0 , # Single X \u2192 0 \"1.0*XXI\" : 0.0 , # Two X's \u2192 0 \"1.0*XXX\" : 1.0 , # All X's \u2192 1 } print ( \" \\n Replayed Results:\" ) print ( f \" { 'Observable' : <15 } { 'Estimated' : <12 } { 'Expected' : <12 } { 'Error' } \" ) print ( \"-\" * 55 ) for obs_str , data in replayed . observables . items (): exp_val = data [ 'expectation_value' ] expected = ghz_x_expectations [ obs_str ] error = abs ( exp_val - expected ) print ( f \" { obs_str : <15 } { exp_val : >11.4f } { expected : >11.4f } { error : >9.4f } \" ) Output: Replayed Results: Observable Estimated Expected Error ------------------------------------------------------- 1.0*XII 0.0156 0.0000 0.0156 1.0*XXI -0.0234 0.0000 0.0234 1.0*XXX 0.9922 1.0000 0.0078 Replay with Noise-Aware Shadows (v1 + MEM) \u00b6 For experiments using MEM, the replay automatically loads the confusion matrix from the manifest. from quartumse import ShadowEstimator from quartumse.shadows import ShadowConfig from quartumse.shadows.config import ShadowVersion from quartumse.shadows.core import Observable from quartumse.reporting.manifest import MitigationConfig # Original experiment (v1 with MEM) config_v1 = ShadowConfig ( version = ShadowVersion . V1_NOISE_AWARE , shadow_size = 256 , random_seed = 42 , apply_inverse_channel = True ) mit_config = MitigationConfig ( techniques = [], # Populated automatically parameters = { \"mem_shots\" : 512 } ) estimator_v1 = ShadowEstimator ( backend = \"aer_simulator\" , shadow_config = config_v1 , mitigation_config = mit_config ) # Run and save result_v1 = estimator_v1 . estimate ( circuit , observables = original_obs , save_manifest = True ) # --- Later: Replay with MEM applied --- # Replay loads confusion matrix path from manifest replayed_v1 = estimator_v1 . replay_from_manifest ( manifest_path = result_v1 . manifest_path , observables = new_observables ) # Noise correction automatically applied during replay! print ( f \"Confusion matrix loaded from: { result_v1 . mitigation_confusion_matrix_path } \" ) Note: The confusion matrix file must still exist at the path recorded in the manifest. If files have been moved, replay will fail with a FileNotFoundError . Advanced: Programmatic Replay Loop \u00b6 Batch-process multiple manifests to compare different observables: from pathlib import Path from quartumse import ShadowEstimator from quartumse.shadows.core import Observable # Define observable set to test across all experiments test_observables = [ Observable ( \"XXX\" ), Observable ( \"YYY\" ), Observable ( \"ZZZ\" ), ] estimator = ShadowEstimator ( backend = \"aer_simulator\" ) results_table = [] # Replay all manifests in directory manifest_dir = Path ( \"data/manifests\" ) for manifest_path in sorted ( manifest_dir . glob ( \"*.json\" )): try : replayed = estimator . replay_from_manifest ( manifest_path = str ( manifest_path ), observables = test_observables ) # Extract experiment metadata import json with open ( manifest_path ) as f : manifest = json . load ( f ) exp_id = manifest [ 'experiment_id' ] backend = manifest [ 'backend' ][ 'backend_name' ] shadow_size = manifest [ 'shadows' ][ 'shadow_size' ] # Store results for obs_str , data in replayed . observables . items (): results_table . append ({ 'experiment_id' : exp_id , 'backend' : backend , 'shadow_size' : shadow_size , 'observable' : obs_str , 'expectation' : data [ 'expectation_value' ], 'ci_width' : data [ 'ci_width' ], }) print ( f \"\u2713 Replayed: { manifest_path . name } \" ) except Exception as e : print ( f \"\u2717 Failed: { manifest_path . name } - { e } \" ) # Convert to DataFrame for analysis import pandas as pd df = pd . DataFrame ( results_table ) print ( \" \\n Replayed Results Summary:\" ) print ( df . groupby ([ 'observable' , 'backend' ])[ 'expectation' ] . agg ([ 'mean' , 'std' , 'count' ])) Replay Validation \u00b6 Verify that replay produces identical results to original run: # Original run result_original = estimator . estimate ( circuit , observables = original_obs , save_manifest = True ) # Replay with SAME observables result_replay = estimator . replay_from_manifest ( manifest_path = result_original . manifest_path , observables = original_obs # Same as original ) # Compare print ( \" \\n Replay Validation:\" ) print ( f \" { 'Observable' : <15 } { 'Original' : <12 } { 'Replayed' : <12 } { 'Match' } \" ) print ( \"-\" * 55 ) for obs_str in result_original . observables . keys (): orig_val = result_original . observables [ obs_str ][ 'expectation_value' ] replay_val = result_replay . observables [ obs_str ][ 'expectation_value' ] match = \"\u2713\" if abs ( orig_val - replay_val ) < 1e-10 else \"\u2717\" print ( f \" { obs_str : <15 } { orig_val : >11.6f } { replay_val : >11.6f } { match : >7 } \" ) Expected output: Replay Validation: Observable Original Replayed Match ------------------------------------------------------- 1.0*ZII 0.003906 0.003906 \u2713 1.0*ZZI 0.996094 0.996094 \u2713 1.0*ZZZ -0.007812 -0.007812 \u2713 Troubleshooting \u00b6 \"Manifest file not found\" - Check path is correct (absolute or relative to current directory) - Verify manifest was saved during original run ( save_manifest=True ) - Use Path(manifest_path).resolve() to see absolute path \"Shot data file not found\" - Manifest contains path to Parquet file with measurement outcomes - Ensure shot data file hasn't been deleted or moved - Check manifest['shot_data_path'] for expected location \"Confusion matrix file not found\" - For v1 noise-aware shadows with MEM - Check manifest['mitigation']['confusion_matrix_path'] - Ensure MEM calibration file still exists at recorded path - Re-run MEM calibration if needed (see MEM v1 Guide ) \"Different results between original and replay\" - Should be bit-identical if observables are the same - Check random seed is recorded in manifest - Verify no floating-point precision issues (compare within tolerance) \"Replay slower than expected\" - Replay should complete in <1 second for typical shadow sizes - Check Parquet file isn't corrupted (try loading with pandas) - Verify no network I/O (all files should be local) Related \u00b6 Run S-T01 GHZ - Generate manifests worth replaying Generate Report - Create HTML reports from replayed results Manifest Schema - Full manifest specification","title":"Replay from Manifest"},{"location":"how-to/replay-from-manifest/#replay-from-manifest","text":"QuartumSE's \"measure once, ask later\" capability lets you compute new observable estimates from saved measurement data without re-executing on quantum hardware.","title":"Replay from Manifest"},{"location":"how-to/replay-from-manifest/#overview","text":"Why replay? - Zero hardware cost - No additional quantum shots needed - Instant results - Compute new observables in seconds - Reproducibility - Same data, different analysis - Exploration - Test hypotheses without waiting for hardware queue What's replayable: - Shadow measurement outcomes (bitstrings + bases) - MEM confusion matrices (for v1 noise-aware shadows) - Backend calibration snapshots - Circuit fingerprints and random seeds - Output directory is configurable; if you ran experiments with --data-dir=/path/to/artifacts the manifest and shot data live underneath that directory instead of the default data/ tree.","title":"Overview"},{"location":"how-to/replay-from-manifest/#quick-start","text":"","title":"Quick Start"},{"location":"how-to/replay-from-manifest/#basic-replay-python-api","text":"from quartumse import ShadowEstimator from quartumse.shadows.core import Observable # Original experiment saved manifest to: data/manifests/a3f2b1c4...json # Create estimator (backend not used during replay) estimator = ShadowEstimator ( backend = \"aer_simulator\" ) # Define NEW observables (different from original run) new_observables = [ Observable ( \"XX\" , coefficient = 1.0 ), Observable ( \"YY\" , coefficient = 1.0 ), Observable ( \"ZZ\" , coefficient = 1.0 ), ] # Replay from saved manifest result = estimator . replay_from_manifest ( manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" , observables = new_observables ) # Access results (same structure as estimate()) for obs_str , data in result . observables . items (): exp_val = data [ 'expectation_value' ] variance = data [ 'variance' ] ci = data [ 'ci_95' ] ci_width = data [ 'ci_width' ] print ( f \" { obs_str } :\" ) print ( f \" Value: { exp_val : .4f } \u00b1 { np . sqrt ( variance ) : .4f } \" ) print ( f \" 95% CI: [ { ci [ 0 ] : .3f } , { ci [ 1 ] : .3f } ] (width: { ci_width : .3f } )\" ) Output: 1.0*XX: Value: 0.9844 \u00b1 0.0312 95% CI: [0.923, 1.046] (width: 0.123) 1.0*YY: Value: 0.9922 \u00b1 0.0289 95% CI: [0.936, 1.048] (width: 0.113) 1.0*ZZ: Value: 1.0039 \u00b1 0.0156 95% CI: [0.973, 1.035] (width: 0.061)","title":"Basic Replay (Python API)"},{"location":"how-to/replay-from-manifest/#finding-manifests","text":"","title":"Finding Manifests"},{"location":"how-to/replay-from-manifest/#list-saved-experiments","text":"Unix/macOS: ls -lt data/manifests/ | head -10 Windows: Get-ChildItem data / manifests / | Sort-Object LastWriteTime -Descending | Select-Object -First 10","title":"List saved experiments"},{"location":"how-to/replay-from-manifest/#inspect-manifest-metadata","text":"import json manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" with open ( manifest_path , 'r' ) as f : manifest = json . load ( f ) print ( f \"Experiment ID: { manifest [ 'experiment_id' ] } \" ) print ( f \"Created: { manifest [ 'created_at' ] } \" ) print ( f \"Backend: { manifest [ 'backend' ][ 'backend_name' ] } \" ) print ( f \"Shadow size: { manifest [ 'shadows' ][ 'shadow_size' ] } \" ) print ( f \"Circuit qubits: { manifest [ 'circuit' ][ 'num_qubits' ] } \" ) print ( f \"Original observables: { [ obs [ 'pauli' ] for obs in manifest [ 'observables' ]] } \" ) Output: Experiment ID: a3f2b1c4-5678-90ab-cdef-1234567890ab Created: 2025-10-29T14:32:15.789012 Backend: aer_simulator Shadow size: 256 Circuit qubits: 3 Original observables: ['ZII', 'ZZI', 'ZZZ']","title":"Inspect manifest metadata"},{"location":"how-to/replay-from-manifest/#complete-replay-example","text":"","title":"Complete Replay Example"},{"location":"how-to/replay-from-manifest/#step-1-run-original-experiment","text":"from qiskit import QuantumCircuit from qiskit_aer import AerSimulator from quartumse import ShadowEstimator from quartumse.shadows import ShadowConfig from quartumse.shadows.core import Observable # Create GHZ state circuit = QuantumCircuit ( 3 ) circuit . h ( 0 ) circuit . cx ( 0 , 1 ) circuit . cx ( 0 , 2 ) # Original observables (Z-type stabilizers) original_obs = [ Observable ( \"ZII\" ), Observable ( \"ZZI\" ), Observable ( \"ZZZ\" ), ] # Run experiment with manifest saving config = ShadowConfig ( shadow_size = 256 , random_seed = 42 ) estimator = ShadowEstimator ( backend = AerSimulator (), shadow_config = config ) result = estimator . estimate ( circuit = circuit , observables = original_obs , save_manifest = True # Important! ) print ( f \"Manifest saved: { result . manifest_path } \" ) print ( f \"Shot data saved: { result . shot_data_path } \" )","title":"Step 1: Run Original Experiment"},{"location":"how-to/replay-from-manifest/#step-2-replay-with-new-observables-daysweeks-later","text":"# No hardware needed! Can run offline with saved data from quartumse import ShadowEstimator from quartumse.shadows.core import Observable # Define new observables (X/Y type for comparison) new_observables = [ Observable ( \"XII\" ), # X on qubit 0 Observable ( \"XXI\" ), # X on qubits 0-1 Observable ( \"XXX\" ), # X on all qubits ] # Replay (backend argument ignored during replay) estimator = ShadowEstimator ( backend = \"aer_simulator\" ) replayed = estimator . replay_from_manifest ( manifest_path = \"data/manifests/a3f2b1c4-5678-90ab-cdef-1234567890ab.json\" , observables = new_observables ) # Compare with analytical expectations for GHZ ghz_x_expectations = { \"1.0*XII\" : 0.0 , # Single X \u2192 0 \"1.0*XXI\" : 0.0 , # Two X's \u2192 0 \"1.0*XXX\" : 1.0 , # All X's \u2192 1 } print ( \" \\n Replayed Results:\" ) print ( f \" { 'Observable' : <15 } { 'Estimated' : <12 } { 'Expected' : <12 } { 'Error' } \" ) print ( \"-\" * 55 ) for obs_str , data in replayed . observables . items (): exp_val = data [ 'expectation_value' ] expected = ghz_x_expectations [ obs_str ] error = abs ( exp_val - expected ) print ( f \" { obs_str : <15 } { exp_val : >11.4f } { expected : >11.4f } { error : >9.4f } \" ) Output: Replayed Results: Observable Estimated Expected Error ------------------------------------------------------- 1.0*XII 0.0156 0.0000 0.0156 1.0*XXI -0.0234 0.0000 0.0234 1.0*XXX 0.9922 1.0000 0.0078","title":"Step 2: Replay with New Observables (Days/Weeks Later)"},{"location":"how-to/replay-from-manifest/#replay-with-noise-aware-shadows-v1-mem","text":"For experiments using MEM, the replay automatically loads the confusion matrix from the manifest. from quartumse import ShadowEstimator from quartumse.shadows import ShadowConfig from quartumse.shadows.config import ShadowVersion from quartumse.shadows.core import Observable from quartumse.reporting.manifest import MitigationConfig # Original experiment (v1 with MEM) config_v1 = ShadowConfig ( version = ShadowVersion . V1_NOISE_AWARE , shadow_size = 256 , random_seed = 42 , apply_inverse_channel = True ) mit_config = MitigationConfig ( techniques = [], # Populated automatically parameters = { \"mem_shots\" : 512 } ) estimator_v1 = ShadowEstimator ( backend = \"aer_simulator\" , shadow_config = config_v1 , mitigation_config = mit_config ) # Run and save result_v1 = estimator_v1 . estimate ( circuit , observables = original_obs , save_manifest = True ) # --- Later: Replay with MEM applied --- # Replay loads confusion matrix path from manifest replayed_v1 = estimator_v1 . replay_from_manifest ( manifest_path = result_v1 . manifest_path , observables = new_observables ) # Noise correction automatically applied during replay! print ( f \"Confusion matrix loaded from: { result_v1 . mitigation_confusion_matrix_path } \" ) Note: The confusion matrix file must still exist at the path recorded in the manifest. If files have been moved, replay will fail with a FileNotFoundError .","title":"Replay with Noise-Aware Shadows (v1 + MEM)"},{"location":"how-to/replay-from-manifest/#advanced-programmatic-replay-loop","text":"Batch-process multiple manifests to compare different observables: from pathlib import Path from quartumse import ShadowEstimator from quartumse.shadows.core import Observable # Define observable set to test across all experiments test_observables = [ Observable ( \"XXX\" ), Observable ( \"YYY\" ), Observable ( \"ZZZ\" ), ] estimator = ShadowEstimator ( backend = \"aer_simulator\" ) results_table = [] # Replay all manifests in directory manifest_dir = Path ( \"data/manifests\" ) for manifest_path in sorted ( manifest_dir . glob ( \"*.json\" )): try : replayed = estimator . replay_from_manifest ( manifest_path = str ( manifest_path ), observables = test_observables ) # Extract experiment metadata import json with open ( manifest_path ) as f : manifest = json . load ( f ) exp_id = manifest [ 'experiment_id' ] backend = manifest [ 'backend' ][ 'backend_name' ] shadow_size = manifest [ 'shadows' ][ 'shadow_size' ] # Store results for obs_str , data in replayed . observables . items (): results_table . append ({ 'experiment_id' : exp_id , 'backend' : backend , 'shadow_size' : shadow_size , 'observable' : obs_str , 'expectation' : data [ 'expectation_value' ], 'ci_width' : data [ 'ci_width' ], }) print ( f \"\u2713 Replayed: { manifest_path . name } \" ) except Exception as e : print ( f \"\u2717 Failed: { manifest_path . name } - { e } \" ) # Convert to DataFrame for analysis import pandas as pd df = pd . DataFrame ( results_table ) print ( \" \\n Replayed Results Summary:\" ) print ( df . groupby ([ 'observable' , 'backend' ])[ 'expectation' ] . agg ([ 'mean' , 'std' , 'count' ]))","title":"Advanced: Programmatic Replay Loop"},{"location":"how-to/replay-from-manifest/#replay-validation","text":"Verify that replay produces identical results to original run: # Original run result_original = estimator . estimate ( circuit , observables = original_obs , save_manifest = True ) # Replay with SAME observables result_replay = estimator . replay_from_manifest ( manifest_path = result_original . manifest_path , observables = original_obs # Same as original ) # Compare print ( \" \\n Replay Validation:\" ) print ( f \" { 'Observable' : <15 } { 'Original' : <12 } { 'Replayed' : <12 } { 'Match' } \" ) print ( \"-\" * 55 ) for obs_str in result_original . observables . keys (): orig_val = result_original . observables [ obs_str ][ 'expectation_value' ] replay_val = result_replay . observables [ obs_str ][ 'expectation_value' ] match = \"\u2713\" if abs ( orig_val - replay_val ) < 1e-10 else \"\u2717\" print ( f \" { obs_str : <15 } { orig_val : >11.6f } { replay_val : >11.6f } { match : >7 } \" ) Expected output: Replay Validation: Observable Original Replayed Match ------------------------------------------------------- 1.0*ZII 0.003906 0.003906 \u2713 1.0*ZZI 0.996094 0.996094 \u2713 1.0*ZZZ -0.007812 -0.007812 \u2713","title":"Replay Validation"},{"location":"how-to/replay-from-manifest/#troubleshooting","text":"\"Manifest file not found\" - Check path is correct (absolute or relative to current directory) - Verify manifest was saved during original run ( save_manifest=True ) - Use Path(manifest_path).resolve() to see absolute path \"Shot data file not found\" - Manifest contains path to Parquet file with measurement outcomes - Ensure shot data file hasn't been deleted or moved - Check manifest['shot_data_path'] for expected location \"Confusion matrix file not found\" - For v1 noise-aware shadows with MEM - Check manifest['mitigation']['confusion_matrix_path'] - Ensure MEM calibration file still exists at recorded path - Re-run MEM calibration if needed (see MEM v1 Guide ) \"Different results between original and replay\" - Should be bit-identical if observables are the same - Check random seed is recorded in manifest - Verify no floating-point precision issues (compare within tolerance) \"Replay slower than expected\" - Replay should complete in <1 second for typical shadow sizes - Check Parquet file isn't corrupted (try loading with pandas) - Verify no network I/O (all files should be local)","title":"Troubleshooting"},{"location":"how-to/replay-from-manifest/#related","text":"Run S-T01 GHZ - Generate manifests worth replaying Generate Report - Create HTML reports from replayed results Manifest Schema - Full manifest specification","title":"Related"},{"location":"how-to/run-automated-pipeline/","text":"Automated Experiment Pipeline (Phase-1) \u00b6 The Phase-1 pipeline automates the reproducible baseline \u2192 shadows v0 \u2192 shadows v1 runs, aggregates metrics, and emits artefacts that can be replayed later. Use this guide when you need to run, extend, or debug the experiments/pipeline package. Metadata schema \u00b6 experiments/pipeline/metadata_schema.py defines the structured metadata that every pipeline run consumes. Provide the fields below in YAML or JSON (see experiments/shadows/examples/extended_ghz/experiment_metadata.yaml for a template): Field Type Purpose experiment string Human-readable experiment name that is echoed in manifests and reports. context string One-paragraph background describing why the run exists. aims list[string] Bullet points for the primary questions the run answers. success_criteria list[string] Quantitative exit checks (used to infer default targets). methods list[string] High-level procedure summary for provenance. budget.total_shots int Equal-budget anchor: total shots to spend per approach (baseline, v0, v1). budget.calibration.shots_per_state int Shots to allocate to each computational-basis state during MEM calibration. budget.calibration.total int Total calibration shots (must equal shots_per_state * 2^n ). budget.v0_shadow_size int Measurement budget for the Phase-1 shadows v0 reference. budget.v1_shadow_size int Measurement budget for the Phase-1 shadows v1 + MEM run (not including calibration shots). device string Default backend descriptor (e.g. aer_simulator or ibm:ibm_brisbane ). discussion_template string Markdown template pre-populated in the generated HTML report. num_qubits int (optional) Override for inferred qubit count (normally derived from the calibration budget). targets map[string, number] (optional) Explicit metric thresholds ( ssr_average , ci_coverage , etc.). ground_truth mapping (optional) Observable \u2192 expectation pairs for MAE/CI/SSR calculations. Tip: Leave targets blank if the success criteria already specify SSR/CI thresholds\u2014 run_full_pipeline.py automatically parses numbers from those strings. Phase-1 equal-budget rules \u00b6 Phase-1 comparisons assume every approach consumes the same total shot budget and distributes resources uniformly across observables: Stage 1 (direct Pauli baseline) divides budget.total_shots evenly across the stabiliser observables using _allocate_shots in experiments/pipeline/_runners.py . Stage 2 ( ShadowVersion.V0_BASELINE ) records the exact budget.v0_shadow_size measurements with mitigation disabled. Stage 3 ( ShadowVersion.V1_NOISE_AWARE ) spends budget.v1_shadow_size on measurement shots and reuses calibration snapshots so that calibration.total + v1_shadow_size = budget.total_shots . The analysis layer ( experiments/pipeline/analyzer.py ) uses the equal-budget metrics from src/quartumse/utils/metrics.py ( compute_mae , compute_ci_coverage , and compute_ssr_equal_budget ) that treat each observable with uniform weight. Violating the equal-budget assumption (for example, by changing the shot allocator or by skipping calibration reuse) invalidates SSR/CI comparisons, so keep the metadata and runner defaults aligned when you extend the pipeline. Calibration reuse and refresh windows \u00b6 The pipeline and CLI share the ReadoutCalibrationManager so that measurement error mitigation (MEM) snapshots are only regenerated when required: Unix/macOS: # Reuse cached confusion matrices unless forced or stale quartumse calibrate-readout \\ --backend ibm:ibm_brisbane \\ --qubit 0 --qubit 1 --qubit 2 --qubit 3 \\ --shots 256 \\ --output-dir validation_data/calibrations \\ --max-age-hours 6 Windows: # Reuse cached confusion matrices unless forced or stale quartumse calibrate-readout ` - -backend ibm : ibm_brisbane ` - -qubit 0 - -qubit 1 - -qubit 2 - -qubit 3 ` - -shots 256 ` - -output-dir validation_data / calibrations ` - -max-age-hours 6 Key behaviours ( src/quartumse/cli.py ): Reuse by default: ensure_calibration returns cached matrices and marks the manifest as \"reused\": true when the existing artefact matches the qubit set and backend descriptor. --max-age-hours : Provide a float to refresh calibrations that are older than the allowed window. Pass --force to ignore age checks entirely. Manifest trail: Each calibration writes <confusion>.manifest.json alongside the .npz , recording backend version, shot counts, and reuse flags for provenance. The automated pipeline ( experiments/pipeline/executor.py ) points its calibration manager at <output>/calibrations/ so reruns in the same directory automatically reuse MEM artefacts as long as the age/force constraints permit. Running the CLI pipeline \u00b6 Use the run_full_pipeline.py entrypoint to execute the three stages, verify artefacts, and render a report. Simulator (Aer) \u00b6 Unix/macOS: python -m experiments.pipeline.run_full_pipeline \\ --metadata experiments/shadows/examples/extended_ghz/experiment_metadata.yaml \\ --output validation_data/pipeline_runs/ghz4_aer \\ --backend aer_simulator Windows: python -m experiments . pipeline . run_full_pipeline ` - -metadata experiments / shadows / examples / extended_ghz / experiment_metadata . yaml ` - -output validation_data / pipeline_runs / ghz4_aer ` - -backend aer_simulator Produces manifests under validation_data/pipeline_runs/ghz4_aer/manifests/ . Stores calibration artefacts in validation_data/pipeline_runs/ghz4_aer/calibrations/ . Writes the result digest ( result_hash.txt ), analysis summary, and HTML report to the same directory. IBM Quantum hardware \u00b6 Unix/macOS: export QISKIT_IBM_TOKEN = \"<your-runtime-token>\" python -m experiments.pipeline.run_full_pipeline \\ --metadata experiments/shadows/examples/extended_ghz/experiment_metadata.yaml \\ --output data/pipeline_runs/ghz4_kyoto \\ --backend ibm:ibm_kyoto Windows (PowerShell): $env:QISKIT_IBM_TOKEN = \"<your-runtime-token>\" python -m experiments . pipeline . run_full_pipeline ` - -metadata experiments / shadows / examples / extended_ghz / experiment_metadata . yaml ` - -output data / pipeline_runs / ghz4_kyoto ` - -backend ibm : ibm_kyoto Windows (Command Prompt): set QISKIT_IBM_TOKEN=<your-runtime-token> python -m experiments.pipeline.run_full_pipeline ^ --metadata experiments/shadows/examples/extended_ghz/experiment_metadata.yaml ^ --output data/pipeline_runs/ghz4_kyoto ^ --backend ibm:ibm_kyoto Set QISKIT_RUNTIME_API_TOKEN / QISKIT_IBM_CHANNEL / QISKIT_IBM_INSTANCE if your hub requires them (see quartumse connect ibm hints in src/quartumse/cli.py ). Hardware runs honour the same equal-budget accounting\u2014MEM calibration shots are deducted from the total and reused when possible. The output directory is Git-ignored; move final manifests to data/manifests/ when publishing results. To override the backend encoded in the metadata file, pass a different --backend value. Omit the flag to use metadata.device as-is. Artefacts and replay workflow \u00b6 After a pipeline run completes, inspect the output directory: Unix/macOS: ls -R validation_data/pipeline_runs/ghz4_aer cat validation_data/pipeline_runs/ghz4_aer/report_*.html | head Windows: Get-ChildItem -Recurse validation_data / pipeline_runs / ghz4_aer Get-Content validation_data / pipeline_runs / ghz4_aer / report_ *. html | Select-Object -First 10 You should see: manifests/ \u2013 baseline, v0, and v1 JSON manifests (Stage 1\u20133). calibrations/ \u2013 MEM confusion matrices plus .manifest.json metadata (only when Phase-1 runs require mitigation). analysis_<hash>.json \u2013 Aggregated metrics ( ssr_average , ci_coverage , MAE) and target evaluation flags. report_<hash>.html \u2013 Full Phase-1 report ready for review or screenshots. result_hash.txt \u2013 Stable digest derived from the manifest payloads. Replaying artefacts does not require backend access: Unix/macOS: # Regenerate the HTML report after editing metadata or narrative sections quartumse report validation_data/pipeline_runs/ghz4_aer/manifests/<manifest>.json \\ --output validation_data/pipeline_runs/ghz4_aer/replay_report.html # Programmatic replay: recompute observables from saved manifests python - <<'PY' from quartumse.estimator import ShadowEstimator from quartumse.reporting.manifest import ProvenanceManifest manifest_path = \"validation_data/pipeline_runs/ghz4_aer/manifests/<manifest>.json\" manifest = ProvenanceManifest.from_json(manifest_path) estimator = ShadowEstimator.replay_from_manifest(manifest) print(estimator) PY Windows: # Regenerate the HTML report after editing metadata or narrative sections quartumse report validation_data / pipeline_runs / ghz4_aer / manifests /< manifest >. json ` - -output validation_data / pipeline_runs / ghz4_aer / replay_report . html # Programmatic replay: recompute observables from saved manifests python -c \"from quartumse.estimator import ShadowEstimator; from quartumse.reporting.manifest import ProvenanceManifest; manifest_path = 'validation_data/pipeline_runs/ghz4_aer/manifests/<manifest>.json'; manifest = ProvenanceManifest.from_json(manifest_path); estimator = ShadowEstimator.replay_from_manifest(manifest); print(estimator)\" The verifier stage ( experiments/pipeline/verifier.py ) checks that shot files and MEM confusion matrices still exist and match their checksums. If you relocate artefacts, update manifest.schema.shot_data_path or keep the directory layout intact so replay continues to pass. Need more automation? Extend experiments/pipeline/executor.py with new stages, but keep the metadata schema and equal-budget invariants intact so downstream analysis and reports remain comparable.","title":"Automated Experiment Pipeline (Phase-1)"},{"location":"how-to/run-automated-pipeline/#automated-experiment-pipeline-phase-1","text":"The Phase-1 pipeline automates the reproducible baseline \u2192 shadows v0 \u2192 shadows v1 runs, aggregates metrics, and emits artefacts that can be replayed later. Use this guide when you need to run, extend, or debug the experiments/pipeline package.","title":"Automated Experiment Pipeline (Phase-1)"},{"location":"how-to/run-automated-pipeline/#metadata-schema","text":"experiments/pipeline/metadata_schema.py defines the structured metadata that every pipeline run consumes. Provide the fields below in YAML or JSON (see experiments/shadows/examples/extended_ghz/experiment_metadata.yaml for a template): Field Type Purpose experiment string Human-readable experiment name that is echoed in manifests and reports. context string One-paragraph background describing why the run exists. aims list[string] Bullet points for the primary questions the run answers. success_criteria list[string] Quantitative exit checks (used to infer default targets). methods list[string] High-level procedure summary for provenance. budget.total_shots int Equal-budget anchor: total shots to spend per approach (baseline, v0, v1). budget.calibration.shots_per_state int Shots to allocate to each computational-basis state during MEM calibration. budget.calibration.total int Total calibration shots (must equal shots_per_state * 2^n ). budget.v0_shadow_size int Measurement budget for the Phase-1 shadows v0 reference. budget.v1_shadow_size int Measurement budget for the Phase-1 shadows v1 + MEM run (not including calibration shots). device string Default backend descriptor (e.g. aer_simulator or ibm:ibm_brisbane ). discussion_template string Markdown template pre-populated in the generated HTML report. num_qubits int (optional) Override for inferred qubit count (normally derived from the calibration budget). targets map[string, number] (optional) Explicit metric thresholds ( ssr_average , ci_coverage , etc.). ground_truth mapping (optional) Observable \u2192 expectation pairs for MAE/CI/SSR calculations. Tip: Leave targets blank if the success criteria already specify SSR/CI thresholds\u2014 run_full_pipeline.py automatically parses numbers from those strings.","title":"Metadata schema"},{"location":"how-to/run-automated-pipeline/#phase-1-equal-budget-rules","text":"Phase-1 comparisons assume every approach consumes the same total shot budget and distributes resources uniformly across observables: Stage 1 (direct Pauli baseline) divides budget.total_shots evenly across the stabiliser observables using _allocate_shots in experiments/pipeline/_runners.py . Stage 2 ( ShadowVersion.V0_BASELINE ) records the exact budget.v0_shadow_size measurements with mitigation disabled. Stage 3 ( ShadowVersion.V1_NOISE_AWARE ) spends budget.v1_shadow_size on measurement shots and reuses calibration snapshots so that calibration.total + v1_shadow_size = budget.total_shots . The analysis layer ( experiments/pipeline/analyzer.py ) uses the equal-budget metrics from src/quartumse/utils/metrics.py ( compute_mae , compute_ci_coverage , and compute_ssr_equal_budget ) that treat each observable with uniform weight. Violating the equal-budget assumption (for example, by changing the shot allocator or by skipping calibration reuse) invalidates SSR/CI comparisons, so keep the metadata and runner defaults aligned when you extend the pipeline.","title":"Phase-1 equal-budget rules"},{"location":"how-to/run-automated-pipeline/#calibration-reuse-and-refresh-windows","text":"The pipeline and CLI share the ReadoutCalibrationManager so that measurement error mitigation (MEM) snapshots are only regenerated when required: Unix/macOS: # Reuse cached confusion matrices unless forced or stale quartumse calibrate-readout \\ --backend ibm:ibm_brisbane \\ --qubit 0 --qubit 1 --qubit 2 --qubit 3 \\ --shots 256 \\ --output-dir validation_data/calibrations \\ --max-age-hours 6 Windows: # Reuse cached confusion matrices unless forced or stale quartumse calibrate-readout ` - -backend ibm : ibm_brisbane ` - -qubit 0 - -qubit 1 - -qubit 2 - -qubit 3 ` - -shots 256 ` - -output-dir validation_data / calibrations ` - -max-age-hours 6 Key behaviours ( src/quartumse/cli.py ): Reuse by default: ensure_calibration returns cached matrices and marks the manifest as \"reused\": true when the existing artefact matches the qubit set and backend descriptor. --max-age-hours : Provide a float to refresh calibrations that are older than the allowed window. Pass --force to ignore age checks entirely. Manifest trail: Each calibration writes <confusion>.manifest.json alongside the .npz , recording backend version, shot counts, and reuse flags for provenance. The automated pipeline ( experiments/pipeline/executor.py ) points its calibration manager at <output>/calibrations/ so reruns in the same directory automatically reuse MEM artefacts as long as the age/force constraints permit.","title":"Calibration reuse and refresh windows"},{"location":"how-to/run-automated-pipeline/#running-the-cli-pipeline","text":"Use the run_full_pipeline.py entrypoint to execute the three stages, verify artefacts, and render a report.","title":"Running the CLI pipeline"},{"location":"how-to/run-automated-pipeline/#simulator-aer","text":"Unix/macOS: python -m experiments.pipeline.run_full_pipeline \\ --metadata experiments/shadows/examples/extended_ghz/experiment_metadata.yaml \\ --output validation_data/pipeline_runs/ghz4_aer \\ --backend aer_simulator Windows: python -m experiments . pipeline . run_full_pipeline ` - -metadata experiments / shadows / examples / extended_ghz / experiment_metadata . yaml ` - -output validation_data / pipeline_runs / ghz4_aer ` - -backend aer_simulator Produces manifests under validation_data/pipeline_runs/ghz4_aer/manifests/ . Stores calibration artefacts in validation_data/pipeline_runs/ghz4_aer/calibrations/ . Writes the result digest ( result_hash.txt ), analysis summary, and HTML report to the same directory.","title":"Simulator (Aer)"},{"location":"how-to/run-automated-pipeline/#ibm-quantum-hardware","text":"Unix/macOS: export QISKIT_IBM_TOKEN = \"<your-runtime-token>\" python -m experiments.pipeline.run_full_pipeline \\ --metadata experiments/shadows/examples/extended_ghz/experiment_metadata.yaml \\ --output data/pipeline_runs/ghz4_kyoto \\ --backend ibm:ibm_kyoto Windows (PowerShell): $env:QISKIT_IBM_TOKEN = \"<your-runtime-token>\" python -m experiments . pipeline . run_full_pipeline ` - -metadata experiments / shadows / examples / extended_ghz / experiment_metadata . yaml ` - -output data / pipeline_runs / ghz4_kyoto ` - -backend ibm : ibm_kyoto Windows (Command Prompt): set QISKIT_IBM_TOKEN=<your-runtime-token> python -m experiments.pipeline.run_full_pipeline ^ --metadata experiments/shadows/examples/extended_ghz/experiment_metadata.yaml ^ --output data/pipeline_runs/ghz4_kyoto ^ --backend ibm:ibm_kyoto Set QISKIT_RUNTIME_API_TOKEN / QISKIT_IBM_CHANNEL / QISKIT_IBM_INSTANCE if your hub requires them (see quartumse connect ibm hints in src/quartumse/cli.py ). Hardware runs honour the same equal-budget accounting\u2014MEM calibration shots are deducted from the total and reused when possible. The output directory is Git-ignored; move final manifests to data/manifests/ when publishing results. To override the backend encoded in the metadata file, pass a different --backend value. Omit the flag to use metadata.device as-is.","title":"IBM Quantum hardware"},{"location":"how-to/run-automated-pipeline/#artefacts-and-replay-workflow","text":"After a pipeline run completes, inspect the output directory: Unix/macOS: ls -R validation_data/pipeline_runs/ghz4_aer cat validation_data/pipeline_runs/ghz4_aer/report_*.html | head Windows: Get-ChildItem -Recurse validation_data / pipeline_runs / ghz4_aer Get-Content validation_data / pipeline_runs / ghz4_aer / report_ *. html | Select-Object -First 10 You should see: manifests/ \u2013 baseline, v0, and v1 JSON manifests (Stage 1\u20133). calibrations/ \u2013 MEM confusion matrices plus .manifest.json metadata (only when Phase-1 runs require mitigation). analysis_<hash>.json \u2013 Aggregated metrics ( ssr_average , ci_coverage , MAE) and target evaluation flags. report_<hash>.html \u2013 Full Phase-1 report ready for review or screenshots. result_hash.txt \u2013 Stable digest derived from the manifest payloads. Replaying artefacts does not require backend access: Unix/macOS: # Regenerate the HTML report after editing metadata or narrative sections quartumse report validation_data/pipeline_runs/ghz4_aer/manifests/<manifest>.json \\ --output validation_data/pipeline_runs/ghz4_aer/replay_report.html # Programmatic replay: recompute observables from saved manifests python - <<'PY' from quartumse.estimator import ShadowEstimator from quartumse.reporting.manifest import ProvenanceManifest manifest_path = \"validation_data/pipeline_runs/ghz4_aer/manifests/<manifest>.json\" manifest = ProvenanceManifest.from_json(manifest_path) estimator = ShadowEstimator.replay_from_manifest(manifest) print(estimator) PY Windows: # Regenerate the HTML report after editing metadata or narrative sections quartumse report validation_data / pipeline_runs / ghz4_aer / manifests /< manifest >. json ` - -output validation_data / pipeline_runs / ghz4_aer / replay_report . html # Programmatic replay: recompute observables from saved manifests python -c \"from quartumse.estimator import ShadowEstimator; from quartumse.reporting.manifest import ProvenanceManifest; manifest_path = 'validation_data/pipeline_runs/ghz4_aer/manifests/<manifest>.json'; manifest = ProvenanceManifest.from_json(manifest_path); estimator = ShadowEstimator.replay_from_manifest(manifest); print(estimator)\" The verifier stage ( experiments/pipeline/verifier.py ) checks that shot files and MEM confusion matrices still exist and match their checksums. If you relocate artefacts, update manifest.schema.shot_data_path or keep the directory layout intact so replay continues to pass. Need more automation? Extend experiments/pipeline/executor.py with new stages, but keep the metadata schema and equal-budget invariants intact so downstream analysis and reports remain comparable.","title":"Artefacts and replay workflow"},{"location":"how-to/run-mem-v1/","text":"Run MEM (v1) \u00b6 Procedures for configuring and executing MEM (v1) runs will be documented here.","title":"Run MEM (v1)"},{"location":"how-to/run-mem-v1/#run-mem-v1","text":"Procedures for configuring and executing MEM (v1) runs will be documented here.","title":"Run MEM (v1)"},{"location":"how-to/run-st01-ghz/","text":"Run S-T01 GHZ Experiment \u00b6 The S-T01/S-T02 experiments validate classical shadows on GHZ states, measuring Shot-Savings Ratio (SSR) and confidence interval (CI) coverage against Phase 1 exit criteria. Overview \u00b6 S-T01 (baseline): Classical shadows v0 without mitigation S-T02 (noise-aware): Classical shadows v1 with MEM calibration Both variants support: - GHZ states from 2-5 qubits - Configurable backends (Aer simulator or IBM Quantum hardware) - Automatic SSR calculation vs direct measurement baseline - Manifest + shot data persistence for replay Quick Start \u00b6 S-T01 (Baseline) on Simulator \u00b6 Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator --variant st01 Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator - -variant st01 Expected output: ==================================================================================== S-T01 GHZ Validation (classical shadows v0 baseline) ==================================================================================== Backend: aer_simulator Shadow size: 256 per GHZ state Baseline shots: 1024 per observable ==================================================================================== Testing GHZ(2)... Observable Shadows Expected Baseline CI Width SSR In CI -------------------------------------------------------------------------------- 1.0*ZI 0.0039 0.0000 0.0020 0.1800 1.45 \u2713 1.0*IZ -0.0078 0.0000 -0.0039 0.1756 1.38 \u2713 1.0*ZZ 0.9961 1.0000 0.9941 0.0488 5.32 \u2713 ============================================================ METRICS for GHZ(2) ============================================================ CI Coverage: 100.00% (target: \u226590%) SSR (estimated): 2.72\u00d7 (target: \u22651.2\u00d7) Shadow size: 256 Baseline shots: 1024 [... similar output for GHZ(3), GHZ(4), GHZ(5) ...] ================================================================================ EXPERIMENT SUMMARY ================================================================================ Qubits CI Coverage SSR Status -------------------------------------------------- 2 100.00% 2.72 \u2713 PASS 3 100.00% 3.14 \u2713 PASS 4 100.00% 2.89 \u2713 PASS 5 93.33% 2.45 \u2713 PASS ================================================================================ \u2713 EXPERIMENT PASSED - Phase 1 exit criteria met! ================================================================================ Artifacts saved: - Manifests: data/manifests/<experiment_id>.json - Shot data: data/shots/<experiment_id>.parquet - Change the base directory with --data-dir (see Common flags ) S-T02 (Noise-Aware) with MEM \u00b6 Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator - -variant st02 Key differences: - Runs MEM calibration before shadow measurements (8 basis states \u00d7 shots) - Applies inverse confusion matrix during reconstruction - Typically shows better SSR on noisy hardware (similar on ideal simulator) Expected output (additional MEM info): ==================================================================================== S-T02 GHZ Validation (classical shadows v1 + MEM) ==================================================================================== Testing GHZ(3)... Step 1: MEM calibration (8 basis states \u00d7 512 shots = 4096 total) Step 2: Shadow measurements (256 shots) Step 3: Noise correction via confusion matrix Confusion matrix (3 qubits): [[0.998 0.002 ... 0.000] [0.001 0.996 ... 0.001] ... [0.000 0.001 ... 0.997]] Observable Shadows Expected Baseline CI Width SSR In CI -------------------------------------------------------------------------------- ... Running on IBM Quantum Hardware \u00b6 Prerequisites \u00b6 Set IBM credentials: Unix/macOS: export QISKIT_IBM_TOKEN = \"your_token_here\" # Optional: specify hub/group/project export QISKIT_IBM_INSTANCE = \"ibm-q/open/main\" Windows (PowerShell): $env:QISKIT_IBM_TOKEN = \"your_token_here\" # Optional: specify hub/group/project $env:QISKIT_IBM_INSTANCE = \"ibm-q/open/main\" Windows (Command Prompt): set QISKIT_IBM_TOKEN=your_token_here rem Optional: specify hub/group/project set QISKIT_IBM_INSTANCE=ibm-q/open/main Check remaining quota: All platforms: quartumse runtime-status --backend ibm:ibm_brisbane Run on Hardware \u00b6 Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py \\ --backend ibm:ibm_brisbane \\ --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py ` - -backend ibm : ibm_brisbane ` - -variant st02 Note: Hardware runs may take 10-30 minutes depending on queue depth and shot counts. Using a Configuration File \u00b6 Create config.yaml : backend : provider : ibm name : ibm_brisbane shadow_size : 512 baseline_shots : 2048 # For S-T02 (MEM) mem_shots : 1024 mem_qubits : [ 0 , 1 , 2 , 3 , 4 ] Run with config: Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py \\ --config config.yaml \\ --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py ` - -config config . yaml ` - -variant st02 Override backend from command line: Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py \\ --config config.yaml \\ --backend ibm:ibmq_qasm_simulator \\ --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py ` - -config config . yaml ` - -backend ibm : ibmq_qasm_simulator ` - -variant st02 Understanding the Output \u00b6 Metrics Explained \u00b6 CI Coverage - Percentage of observables where true value falls within 95% confidence interval - Target: \u226590% for Phase 1 validation - Typical: 93-100% on simulator, 85-95% on hardware SSR (Shot-Savings Ratio) - Ratio of baseline shots to shadow shots needed for equivalent precision - Formula: SSR = (baseline_shots / shadow_size) \u00d7 (baseline_error / shadow_error)\u00b2 - Target: \u22651.2\u00d7 for Phase 1 validation - Typical: 2-7\u00d7 on simulator, 1.1-3\u00d7 on noisy hardware Status - \u2713 PASS: Both CI coverage \u226590% and SSR \u22651.2\u00d7 - \u2717 FAIL: One or more criteria not met Observable Notation \u00b6 ZI = Z on qubit 0, Identity on qubit 1 ZZ = Z on both qubits ZZZ = Z on all three qubits For GHZ states: - Even number of Z operators: expectation = +1 - Odd number of Z operators: expectation = 0 Troubleshooting \u00b6 \"Unable to resolve IBM backend\" - Ensure QISKIT_IBM_TOKEN is set - Check backend name is correct (use quartumse runtime-status to list) - Verify credentials have access to the specified backend \"ModuleNotFoundError: qiskit_ibm_runtime\" - Install runtime dependencies: pip install quartumse[mitigation] \"Insufficient runtime quota\" - Check remaining quota: quartumse runtime-status - Wait for monthly reset or use simulator - Reduce shot counts in config file Poor SSR on hardware (<1.2\u00d7) - Expected for very noisy backends - Try S-T02 variant with MEM (typically improves SSR by 20-40%) - Use backends with lower readout error rates Manifest not found during replay - Check data/manifests/ directory exists - Verify experiment completed successfully (look for \"\u2713 EXPERIMENT PASSED\") - Use full path to manifest file Next Steps \u00b6 Replay experiments: See Replay from Manifest Generate reports: See Generate Report Run custom experiments: Modify script or use notebooks Hardware validation: Follow IBM Runtime Runbook Related \u00b6 MEM v1 Guide - Details on measurement error mitigation Testing Guide - Automated test suite and markers Phase 1 Task Checklist - Exit criteria Common flags \u00b6 The experiment script now shares the same CLI surface as other QuartumSE runs: Flag Purpose Default --backend Override the backend descriptor ( aer_simulator , ibm:ibm_brisbane , etc.) aer_simulator --shadow-size Number of classical shadow shots per GHZ size 500 (configurable via YAML) --seed Random seed used when sampling classical shadows 42 --data-dir Base directory for manifests, shot archives, and MEM data data/ All parameters can also be provided through the optional YAML config file. CLI flags always win over configuration values, making it easy to experiment with different shot budgets or output locations ad-hoc.","title":"Run S-T01 GHZ"},{"location":"how-to/run-st01-ghz/#run-s-t01-ghz-experiment","text":"The S-T01/S-T02 experiments validate classical shadows on GHZ states, measuring Shot-Savings Ratio (SSR) and confidence interval (CI) coverage against Phase 1 exit criteria.","title":"Run S-T01 GHZ Experiment"},{"location":"how-to/run-st01-ghz/#overview","text":"S-T01 (baseline): Classical shadows v0 without mitigation S-T02 (noise-aware): Classical shadows v1 with MEM calibration Both variants support: - GHZ states from 2-5 qubits - Configurable backends (Aer simulator or IBM Quantum hardware) - Automatic SSR calculation vs direct measurement baseline - Manifest + shot data persistence for replay","title":"Overview"},{"location":"how-to/run-st01-ghz/#quick-start","text":"","title":"Quick Start"},{"location":"how-to/run-st01-ghz/#s-t01-baseline-on-simulator","text":"Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator --variant st01 Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator - -variant st01 Expected output: ==================================================================================== S-T01 GHZ Validation (classical shadows v0 baseline) ==================================================================================== Backend: aer_simulator Shadow size: 256 per GHZ state Baseline shots: 1024 per observable ==================================================================================== Testing GHZ(2)... Observable Shadows Expected Baseline CI Width SSR In CI -------------------------------------------------------------------------------- 1.0*ZI 0.0039 0.0000 0.0020 0.1800 1.45 \u2713 1.0*IZ -0.0078 0.0000 -0.0039 0.1756 1.38 \u2713 1.0*ZZ 0.9961 1.0000 0.9941 0.0488 5.32 \u2713 ============================================================ METRICS for GHZ(2) ============================================================ CI Coverage: 100.00% (target: \u226590%) SSR (estimated): 2.72\u00d7 (target: \u22651.2\u00d7) Shadow size: 256 Baseline shots: 1024 [... similar output for GHZ(3), GHZ(4), GHZ(5) ...] ================================================================================ EXPERIMENT SUMMARY ================================================================================ Qubits CI Coverage SSR Status -------------------------------------------------- 2 100.00% 2.72 \u2713 PASS 3 100.00% 3.14 \u2713 PASS 4 100.00% 2.89 \u2713 PASS 5 93.33% 2.45 \u2713 PASS ================================================================================ \u2713 EXPERIMENT PASSED - Phase 1 exit criteria met! ================================================================================ Artifacts saved: - Manifests: data/manifests/<experiment_id>.json - Shot data: data/shots/<experiment_id>.parquet - Change the base directory with --data-dir (see Common flags )","title":"S-T01 (Baseline) on Simulator"},{"location":"how-to/run-st01-ghz/#s-t02-noise-aware-with-mem","text":"Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator - -variant st02 Key differences: - Runs MEM calibration before shadow measurements (8 basis states \u00d7 shots) - Applies inverse confusion matrix during reconstruction - Typically shows better SSR on noisy hardware (similar on ideal simulator) Expected output (additional MEM info): ==================================================================================== S-T02 GHZ Validation (classical shadows v1 + MEM) ==================================================================================== Testing GHZ(3)... Step 1: MEM calibration (8 basis states \u00d7 512 shots = 4096 total) Step 2: Shadow measurements (256 shots) Step 3: Noise correction via confusion matrix Confusion matrix (3 qubits): [[0.998 0.002 ... 0.000] [0.001 0.996 ... 0.001] ... [0.000 0.001 ... 0.997]] Observable Shadows Expected Baseline CI Width SSR In CI -------------------------------------------------------------------------------- ...","title":"S-T02 (Noise-Aware) with MEM"},{"location":"how-to/run-st01-ghz/#running-on-ibm-quantum-hardware","text":"","title":"Running on IBM Quantum Hardware"},{"location":"how-to/run-st01-ghz/#prerequisites","text":"Set IBM credentials: Unix/macOS: export QISKIT_IBM_TOKEN = \"your_token_here\" # Optional: specify hub/group/project export QISKIT_IBM_INSTANCE = \"ibm-q/open/main\" Windows (PowerShell): $env:QISKIT_IBM_TOKEN = \"your_token_here\" # Optional: specify hub/group/project $env:QISKIT_IBM_INSTANCE = \"ibm-q/open/main\" Windows (Command Prompt): set QISKIT_IBM_TOKEN=your_token_here rem Optional: specify hub/group/project set QISKIT_IBM_INSTANCE=ibm-q/open/main Check remaining quota: All platforms: quartumse runtime-status --backend ibm:ibm_brisbane","title":"Prerequisites"},{"location":"how-to/run-st01-ghz/#run-on-hardware","text":"Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py \\ --backend ibm:ibm_brisbane \\ --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py ` - -backend ibm : ibm_brisbane ` - -variant st02 Note: Hardware runs may take 10-30 minutes depending on queue depth and shot counts.","title":"Run on Hardware"},{"location":"how-to/run-st01-ghz/#using-a-configuration-file","text":"Create config.yaml : backend : provider : ibm name : ibm_brisbane shadow_size : 512 baseline_shots : 2048 # For S-T02 (MEM) mem_shots : 1024 mem_qubits : [ 0 , 1 , 2 , 3 , 4 ] Run with config: Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py \\ --config config.yaml \\ --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py ` - -config config . yaml ` - -variant st02 Override backend from command line: Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py \\ --config config.yaml \\ --backend ibm:ibmq_qasm_simulator \\ --variant st02 Windows: python experiments / shadows / S_T01_ghz_baseline . py ` - -config config . yaml ` - -backend ibm : ibmq_qasm_simulator ` - -variant st02","title":"Using a Configuration File"},{"location":"how-to/run-st01-ghz/#understanding-the-output","text":"","title":"Understanding the Output"},{"location":"how-to/run-st01-ghz/#metrics-explained","text":"CI Coverage - Percentage of observables where true value falls within 95% confidence interval - Target: \u226590% for Phase 1 validation - Typical: 93-100% on simulator, 85-95% on hardware SSR (Shot-Savings Ratio) - Ratio of baseline shots to shadow shots needed for equivalent precision - Formula: SSR = (baseline_shots / shadow_size) \u00d7 (baseline_error / shadow_error)\u00b2 - Target: \u22651.2\u00d7 for Phase 1 validation - Typical: 2-7\u00d7 on simulator, 1.1-3\u00d7 on noisy hardware Status - \u2713 PASS: Both CI coverage \u226590% and SSR \u22651.2\u00d7 - \u2717 FAIL: One or more criteria not met","title":"Metrics Explained"},{"location":"how-to/run-st01-ghz/#observable-notation","text":"ZI = Z on qubit 0, Identity on qubit 1 ZZ = Z on both qubits ZZZ = Z on all three qubits For GHZ states: - Even number of Z operators: expectation = +1 - Odd number of Z operators: expectation = 0","title":"Observable Notation"},{"location":"how-to/run-st01-ghz/#troubleshooting","text":"\"Unable to resolve IBM backend\" - Ensure QISKIT_IBM_TOKEN is set - Check backend name is correct (use quartumse runtime-status to list) - Verify credentials have access to the specified backend \"ModuleNotFoundError: qiskit_ibm_runtime\" - Install runtime dependencies: pip install quartumse[mitigation] \"Insufficient runtime quota\" - Check remaining quota: quartumse runtime-status - Wait for monthly reset or use simulator - Reduce shot counts in config file Poor SSR on hardware (<1.2\u00d7) - Expected for very noisy backends - Try S-T02 variant with MEM (typically improves SSR by 20-40%) - Use backends with lower readout error rates Manifest not found during replay - Check data/manifests/ directory exists - Verify experiment completed successfully (look for \"\u2713 EXPERIMENT PASSED\") - Use full path to manifest file","title":"Troubleshooting"},{"location":"how-to/run-st01-ghz/#next-steps","text":"Replay experiments: See Replay from Manifest Generate reports: See Generate Report Run custom experiments: Modify script or use notebooks Hardware validation: Follow IBM Runtime Runbook","title":"Next Steps"},{"location":"how-to/run-st01-ghz/#related","text":"MEM v1 Guide - Details on measurement error mitigation Testing Guide - Automated test suite and markers Phase 1 Task Checklist - Exit criteria","title":"Related"},{"location":"how-to/run-st01-ghz/#common-flags","text":"The experiment script now shares the same CLI surface as other QuartumSE runs: Flag Purpose Default --backend Override the backend descriptor ( aer_simulator , ibm:ibm_brisbane , etc.) aer_simulator --shadow-size Number of classical shadow shots per GHZ size 500 (configurable via YAML) --seed Random seed used when sampling classical shadows 42 --data-dir Base directory for manifests, shot archives, and MEM data data/ All parameters can also be provided through the optional YAML config file. CLI flags always win over configuration values, making it easy to experiment with different shot budgets or output locations ad-hoc.","title":"Common flags"},{"location":"how-to/run-tests/","text":"Run QuartumSE Tests \u00b6 This guide explains how to exercise the automated test suites, when to run hardware checks, and which notebooks provide manual validation coverage. It replaces the older TESTING_GUIDE.md marketing summary with a concise workflow reference. Test matrix overview \u00b6 Layer Marker(s) Purpose Unit (default) Fast logic tests for shadows, manifests, and utilities Integration integration Exercising estimator + storage pipelines on simulators Slow slow Longer-running variance/CI checks Hardware hardware Requires IBM Quantum credentials and quota Tests use pytest markers so you can opt into the heavier scenarios as needed. Running the suites \u00b6 Unix/macOS: # Core unit tests (quick feedback) pytest tests/unit -v # Fast integration matrix (skip slow + hardware markers) pytest tests -m \"not slow and not hardware\" -v # Include slow scenarios (still skip hardware) pytest tests -m \"not hardware\" -v --durations = 20 # Hardware runs (requires QISKIT_IBM_TOKEN, see ../ops/runtime_runbook.md) pytest tests -m hardware -v Windows: # Core unit tests (quick feedback) pytest tests / unit -v # Fast integration matrix (skip slow + hardware markers) pytest tests -m \"not slow and not hardware\" -v # Include slow scenarios (still skip hardware) pytest tests -m \"not hardware\" -v - -durations = 20 # Hardware runs (requires QISKIT_IBM_TOKEN, see ../ops/runtime_runbook.md) pytest tests -m hardware -v Enable coverage reporting (mirrors the CI configuration) when preparing releases: Unix/macOS: pytest --cov --cov-report = term-missing --cov-report = xml --cov-report = html Windows: pytest - -cov - -cov-report = term-missing - -cov-report = xml - -cov-report = html This writes coverage.xml for Codecov uploads and an htmlcov/ directory for annotated source review. Manual validation notebooks \u00b6 Three curated notebooks cover the major user journeys: notebooks/quickstart_shot_persistence.ipynb \u2013 GHZ classical shadows demo with manifest + Parquet replay. notebooks/noise_aware_shadows_demo.ipynb \u2013 MEM-enhanced (v1) workflow and confusion-matrix diagnostics. notebooks/comprehensive_test_suite.ipynb \u2013 End-to-end path combining CLI, replay, and reporting. The notebooks folder now contains only the actively maintained tutorials so new users can focus on the recommended path. Historical experiments have been retired from version control to keep the repo lightweight. Experiment scripts \u00b6 The active experiment scripts are under experiments/shadows/ and experiments/validation/ . Legacy scaffolds were removed during the repo cleanup, so everything under experiments/ is production-supported. The S\u2011T01 GHZ baseline remains the canonical CLI example: Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator Pass --backend ibm:<device> to exercise the IBM Runtime integration. Hardware runs automatically persist manifests and shot data under data/ . Hardware readiness checklist \u00b6 Before running the hardware test marker or the CLI against real backends: Export QISKIT_IBM_TOKEN (and optional instance overrides). Ensure qiskit-ibm-runtime is installed ( pip install qiskit-ibm-runtime or pip install quartumse[mitigation] ). Confirm remaining quota via quartumse runtime-status . Schedule runs inside the free-tier 10 minute window. See the IBM Runtime runbook for quota guidance and webhook notifications. Troubleshooting tips \u00b6 Missing optional dependencies \u2013 install quartumse[dev,mitigation] to enable MEM notebooks and tests. Runtime quota errors \u2013 rerun on the Aer simulator or wait for the next monthly reset; manifests still capture simulated evidence. Inconsistent notebook output \u2013 clear previous artifacts under notebook_data/ or supply a unique data_dir when instantiating ShadowEstimator . For a broader program view, pair this document with the updated Project Bible and Roadmap .","title":"Run QuartumSE Tests"},{"location":"how-to/run-tests/#run-quartumse-tests","text":"This guide explains how to exercise the automated test suites, when to run hardware checks, and which notebooks provide manual validation coverage. It replaces the older TESTING_GUIDE.md marketing summary with a concise workflow reference.","title":"Run QuartumSE Tests"},{"location":"how-to/run-tests/#test-matrix-overview","text":"Layer Marker(s) Purpose Unit (default) Fast logic tests for shadows, manifests, and utilities Integration integration Exercising estimator + storage pipelines on simulators Slow slow Longer-running variance/CI checks Hardware hardware Requires IBM Quantum credentials and quota Tests use pytest markers so you can opt into the heavier scenarios as needed.","title":"Test matrix overview"},{"location":"how-to/run-tests/#running-the-suites","text":"Unix/macOS: # Core unit tests (quick feedback) pytest tests/unit -v # Fast integration matrix (skip slow + hardware markers) pytest tests -m \"not slow and not hardware\" -v # Include slow scenarios (still skip hardware) pytest tests -m \"not hardware\" -v --durations = 20 # Hardware runs (requires QISKIT_IBM_TOKEN, see ../ops/runtime_runbook.md) pytest tests -m hardware -v Windows: # Core unit tests (quick feedback) pytest tests / unit -v # Fast integration matrix (skip slow + hardware markers) pytest tests -m \"not slow and not hardware\" -v # Include slow scenarios (still skip hardware) pytest tests -m \"not hardware\" -v - -durations = 20 # Hardware runs (requires QISKIT_IBM_TOKEN, see ../ops/runtime_runbook.md) pytest tests -m hardware -v Enable coverage reporting (mirrors the CI configuration) when preparing releases: Unix/macOS: pytest --cov --cov-report = term-missing --cov-report = xml --cov-report = html Windows: pytest - -cov - -cov-report = term-missing - -cov-report = xml - -cov-report = html This writes coverage.xml for Codecov uploads and an htmlcov/ directory for annotated source review.","title":"Running the suites"},{"location":"how-to/run-tests/#manual-validation-notebooks","text":"Three curated notebooks cover the major user journeys: notebooks/quickstart_shot_persistence.ipynb \u2013 GHZ classical shadows demo with manifest + Parquet replay. notebooks/noise_aware_shadows_demo.ipynb \u2013 MEM-enhanced (v1) workflow and confusion-matrix diagnostics. notebooks/comprehensive_test_suite.ipynb \u2013 End-to-end path combining CLI, replay, and reporting. The notebooks folder now contains only the actively maintained tutorials so new users can focus on the recommended path. Historical experiments have been retired from version control to keep the repo lightweight.","title":"Manual validation notebooks"},{"location":"how-to/run-tests/#experiment-scripts","text":"The active experiment scripts are under experiments/shadows/ and experiments/validation/ . Legacy scaffolds were removed during the repo cleanup, so everything under experiments/ is production-supported. The S\u2011T01 GHZ baseline remains the canonical CLI example: Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator Pass --backend ibm:<device> to exercise the IBM Runtime integration. Hardware runs automatically persist manifests and shot data under data/ .","title":"Experiment scripts"},{"location":"how-to/run-tests/#hardware-readiness-checklist","text":"Before running the hardware test marker or the CLI against real backends: Export QISKIT_IBM_TOKEN (and optional instance overrides). Ensure qiskit-ibm-runtime is installed ( pip install qiskit-ibm-runtime or pip install quartumse[mitigation] ). Confirm remaining quota via quartumse runtime-status . Schedule runs inside the free-tier 10 minute window. See the IBM Runtime runbook for quota guidance and webhook notifications.","title":"Hardware readiness checklist"},{"location":"how-to/run-tests/#troubleshooting-tips","text":"Missing optional dependencies \u2013 install quartumse[dev,mitigation] to enable MEM notebooks and tests. Runtime quota errors \u2013 rerun on the Aer simulator or wait for the next monthly reset; manifests still capture simulated evidence. Inconsistent notebook output \u2013 clear previous artifacts under notebook_data/ or supply a unique data_dir when instantiating ShadowEstimator . For a broader program view, pair this document with the updated Project Bible and Roadmap .","title":"Troubleshooting tips"},{"location":"ops/ci_expansion_guide/","text":"CI Matrix Expansion Guide \u00b6 This guide explains how to expand the GitHub Actions CI matrix from the reduced configuration (Phase 1-2) to full multi-platform testing when the repository becomes public. Current State (Phase 1-2) \u00b6 Configuration: Reduced matrix File: .github/workflows/ci.yml Jobs: 1 (Ubuntu + Python 3.11 only) Reason: Private repository + cost optimization matrix : os : [ ubuntu-latest ] python-version : [ \"3.11\" ] What's tested: - \u2705 Code formatting (black, ruff) - \u2705 Type checking (mypy) - \u2705 Core unit tests on Python 3.11 - \u2705 Coverage reporting to Codecov What's NOT tested: - \u274c Windows compatibility - \u274c macOS compatibility - \u274c Python 3.10, 3.12, 3.13 edge cases When to Expand \u00b6 Trigger: Repository becomes public (planned for Phase 3) Phase 3 Checklist Item: - Internal validation complete - SSR \u2265 1.5\u00d7 achieved - Ready for external contributors - No secrets in Git history - \u2192 Make repo public - \u2192 Expand CI matrix Why wait until Phase 3: - Phase 1-2: Private repo, cost-sensitive, core team only - Phase 3+: Public repo, unlimited Actions, external contributors need cross-platform validation Expansion Steps \u00b6 Step 1: Verify Repository is Public \u00b6 Check visibility: # Visit repo settings open https://github.com/QuartumSE/quartumse/settings # Or check with gh CLI gh repo view QuartumSE/quartumse --json visibility -q .visibility # Should output: \"public\" Confirm unlimited Actions: - Go to: https://github.com/settings/billing - Public repos show: \"GitHub Actions: Unlimited\" \u2705 Step 2: Edit CI Workflow \u00b6 File: .github/workflows/ci.yml Find the matrix section (around line 14): matrix : # REDUCED MATRIX: Keeping private repo during Phase 1-2 # ... os : [ ubuntu-latest ] python-version : [ \"3.11\" ] # # EXPAND WHEN REPO GOES PUBLIC (Phase 3+): # ... Replace with full matrix: matrix : os : [ ubuntu-latest , macos-latest , windows-latest ] python-version : [ \"3.10\" , \"3.11\" , \"3.12\" , \"3.13\" ] Remove all the comment blocks about reduced/expanded matrix. Step 3: Test the Expansion \u00b6 Create a test PR: # Create a branch git checkout -b ci/expand-matrix # Edit .github/workflows/ci.yml (apply Step 2 changes) # Commit git add .github/workflows/ci.yml git commit -m \"Expand CI matrix to full platform coverage Repository is now public, enabling full cross-platform testing: - 3 operating systems (Ubuntu, macOS, Windows) - 4 Python versions (3.10, 3.11, 3.12, 3.13) - Total: 12 test jobs per run This provides comprehensive validation for external contributors and ensures QuartumSE works across all supported environments.\" # Push and create PR git push -u origin ci/expand-matrix gh pr create --title \"Expand CI matrix for public repo\" --body \"Restores full cross-platform testing now that repo is public. See docs/ops/ci_expansion_guide.md\" Watch the PR checks: - All 12 jobs should run - Expect some Windows/macOS-specific issues initially - Fix any platform-specific failures before merging Step 4: Fix Platform-Specific Issues \u00b6 Common issues when expanding to full matrix: Windows Path Issues \u00b6 # Before (Unix-only) path = \"data/manifests/file.json\" # After (cross-platform) from pathlib import Path path = Path ( \"data\" ) / \"manifests\" / \"file.json\" macOS Line Ending Issues \u00b6 # In CI workflow, normalize line endings - name: Normalize line endings ( macOS ) if : runner.os == 'macOS' run: | find . -name \"*.py\" -exec dos2unix {} \\; Python 3.13 Compatibility \u00b6 # Check for deprecated features removed in 3.13 # Update dependencies if needed pip install -- upgrade qiskit qiskit - aer Step 5: Update Documentation \u00b6 Update references mentioning reduced matrix: CHANGELOG.md: ## [Unreleased] ### Changed - Expanded CI matrix to full platform coverage (12 jobs) now that repository is public README.md badges: [ ![CI ]( https://github.com/quartumse/quartumse/workflows/CI/badge.svg )](https://github.com/quartumse/quartumse/actions) Badge will now show \"12 passing\" instead of \"1 passing\" CONTRIBUTING.md: Add note about cross-platform testing: ## Testing Pull requests are tested on: - Ubuntu, macOS, Windows - Python 3.10, 3.11, 3.12, 3.13 Ensure your changes work on all platforms before submitting. Step 6: Monitor Actions Usage \u00b6 Even with unlimited minutes, monitor for efficiency: # Check Actions usage gh api /repos/QuartumSE/quartumse/actions/runs \\ --jq '.workflow_runs[] | select(.name==\"CI\") | {id, status, conclusion, duration: .run_duration_ms}' # View workflow run times gh run list --workflow = ci.yml --limit 10 Optimization tips if runs are slow: - Use fail-fast: true to cancel remaining jobs on first failure - Cache pip dependencies with actions/cache - Run expensive checks (mypy, integration tests) only on one platform Rollback Procedure \u00b6 If full matrix causes issues, temporarily rollback: Quick rollback: matrix : os : [ ubuntu-latest ] # Rollback to single platform python-version : [ \"3.11\" ] Fix issues offline: - Test problematic platforms locally - Fix compatibility issues - Re-expand when ready Expected Results \u00b6 Before Expansion (Current) \u00b6 \u2705 test (ubuntu-latest, 3.11) Time: ~3 minutes Coverage: Core functionality only After Expansion (Phase 3+) \u00b6 \u2705 test (ubuntu-latest, 3.10) \u2705 test (ubuntu-latest, 3.11) \u2705 test (ubuntu-latest, 3.12) \u2705 test (ubuntu-latest, 3.13) \u2705 test (macos-latest, 3.10) \u2705 test (macos-latest, 3.11) \u2705 test (macos-latest, 3.12) \u2705 test (macos-latest, 3.13) \u2705 test (windows-latest, 3.10) \u2705 test (windows-latest, 3.11) \u2705 test (windows-latest, 3.12) \u2705 test (windows-latest, 3.13) Time: ~8-10 minutes (parallel execution) Coverage: Full cross-platform validation Validation Checklist \u00b6 After expansion, verify: [ ] All 12 jobs complete successfully [ ] Coverage reports upload correctly (Ubuntu 3.11 job) [ ] No platform-specific test failures [ ] Codecov badge shows correct coverage [ ] CI badge shows \"12 passing\" [ ] PR checks show all jobs [ ] No excessive Actions usage warnings [ ] Windows path handling works [ ] macOS-specific issues resolved [ ] Python 3.10-3.13 all pass Cost Analysis \u00b6 Private Repo (Current) \u00b6 Matrix: 1 job Duration: 3 min Cost per run: 3 minutes (Ubuntu 1\u00d7) Monthly estimate: ~100-300 minutes Status: Within 2,000 min free tier Public Repo (After Expansion) \u00b6 Matrix: 12 jobs Duration: 3-4 min per job (parallel) Cost per run: 0 minutes (unlimited for public) Monthly estimate: Unlimited \u2705 Status: Free forever Key takeaway: Expansion is cost-free once repo is public! Related Documentation \u00b6 Phase 1 Task Checklist Roadmap - Phase 3 milestones CI Workflow - Current configuration GitHub Actions Pricing Questions? \u00b6 When should I expand? \u2192 When repo becomes public (Phase 3+) Can I expand before repo is public? \u2192 Not recommended (cost ~$20-50/month for private repos) What if some jobs fail after expansion? \u2192 Normal! Fix platform-specific issues and re-run Do I need to change anything else? \u2192 Just update docs mentioning \"reduced matrix\" or \"Ubuntu only\" How do I test locally before expanding? \u2192 Use tox with multiple Python versions, test on VM for other OSes Last updated: 2025-10-30 Next review: Phase 3 (Internal Validation) - Target Mar 2026","title":"CI Matrix Expansion Guide"},{"location":"ops/ci_expansion_guide/#ci-matrix-expansion-guide","text":"This guide explains how to expand the GitHub Actions CI matrix from the reduced configuration (Phase 1-2) to full multi-platform testing when the repository becomes public.","title":"CI Matrix Expansion Guide"},{"location":"ops/ci_expansion_guide/#current-state-phase-1-2","text":"Configuration: Reduced matrix File: .github/workflows/ci.yml Jobs: 1 (Ubuntu + Python 3.11 only) Reason: Private repository + cost optimization matrix : os : [ ubuntu-latest ] python-version : [ \"3.11\" ] What's tested: - \u2705 Code formatting (black, ruff) - \u2705 Type checking (mypy) - \u2705 Core unit tests on Python 3.11 - \u2705 Coverage reporting to Codecov What's NOT tested: - \u274c Windows compatibility - \u274c macOS compatibility - \u274c Python 3.10, 3.12, 3.13 edge cases","title":"Current State (Phase 1-2)"},{"location":"ops/ci_expansion_guide/#when-to-expand","text":"Trigger: Repository becomes public (planned for Phase 3) Phase 3 Checklist Item: - Internal validation complete - SSR \u2265 1.5\u00d7 achieved - Ready for external contributors - No secrets in Git history - \u2192 Make repo public - \u2192 Expand CI matrix Why wait until Phase 3: - Phase 1-2: Private repo, cost-sensitive, core team only - Phase 3+: Public repo, unlimited Actions, external contributors need cross-platform validation","title":"When to Expand"},{"location":"ops/ci_expansion_guide/#expansion-steps","text":"","title":"Expansion Steps"},{"location":"ops/ci_expansion_guide/#step-1-verify-repository-is-public","text":"Check visibility: # Visit repo settings open https://github.com/QuartumSE/quartumse/settings # Or check with gh CLI gh repo view QuartumSE/quartumse --json visibility -q .visibility # Should output: \"public\" Confirm unlimited Actions: - Go to: https://github.com/settings/billing - Public repos show: \"GitHub Actions: Unlimited\" \u2705","title":"Step 1: Verify Repository is Public"},{"location":"ops/ci_expansion_guide/#step-2-edit-ci-workflow","text":"File: .github/workflows/ci.yml Find the matrix section (around line 14): matrix : # REDUCED MATRIX: Keeping private repo during Phase 1-2 # ... os : [ ubuntu-latest ] python-version : [ \"3.11\" ] # # EXPAND WHEN REPO GOES PUBLIC (Phase 3+): # ... Replace with full matrix: matrix : os : [ ubuntu-latest , macos-latest , windows-latest ] python-version : [ \"3.10\" , \"3.11\" , \"3.12\" , \"3.13\" ] Remove all the comment blocks about reduced/expanded matrix.","title":"Step 2: Edit CI Workflow"},{"location":"ops/ci_expansion_guide/#step-3-test-the-expansion","text":"Create a test PR: # Create a branch git checkout -b ci/expand-matrix # Edit .github/workflows/ci.yml (apply Step 2 changes) # Commit git add .github/workflows/ci.yml git commit -m \"Expand CI matrix to full platform coverage Repository is now public, enabling full cross-platform testing: - 3 operating systems (Ubuntu, macOS, Windows) - 4 Python versions (3.10, 3.11, 3.12, 3.13) - Total: 12 test jobs per run This provides comprehensive validation for external contributors and ensures QuartumSE works across all supported environments.\" # Push and create PR git push -u origin ci/expand-matrix gh pr create --title \"Expand CI matrix for public repo\" --body \"Restores full cross-platform testing now that repo is public. See docs/ops/ci_expansion_guide.md\" Watch the PR checks: - All 12 jobs should run - Expect some Windows/macOS-specific issues initially - Fix any platform-specific failures before merging","title":"Step 3: Test the Expansion"},{"location":"ops/ci_expansion_guide/#step-4-fix-platform-specific-issues","text":"Common issues when expanding to full matrix:","title":"Step 4: Fix Platform-Specific Issues"},{"location":"ops/ci_expansion_guide/#windows-path-issues","text":"# Before (Unix-only) path = \"data/manifests/file.json\" # After (cross-platform) from pathlib import Path path = Path ( \"data\" ) / \"manifests\" / \"file.json\"","title":"Windows Path Issues"},{"location":"ops/ci_expansion_guide/#macos-line-ending-issues","text":"# In CI workflow, normalize line endings - name: Normalize line endings ( macOS ) if : runner.os == 'macOS' run: | find . -name \"*.py\" -exec dos2unix {} \\;","title":"macOS Line Ending Issues"},{"location":"ops/ci_expansion_guide/#python-313-compatibility","text":"# Check for deprecated features removed in 3.13 # Update dependencies if needed pip install -- upgrade qiskit qiskit - aer","title":"Python 3.13 Compatibility"},{"location":"ops/ci_expansion_guide/#step-5-update-documentation","text":"Update references mentioning reduced matrix: CHANGELOG.md: ## [Unreleased] ### Changed - Expanded CI matrix to full platform coverage (12 jobs) now that repository is public README.md badges: [ ![CI ]( https://github.com/quartumse/quartumse/workflows/CI/badge.svg )](https://github.com/quartumse/quartumse/actions) Badge will now show \"12 passing\" instead of \"1 passing\" CONTRIBUTING.md: Add note about cross-platform testing: ## Testing Pull requests are tested on: - Ubuntu, macOS, Windows - Python 3.10, 3.11, 3.12, 3.13 Ensure your changes work on all platforms before submitting.","title":"Step 5: Update Documentation"},{"location":"ops/ci_expansion_guide/#step-6-monitor-actions-usage","text":"Even with unlimited minutes, monitor for efficiency: # Check Actions usage gh api /repos/QuartumSE/quartumse/actions/runs \\ --jq '.workflow_runs[] | select(.name==\"CI\") | {id, status, conclusion, duration: .run_duration_ms}' # View workflow run times gh run list --workflow = ci.yml --limit 10 Optimization tips if runs are slow: - Use fail-fast: true to cancel remaining jobs on first failure - Cache pip dependencies with actions/cache - Run expensive checks (mypy, integration tests) only on one platform","title":"Step 6: Monitor Actions Usage"},{"location":"ops/ci_expansion_guide/#rollback-procedure","text":"If full matrix causes issues, temporarily rollback: Quick rollback: matrix : os : [ ubuntu-latest ] # Rollback to single platform python-version : [ \"3.11\" ] Fix issues offline: - Test problematic platforms locally - Fix compatibility issues - Re-expand when ready","title":"Rollback Procedure"},{"location":"ops/ci_expansion_guide/#expected-results","text":"","title":"Expected Results"},{"location":"ops/ci_expansion_guide/#before-expansion-current","text":"\u2705 test (ubuntu-latest, 3.11) Time: ~3 minutes Coverage: Core functionality only","title":"Before Expansion (Current)"},{"location":"ops/ci_expansion_guide/#after-expansion-phase-3","text":"\u2705 test (ubuntu-latest, 3.10) \u2705 test (ubuntu-latest, 3.11) \u2705 test (ubuntu-latest, 3.12) \u2705 test (ubuntu-latest, 3.13) \u2705 test (macos-latest, 3.10) \u2705 test (macos-latest, 3.11) \u2705 test (macos-latest, 3.12) \u2705 test (macos-latest, 3.13) \u2705 test (windows-latest, 3.10) \u2705 test (windows-latest, 3.11) \u2705 test (windows-latest, 3.12) \u2705 test (windows-latest, 3.13) Time: ~8-10 minutes (parallel execution) Coverage: Full cross-platform validation","title":"After Expansion (Phase 3+)"},{"location":"ops/ci_expansion_guide/#validation-checklist","text":"After expansion, verify: [ ] All 12 jobs complete successfully [ ] Coverage reports upload correctly (Ubuntu 3.11 job) [ ] No platform-specific test failures [ ] Codecov badge shows correct coverage [ ] CI badge shows \"12 passing\" [ ] PR checks show all jobs [ ] No excessive Actions usage warnings [ ] Windows path handling works [ ] macOS-specific issues resolved [ ] Python 3.10-3.13 all pass","title":"Validation Checklist"},{"location":"ops/ci_expansion_guide/#cost-analysis","text":"","title":"Cost Analysis"},{"location":"ops/ci_expansion_guide/#private-repo-current","text":"Matrix: 1 job Duration: 3 min Cost per run: 3 minutes (Ubuntu 1\u00d7) Monthly estimate: ~100-300 minutes Status: Within 2,000 min free tier","title":"Private Repo (Current)"},{"location":"ops/ci_expansion_guide/#public-repo-after-expansion","text":"Matrix: 12 jobs Duration: 3-4 min per job (parallel) Cost per run: 0 minutes (unlimited for public) Monthly estimate: Unlimited \u2705 Status: Free forever Key takeaway: Expansion is cost-free once repo is public!","title":"Public Repo (After Expansion)"},{"location":"ops/ci_expansion_guide/#related-documentation","text":"Phase 1 Task Checklist Roadmap - Phase 3 milestones CI Workflow - Current configuration GitHub Actions Pricing","title":"Related Documentation"},{"location":"ops/ci_expansion_guide/#questions","text":"When should I expand? \u2192 When repo becomes public (Phase 3+) Can I expand before repo is public? \u2192 Not recommended (cost ~$20-50/month for private repos) What if some jobs fail after expansion? \u2192 Normal! Fix platform-specific issues and re-run Do I need to change anything else? \u2192 Just update docs mentioning \"reduced matrix\" or \"Ubuntu only\" How do I test locally before expanding? \u2192 Use tox with multiple Python versions, test on VM for other OSes Last updated: 2025-10-30 Next review: Phase 3 (Internal Validation) - Target Mar 2026","title":"Questions?"},{"location":"ops/runtime_runbook/","text":"IBM Runtime Operations Runbook \u00b6 This runbook tracks IBM Quantum free-tier runtime usage and provides quick recovery paths when the quota is depleted. Free-tier quota snapshot \u00b6 Monthly allocation: 600 seconds (10 minutes) of wall-clock runtime on physical devices per calendar month under the IBM Quantum Free/Open plan. Concurrency limits: Up to 5 pending jobs and 1 running job per account/instance at a time. Excess submissions are rejected until the queue drains. Reset schedule: Allocation resets at 00:00 UTC on the first day of each month. Unused minutes do not roll over. Checking remaining runtime \u00b6 Via IBM Quantum portal \u00b6 Sign in to https://quantum.ibm.com . Open My Account \u2192 Usage and select the active hub/group/project. Review the Runtime usage panel for remaining seconds, refresh date, and pending job caps. Via QuartumSE CLI (preferred for automation) \u00b6 quartumse runtime-status --backend ibm:ibmq_brisbane --instance ibm-q/open/main Dependency: Ensure qiskit-ibm-runtime is installed ( pip install qiskit-ibm-runtime ). Key behaviours: The command queries queue depth, quota consumption, and refresh date using the IBM Runtime API. \u3010F:src/quartumse/utils/runtime_monitor.py\u2020L44-L193\u3011 Pass --json for machine-readable output (suitable for CI dashboards). Provide credentials via standard environment variables ( QISKIT_IBM_TOKEN , QISKIT_IBM_CHANNEL , QISKIT_IBM_INSTANCE ) or CLI overrides. Notifications \u00b6 Set QUARTUMSE_SLACK_WEBHOOK (or pass --slack-webhook ) to push the status summary into project chat. Use --dry-run during testing to avoid posting. \u3010F:src/quartumse/cli.py\u2020L119-L213\u3011 The webhook payload includes queue depth, quota usage, and the next reset date for quick triage. Fallback backends when quota is exhausted \u00b6 Scenario Immediate action Notes Free-tier minutes depleted mid-sprint Switch estimator config to ibm:ibmq_qasm_simulator or aer_simulator for continued functional work. Simulator runs do not consume runtime quota but still validate integration paths. Hardware-specific regression blocking validation Use qiskit.providers.fake_provider fake backends to reproduce calibration-dependent logic without hardware access. Capture manifests to document the simulated evidence until hardware minutes refresh. Queue cap reached (max pending jobs) Pause new submissions, monitor quartumse runtime-status --json until pendingJobs < maxPendingJobs . CLI returns job caps from the active instance, mirroring portal data. Operational cadence \u00b6 Monthly review: First business day of each month, review the runtime usage dashboard, confirm remaining free-tier minutes, and update fallback readiness in this runbook. Weekly spot-check (Mondays): Run quartumse runtime-status --json against the primary hardware backend and capture output in the team notebook to watch queue health trends. Incident response tips \u00b6 If quota hits zero before reset, pivot planned hardware executions to simulator-only experiments and reschedule hardware runs after the reset date. When backlog persists beyond 24 hours, escalate in the team comms channel with the webhook payload and consider re-prioritising experiments toward simulator coverage. Document any quota-related delays in STATUS_REPORT.md for visibility during phase reviews.","title":"IBM Runtime Runbook"},{"location":"ops/runtime_runbook/#ibm-runtime-operations-runbook","text":"This runbook tracks IBM Quantum free-tier runtime usage and provides quick recovery paths when the quota is depleted.","title":"IBM Runtime Operations Runbook"},{"location":"ops/runtime_runbook/#free-tier-quota-snapshot","text":"Monthly allocation: 600 seconds (10 minutes) of wall-clock runtime on physical devices per calendar month under the IBM Quantum Free/Open plan. Concurrency limits: Up to 5 pending jobs and 1 running job per account/instance at a time. Excess submissions are rejected until the queue drains. Reset schedule: Allocation resets at 00:00 UTC on the first day of each month. Unused minutes do not roll over.","title":"Free-tier quota snapshot"},{"location":"ops/runtime_runbook/#checking-remaining-runtime","text":"","title":"Checking remaining runtime"},{"location":"ops/runtime_runbook/#via-ibm-quantum-portal","text":"Sign in to https://quantum.ibm.com . Open My Account \u2192 Usage and select the active hub/group/project. Review the Runtime usage panel for remaining seconds, refresh date, and pending job caps.","title":"Via IBM Quantum portal"},{"location":"ops/runtime_runbook/#via-quartumse-cli-preferred-for-automation","text":"quartumse runtime-status --backend ibm:ibmq_brisbane --instance ibm-q/open/main Dependency: Ensure qiskit-ibm-runtime is installed ( pip install qiskit-ibm-runtime ). Key behaviours: The command queries queue depth, quota consumption, and refresh date using the IBM Runtime API. \u3010F:src/quartumse/utils/runtime_monitor.py\u2020L44-L193\u3011 Pass --json for machine-readable output (suitable for CI dashboards). Provide credentials via standard environment variables ( QISKIT_IBM_TOKEN , QISKIT_IBM_CHANNEL , QISKIT_IBM_INSTANCE ) or CLI overrides.","title":"Via QuartumSE CLI (preferred for automation)"},{"location":"ops/runtime_runbook/#notifications","text":"Set QUARTUMSE_SLACK_WEBHOOK (or pass --slack-webhook ) to push the status summary into project chat. Use --dry-run during testing to avoid posting. \u3010F:src/quartumse/cli.py\u2020L119-L213\u3011 The webhook payload includes queue depth, quota usage, and the next reset date for quick triage.","title":"Notifications"},{"location":"ops/runtime_runbook/#fallback-backends-when-quota-is-exhausted","text":"Scenario Immediate action Notes Free-tier minutes depleted mid-sprint Switch estimator config to ibm:ibmq_qasm_simulator or aer_simulator for continued functional work. Simulator runs do not consume runtime quota but still validate integration paths. Hardware-specific regression blocking validation Use qiskit.providers.fake_provider fake backends to reproduce calibration-dependent logic without hardware access. Capture manifests to document the simulated evidence until hardware minutes refresh. Queue cap reached (max pending jobs) Pause new submissions, monitor quartumse runtime-status --json until pendingJobs < maxPendingJobs . CLI returns job caps from the active instance, mirroring portal data.","title":"Fallback backends when quota is exhausted"},{"location":"ops/runtime_runbook/#operational-cadence","text":"Monthly review: First business day of each month, review the runtime usage dashboard, confirm remaining free-tier minutes, and update fallback readiness in this runbook. Weekly spot-check (Mondays): Run quartumse runtime-status --json against the primary hardware backend and capture output in the team notebook to watch queue health trends.","title":"Operational cadence"},{"location":"ops/runtime_runbook/#incident-response-tips","text":"If quota hits zero before reset, pivot planned hardware executions to simulator-only experiments and reschedule hardware runs after the reset date. When backlog persists beyond 24 hours, escalate in the team comms channel with the webhook payload and consider re-prioritising experiments toward simulator coverage. Document any quota-related delays in STATUS_REPORT.md for visibility during phase reviews.","title":"Incident response tips"},{"location":"reference/cli/","text":"Usage: quartumse [OPTIONS] COMMAND [ARGS]... Quantum measurement optimization toolkit +- Options -------------------------------------------------------------------+ | --install-completion Install completion for the current shell. | | --show-completion Show completion for the current shell, to | | copy it or customize the installation. | | --help Show this message and exit. | +-----------------------------------------------------------------------------+ +- Commands ------------------------------------------------------------------+ | version Show QuartumSE version. | | run Validate configuration and resolve experiment backend. | | calibrate-readout Calibrate readout confusion matrices and persist | | metadata. | | report Generate report from manifest. | | benchmark Run benchmark suite. | | runtime-status Report IBM queue depth and runtime quota usage. | +-----------------------------------------------------------------------------+","title":"CLI"},{"location":"reference/observable-notation/","text":"Observable Notation Reference \u00b6 QuartumSE uses Pauli strings to specify quantum observables for expectation value estimation. This guide explains the notation, common patterns, and expected values for standard quantum states. Pauli String Syntax \u00b6 Basic Format \u00b6 An observable is written as a string of Pauli operators, one per qubit: from quartumse.shadows.core import Observable # Single-qubit observables Observable ( \"X\" ) # X (Pauli-X) on qubit 0 Observable ( \"Y\" ) # Y (Pauli-Y) on qubit 0 Observable ( \"Z\" ) # Z (Pauli-Z) on qubit 0 Observable ( \"I\" ) # I (Identity) on qubit 0 # Multi-qubit observables Observable ( \"ZII\" ) # Z on qubit 0, Identity on qubits 1 and 2 Observable ( \"ZZI\" ) # Z on qubits 0 and 1, Identity on qubit 2 Observable ( \"ZZZ\" ) # Z on all three qubits Observable ( \"XXX\" ) # X on all three qubits Qubit Ordering \u00b6 QuartumSE uses little-endian (Qiskit-style) qubit ordering: - Leftmost character = qubit 0 - Rightmost character = highest-index qubit # For a 3-qubit circuit Observable ( \"XYZ\" ) # X on qubit 0 # Y on qubit 1 # Z on qubit 2 Coefficients \u00b6 Observables can have multiplicative coefficients: Observable ( \"ZZ\" , coefficient = 0.5 ) # 0.5 \u00d7 Z\u2080Z\u2081 Observable ( \"XX\" , coefficient =- 1.0 ) # -1.0 \u00d7 X\u2080X\u2081 Output format: Results use {coefficient}*{pauli_string} notation: result . observables . keys () # Returns: ['1.0*ZII', '1.0*ZZI', '1.0*ZZZ'] Pauli Operator Properties \u00b6 Eigenvalues \u00b6 All Pauli operators have eigenvalues \u00b11 : Operator Eigenstate Eigenvalue X |+\u27e9 = (|0\u27e9 + |1\u27e9)/\u221a2 +1 X |-\u27e9 = (|0\u27e9 - |1\u27e9)/\u221a2 -1 Y |i+\u27e9 = (|0\u27e9 + i|1\u27e9)/\u221a2 +1 Y |i-\u27e9 = (|0\u27e9 - i|1\u27e9)/\u221a2 -1 Z |0\u27e9 +1 Z |1\u27e9 -1 Commutation Relations \u00b6 Observables commute if they share qubits only on I or matching Pauli operators Observables anticommute if they differ on an odd number of qubits # Commuting observables (can measure simultaneously) Observable ( \"ZII\" ) and Observable ( \"ZZI\" ) # \u2713 commute Observable ( \"XII\" ) and Observable ( \"YZZ\" ) # \u2713 commute (different qubits) # Anticommuting observables (cannot measure simultaneously) Observable ( \"XII\" ) and Observable ( \"ZII\" ) # \u2717 anticommute (differ on qubit 0) Observable ( \"ZZ\" ) and Observable ( \"XZ\" ) # \u2717 anticommute (differ on qubit 0) Expected Values for Common States \u00b6 Computational Basis States \u00b6 |00\u27e9 state: Observable ( \"ZI\" ): + 1.0 # Z\u2080 = +1 (qubit 0 is |0\u27e9) Observable ( \"IZ\" ): + 1.0 # Z\u2081 = +1 (qubit 1 is |0\u27e9) Observable ( \"ZZ\" ): + 1.0 # Z\u2080Z\u2081 = (+1)(+1) = +1 Observable ( \"XI\" ): 0.0 # X\u2080 = 0 (|0\u27e9 is equal superposition of X eigenstates) Observable ( \"XX\" ): 0.0 # X\u2080X\u2081 = 0 |11\u27e9 state: Observable ( \"ZI\" ): - 1.0 # Z\u2080 = -1 (qubit 0 is |1\u27e9) Observable ( \"IZ\" ): - 1.0 # Z\u2081 = -1 (qubit 1 is |1\u27e9) Observable ( \"ZZ\" ): + 1.0 # Z\u2080Z\u2081 = (-1)(-1) = +1 Observable ( \"XI\" ): 0.0 # X\u2080 = 0 Observable ( \"XX\" ): 0.0 # X\u2080X\u2081 = 0 Bell States \u00b6 |\u03a6\u207a\u27e9 = (|00\u27e9 + |11\u27e9)/\u221a2 (Bell state): Observable ( \"ZI\" ): 0.0 # Equal |0\u27e9 and |1\u27e9 on qubit 0 Observable ( \"IZ\" ): 0.0 # Equal |0\u27e9 and |1\u27e9 on qubit 1 Observable ( \"ZZ\" ): + 1.0 # Correlated: both qubits have same parity Observable ( \"XX\" ): + 1.0 # Both qubits in X-basis |+\u27e9 Observable ( \"YY\" ): - 1.0 # Y correlation Observable ( \"XY\" ): 0.0 # No XY correlation |\u03a8\u207b\u27e9 = (|01\u27e9 - |10\u27e9)/\u221a2 (singlet state): Observable ( \"ZI\" ): 0.0 Observable ( \"IZ\" ): 0.0 Observable ( \"ZZ\" ): - 1.0 # Anti-correlated Observable ( \"XX\" ): - 1.0 Observable ( \"YY\" ): - 1.0 Observable ( \"XZ\" ): 0.0 GHZ States \u00b6 |GHZ(3)\u27e9 = (|000\u27e9 + |111\u27e9)/\u221a2: GHZ states are stabilized by even-parity Z operators: # Single-qubit Z observables \u2192 0 (equal superposition) Observable ( \"ZII\" ): 0.0 Observable ( \"IZI\" ): 0.0 Observable ( \"IIZ\" ): 0.0 # Two-qubit Z observables \u2192 +1 (even parity stabilizers) Observable ( \"ZZI\" ): + 1.0 Observable ( \"ZIZ\" ): + 1.0 Observable ( \"IZZ\" ): + 1.0 # Three-qubit Z observables \u2192 -1 (odd parity) Observable ( \"ZZZ\" ): - 1.0 # X observables Observable ( \"XXX\" ): + 1.0 # All qubits in |+\u27e9 superposition Observable ( \"XII\" ): 0.0 # Single X \u2192 0 Observable ( \"XXI\" ): 0.0 # Two X's \u2192 0 Rule for GHZ: - Even number of Z operators \u2192 +1 - Odd number of Z operators \u2192 0 (for single Z) or -1 (for all Z) |GHZ(4)\u27e9 = (|0000\u27e9 + |1111\u27e9)/\u221a2: Observable ( \"ZIII\" ): 0.0 Observable ( \"ZZII\" ): + 1.0 Observable ( \"ZZZI\" ): + 1.0 Observable ( \"ZZZZ\" ): + 1.0 # Even number (4) of Z's Observable ( \"XXXX\" ): + 1.0 W States \u00b6 |W(3)\u27e9 = (|001\u27e9 + |010\u27e9 + |100\u27e9)/\u221a3: Observable ( \"ZII\" ): - 1 / 3 # Two |0\u27e9 components, one |1\u27e9 Observable ( \"IZI\" ): - 1 / 3 Observable ( \"IIZ\" ): - 1 / 3 Observable ( \"ZZZ\" ): - 1 / 3 # Mix of even/odd parities Observable ( \"XXX\" ): 1 / 3 # Partial correlation Hamiltonian Observables \u00b6 Many quantum algorithms estimate Hamiltonian energy : Ising Model (1D chain) \u00b6 # H = \u03a3\u1d62 J\u1d62Z\u1d62Z\u1d62\u208a\u2081 + \u03a3\u1d62h\u1d62Z\u1d62 # For 3 qubits with J=-1.0, h=0.5: Observable ( \"ZZI\" , coefficient =- 1.0 ) # Z\u2080Z\u2081 interaction Observable ( \"IZZ\" , coefficient =- 1.0 ) # Z\u2081Z\u2082 interaction Observable ( \"ZII\" , coefficient = 0.5 ) # Z\u2080 field Observable ( \"IZI\" , coefficient = 0.5 ) # Z\u2081 field Observable ( \"IIZ\" , coefficient = 0.5 ) # Z\u2082 field # Total energy = sum of all observable expectation values Molecular Hamiltonians (VQE) \u00b6 # H\u2082 molecule (example coefficients) Observable ( \"II\" , coefficient =- 0.8105 ) # Identity term Observable ( \"ZI\" , coefficient = 0.1721 ) # Z\u2080 term Observable ( \"IZ\" , coefficient = 0.1721 ) # Z\u2081 term Observable ( \"ZZ\" , coefficient =- 0.2279 ) # Z\u2080Z\u2081 correlation Observable ( \"XX\" , coefficient = 0.1809 ) # XX exchange Observable ( \"YY\" , coefficient = 0.1809 ) # YY exchange Interpreting Results \u00b6 Confidence Intervals \u00b6 QuartumSE reports 95% confidence intervals around expectation values: result . observables [ \"1.0*ZZZ\" ] # { # 'expectation_value': -0.9922, # 'variance': 0.0156, # 'ci_95': (-1.0353, -0.9491), # 'ci_width': 0.0862 # } Interpretation: - Expectation value : -0.9922 (close to theoretical -1.0 for GHZ) - Variance : 0.0156 (measurement uncertainty) - 95% CI : [-1.0353, -0.9491] (true value likely in this range) - CI width : 0.0862 (precision measure; smaller is better) Comparing to Theory \u00b6 Example: GHZ(3) validation # Theoretical expectations theory = { \"1.0*ZII\" : 0.0 , \"1.0*ZZI\" : + 1.0 , \"1.0*ZZZ\" : - 1.0 , } # Experimental results for obs_str , data in result . observables . items (): estimated = data [ 'expectation_value' ] expected = theory [ obs_str ] ci = data [ 'ci_95' ] # Check if theory is within confidence interval in_ci = ci [ 0 ] <= expected <= ci [ 1 ] status = \"\u2713\" if in_ci else \"\u2717\" print ( f \" { obs_str : 10 } Est: { estimated : +.4f } \" f \"Theory: { expected : +.4f } { status } \" ) Output: 1.0*ZII Est: +0.0039 Theory: +0.0000 \u2713 1.0*ZZI Est: +0.9961 Theory: +1.0000 \u2713 1.0*ZZZ Est: -0.9922 Theory: -1.0000 \u2713 Common Patterns \u00b6 Entanglement Witnesses \u00b6 CHSH Inequality: # S = |\u27e8XX\u27e9 + \u27e8XY\u27e9 + \u27e8YX\u27e9 - \u27e8YY\u27e9| # Classical limit: S \u2264 2 # Quantum (Bell state): S = 2\u221a2 \u2248 2.828 Observable ( \"XX\" ) # \u27e8XX\u27e9 Observable ( \"XY\" ) # \u27e8XY\u27e9 Observable ( \"YX\" ) # \u27e8YX\u27e9 Observable ( \"YY\" ) # \u27e8YY\u27e9 Fidelity Estimation \u00b6 Overlap with target state |\u03c8\u27e9: # F = \u27e8\u03c8|\u03c1|\u03c8\u27e9 = \u03a3\u1d62 \u27e8P\u1d62\u27e9 / 2\u207f # where P\u1d62 are stabilizer observables # For GHZ(3): observables = [ Observable ( \"ZZI\" ), # Stabilizer 1 Observable ( \"IZZ\" ), # Stabilizer 2 ] # F \u2248 (1 + \u27e8ZZI\u27e9 + \u27e8IZZ\u27e9 + \u27e8ZZI\u27e9\u27e8IZZ\u27e9) / 4 Symmetry Testing \u00b6 Check parity symmetry: # Even parity: should be +1 or -1 Observable ( \"ZZZZ\" ) # If result \u2248 0, state lacks definite parity # (e.g., equal mixture of even/odd states) Further Reading \u00b6 Qiskit Observable Tutorial : qiskit.org/documentation/tutorials/operators Pauli Operator Algebra : See Nielsen & Chuang, Chapter 2 Classical Shadows Theory : Huang, Kueng, Preskill (2020) QuartumSE Architecture : docs/explanation/shadows-theory.md Quick Reference \u00b6 Observable Name Measures I Identity Always +1 X Pauli-X X-basis projection Y Pauli-Y Y-basis projection Z Pauli-Z Computational basis projection ZZ Z-correlation Parity between two qubits XX X-correlation X-basis entanglement ZZZ 3-qubit parity Stabilizer for GHZ states Tip: When debugging, start with Z observables ( ZI , ZZ , etc.) since they measure in the computational basis and are easier to interpret from bitstring histograms.","title":"Observable Notation"},{"location":"reference/observable-notation/#observable-notation-reference","text":"QuartumSE uses Pauli strings to specify quantum observables for expectation value estimation. This guide explains the notation, common patterns, and expected values for standard quantum states.","title":"Observable Notation Reference"},{"location":"reference/observable-notation/#pauli-string-syntax","text":"","title":"Pauli String Syntax"},{"location":"reference/observable-notation/#basic-format","text":"An observable is written as a string of Pauli operators, one per qubit: from quartumse.shadows.core import Observable # Single-qubit observables Observable ( \"X\" ) # X (Pauli-X) on qubit 0 Observable ( \"Y\" ) # Y (Pauli-Y) on qubit 0 Observable ( \"Z\" ) # Z (Pauli-Z) on qubit 0 Observable ( \"I\" ) # I (Identity) on qubit 0 # Multi-qubit observables Observable ( \"ZII\" ) # Z on qubit 0, Identity on qubits 1 and 2 Observable ( \"ZZI\" ) # Z on qubits 0 and 1, Identity on qubit 2 Observable ( \"ZZZ\" ) # Z on all three qubits Observable ( \"XXX\" ) # X on all three qubits","title":"Basic Format"},{"location":"reference/observable-notation/#qubit-ordering","text":"QuartumSE uses little-endian (Qiskit-style) qubit ordering: - Leftmost character = qubit 0 - Rightmost character = highest-index qubit # For a 3-qubit circuit Observable ( \"XYZ\" ) # X on qubit 0 # Y on qubit 1 # Z on qubit 2","title":"Qubit Ordering"},{"location":"reference/observable-notation/#coefficients","text":"Observables can have multiplicative coefficients: Observable ( \"ZZ\" , coefficient = 0.5 ) # 0.5 \u00d7 Z\u2080Z\u2081 Observable ( \"XX\" , coefficient =- 1.0 ) # -1.0 \u00d7 X\u2080X\u2081 Output format: Results use {coefficient}*{pauli_string} notation: result . observables . keys () # Returns: ['1.0*ZII', '1.0*ZZI', '1.0*ZZZ']","title":"Coefficients"},{"location":"reference/observable-notation/#pauli-operator-properties","text":"","title":"Pauli Operator Properties"},{"location":"reference/observable-notation/#eigenvalues","text":"All Pauli operators have eigenvalues \u00b11 : Operator Eigenstate Eigenvalue X |+\u27e9 = (|0\u27e9 + |1\u27e9)/\u221a2 +1 X |-\u27e9 = (|0\u27e9 - |1\u27e9)/\u221a2 -1 Y |i+\u27e9 = (|0\u27e9 + i|1\u27e9)/\u221a2 +1 Y |i-\u27e9 = (|0\u27e9 - i|1\u27e9)/\u221a2 -1 Z |0\u27e9 +1 Z |1\u27e9 -1","title":"Eigenvalues"},{"location":"reference/observable-notation/#commutation-relations","text":"Observables commute if they share qubits only on I or matching Pauli operators Observables anticommute if they differ on an odd number of qubits # Commuting observables (can measure simultaneously) Observable ( \"ZII\" ) and Observable ( \"ZZI\" ) # \u2713 commute Observable ( \"XII\" ) and Observable ( \"YZZ\" ) # \u2713 commute (different qubits) # Anticommuting observables (cannot measure simultaneously) Observable ( \"XII\" ) and Observable ( \"ZII\" ) # \u2717 anticommute (differ on qubit 0) Observable ( \"ZZ\" ) and Observable ( \"XZ\" ) # \u2717 anticommute (differ on qubit 0)","title":"Commutation Relations"},{"location":"reference/observable-notation/#expected-values-for-common-states","text":"","title":"Expected Values for Common States"},{"location":"reference/observable-notation/#computational-basis-states","text":"|00\u27e9 state: Observable ( \"ZI\" ): + 1.0 # Z\u2080 = +1 (qubit 0 is |0\u27e9) Observable ( \"IZ\" ): + 1.0 # Z\u2081 = +1 (qubit 1 is |0\u27e9) Observable ( \"ZZ\" ): + 1.0 # Z\u2080Z\u2081 = (+1)(+1) = +1 Observable ( \"XI\" ): 0.0 # X\u2080 = 0 (|0\u27e9 is equal superposition of X eigenstates) Observable ( \"XX\" ): 0.0 # X\u2080X\u2081 = 0 |11\u27e9 state: Observable ( \"ZI\" ): - 1.0 # Z\u2080 = -1 (qubit 0 is |1\u27e9) Observable ( \"IZ\" ): - 1.0 # Z\u2081 = -1 (qubit 1 is |1\u27e9) Observable ( \"ZZ\" ): + 1.0 # Z\u2080Z\u2081 = (-1)(-1) = +1 Observable ( \"XI\" ): 0.0 # X\u2080 = 0 Observable ( \"XX\" ): 0.0 # X\u2080X\u2081 = 0","title":"Computational Basis States"},{"location":"reference/observable-notation/#bell-states","text":"|\u03a6\u207a\u27e9 = (|00\u27e9 + |11\u27e9)/\u221a2 (Bell state): Observable ( \"ZI\" ): 0.0 # Equal |0\u27e9 and |1\u27e9 on qubit 0 Observable ( \"IZ\" ): 0.0 # Equal |0\u27e9 and |1\u27e9 on qubit 1 Observable ( \"ZZ\" ): + 1.0 # Correlated: both qubits have same parity Observable ( \"XX\" ): + 1.0 # Both qubits in X-basis |+\u27e9 Observable ( \"YY\" ): - 1.0 # Y correlation Observable ( \"XY\" ): 0.0 # No XY correlation |\u03a8\u207b\u27e9 = (|01\u27e9 - |10\u27e9)/\u221a2 (singlet state): Observable ( \"ZI\" ): 0.0 Observable ( \"IZ\" ): 0.0 Observable ( \"ZZ\" ): - 1.0 # Anti-correlated Observable ( \"XX\" ): - 1.0 Observable ( \"YY\" ): - 1.0 Observable ( \"XZ\" ): 0.0","title":"Bell States"},{"location":"reference/observable-notation/#ghz-states","text":"|GHZ(3)\u27e9 = (|000\u27e9 + |111\u27e9)/\u221a2: GHZ states are stabilized by even-parity Z operators: # Single-qubit Z observables \u2192 0 (equal superposition) Observable ( \"ZII\" ): 0.0 Observable ( \"IZI\" ): 0.0 Observable ( \"IIZ\" ): 0.0 # Two-qubit Z observables \u2192 +1 (even parity stabilizers) Observable ( \"ZZI\" ): + 1.0 Observable ( \"ZIZ\" ): + 1.0 Observable ( \"IZZ\" ): + 1.0 # Three-qubit Z observables \u2192 -1 (odd parity) Observable ( \"ZZZ\" ): - 1.0 # X observables Observable ( \"XXX\" ): + 1.0 # All qubits in |+\u27e9 superposition Observable ( \"XII\" ): 0.0 # Single X \u2192 0 Observable ( \"XXI\" ): 0.0 # Two X's \u2192 0 Rule for GHZ: - Even number of Z operators \u2192 +1 - Odd number of Z operators \u2192 0 (for single Z) or -1 (for all Z) |GHZ(4)\u27e9 = (|0000\u27e9 + |1111\u27e9)/\u221a2: Observable ( \"ZIII\" ): 0.0 Observable ( \"ZZII\" ): + 1.0 Observable ( \"ZZZI\" ): + 1.0 Observable ( \"ZZZZ\" ): + 1.0 # Even number (4) of Z's Observable ( \"XXXX\" ): + 1.0","title":"GHZ States"},{"location":"reference/observable-notation/#w-states","text":"|W(3)\u27e9 = (|001\u27e9 + |010\u27e9 + |100\u27e9)/\u221a3: Observable ( \"ZII\" ): - 1 / 3 # Two |0\u27e9 components, one |1\u27e9 Observable ( \"IZI\" ): - 1 / 3 Observable ( \"IIZ\" ): - 1 / 3 Observable ( \"ZZZ\" ): - 1 / 3 # Mix of even/odd parities Observable ( \"XXX\" ): 1 / 3 # Partial correlation","title":"W States"},{"location":"reference/observable-notation/#hamiltonian-observables","text":"Many quantum algorithms estimate Hamiltonian energy :","title":"Hamiltonian Observables"},{"location":"reference/observable-notation/#ising-model-1d-chain","text":"# H = \u03a3\u1d62 J\u1d62Z\u1d62Z\u1d62\u208a\u2081 + \u03a3\u1d62h\u1d62Z\u1d62 # For 3 qubits with J=-1.0, h=0.5: Observable ( \"ZZI\" , coefficient =- 1.0 ) # Z\u2080Z\u2081 interaction Observable ( \"IZZ\" , coefficient =- 1.0 ) # Z\u2081Z\u2082 interaction Observable ( \"ZII\" , coefficient = 0.5 ) # Z\u2080 field Observable ( \"IZI\" , coefficient = 0.5 ) # Z\u2081 field Observable ( \"IIZ\" , coefficient = 0.5 ) # Z\u2082 field # Total energy = sum of all observable expectation values","title":"Ising Model (1D chain)"},{"location":"reference/observable-notation/#molecular-hamiltonians-vqe","text":"# H\u2082 molecule (example coefficients) Observable ( \"II\" , coefficient =- 0.8105 ) # Identity term Observable ( \"ZI\" , coefficient = 0.1721 ) # Z\u2080 term Observable ( \"IZ\" , coefficient = 0.1721 ) # Z\u2081 term Observable ( \"ZZ\" , coefficient =- 0.2279 ) # Z\u2080Z\u2081 correlation Observable ( \"XX\" , coefficient = 0.1809 ) # XX exchange Observable ( \"YY\" , coefficient = 0.1809 ) # YY exchange","title":"Molecular Hamiltonians (VQE)"},{"location":"reference/observable-notation/#interpreting-results","text":"","title":"Interpreting Results"},{"location":"reference/observable-notation/#confidence-intervals","text":"QuartumSE reports 95% confidence intervals around expectation values: result . observables [ \"1.0*ZZZ\" ] # { # 'expectation_value': -0.9922, # 'variance': 0.0156, # 'ci_95': (-1.0353, -0.9491), # 'ci_width': 0.0862 # } Interpretation: - Expectation value : -0.9922 (close to theoretical -1.0 for GHZ) - Variance : 0.0156 (measurement uncertainty) - 95% CI : [-1.0353, -0.9491] (true value likely in this range) - CI width : 0.0862 (precision measure; smaller is better)","title":"Confidence Intervals"},{"location":"reference/observable-notation/#comparing-to-theory","text":"Example: GHZ(3) validation # Theoretical expectations theory = { \"1.0*ZII\" : 0.0 , \"1.0*ZZI\" : + 1.0 , \"1.0*ZZZ\" : - 1.0 , } # Experimental results for obs_str , data in result . observables . items (): estimated = data [ 'expectation_value' ] expected = theory [ obs_str ] ci = data [ 'ci_95' ] # Check if theory is within confidence interval in_ci = ci [ 0 ] <= expected <= ci [ 1 ] status = \"\u2713\" if in_ci else \"\u2717\" print ( f \" { obs_str : 10 } Est: { estimated : +.4f } \" f \"Theory: { expected : +.4f } { status } \" ) Output: 1.0*ZII Est: +0.0039 Theory: +0.0000 \u2713 1.0*ZZI Est: +0.9961 Theory: +1.0000 \u2713 1.0*ZZZ Est: -0.9922 Theory: -1.0000 \u2713","title":"Comparing to Theory"},{"location":"reference/observable-notation/#common-patterns","text":"","title":"Common Patterns"},{"location":"reference/observable-notation/#entanglement-witnesses","text":"CHSH Inequality: # S = |\u27e8XX\u27e9 + \u27e8XY\u27e9 + \u27e8YX\u27e9 - \u27e8YY\u27e9| # Classical limit: S \u2264 2 # Quantum (Bell state): S = 2\u221a2 \u2248 2.828 Observable ( \"XX\" ) # \u27e8XX\u27e9 Observable ( \"XY\" ) # \u27e8XY\u27e9 Observable ( \"YX\" ) # \u27e8YX\u27e9 Observable ( \"YY\" ) # \u27e8YY\u27e9","title":"Entanglement Witnesses"},{"location":"reference/observable-notation/#fidelity-estimation","text":"Overlap with target state |\u03c8\u27e9: # F = \u27e8\u03c8|\u03c1|\u03c8\u27e9 = \u03a3\u1d62 \u27e8P\u1d62\u27e9 / 2\u207f # where P\u1d62 are stabilizer observables # For GHZ(3): observables = [ Observable ( \"ZZI\" ), # Stabilizer 1 Observable ( \"IZZ\" ), # Stabilizer 2 ] # F \u2248 (1 + \u27e8ZZI\u27e9 + \u27e8IZZ\u27e9 + \u27e8ZZI\u27e9\u27e8IZZ\u27e9) / 4","title":"Fidelity Estimation"},{"location":"reference/observable-notation/#symmetry-testing","text":"Check parity symmetry: # Even parity: should be +1 or -1 Observable ( \"ZZZZ\" ) # If result \u2248 0, state lacks definite parity # (e.g., equal mixture of even/odd states)","title":"Symmetry Testing"},{"location":"reference/observable-notation/#further-reading","text":"Qiskit Observable Tutorial : qiskit.org/documentation/tutorials/operators Pauli Operator Algebra : See Nielsen & Chuang, Chapter 2 Classical Shadows Theory : Huang, Kueng, Preskill (2020) QuartumSE Architecture : docs/explanation/shadows-theory.md","title":"Further Reading"},{"location":"reference/observable-notation/#quick-reference","text":"Observable Name Measures I Identity Always +1 X Pauli-X X-basis projection Y Pauli-Y Y-basis projection Z Pauli-Z Computational basis projection ZZ Z-correlation Parity between two qubits XX X-correlation X-basis entanglement ZZZ 3-qubit parity Stabilizer for GHZ states Tip: When debugging, start with Z observables ( ZI , ZZ , etc.) since they measure in the computational basis and are easier to interpret from bitstring histograms.","title":"Quick Reference"},{"location":"reference/api/","text":"API Reference \u00b6 QuartumSE - Quantum Measurement Optimization & Observability Platform A vendor-neutral framework for running quantum experiments with: - Classical shadows for shot-efficient observable estimation - Rigorous error mitigation and confidence intervals - Full provenance tracking and reproducibility - Cross-platform backend support (IBM, AWS, and more) License: Apache 2.0 ClassicalShadows \u00b6 Bases: ABC Abstract base class for classical shadows implementations. Different versions (v0-v4) subclass this to provide specific algorithms. Source code in src/quartumse/shadows/core.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class ClassicalShadows ( ABC ): \"\"\" Abstract base class for classical shadows implementations. Different versions (v0-v4) subclass this to provide specific algorithms. \"\"\" def __init__ ( self , config : Any ): self . config = config self . shadow_data : np . ndarray | None = None self . measurement_bases : np . ndarray | None = None self . measurement_outcomes : np . ndarray | None = None @abstractmethod def generate_measurement_circuits ( self , base_circuit : QuantumCircuit , num_shadows : int ) -> list [ QuantumCircuit ]: \"\"\" Generate randomized measurement circuits for shadows protocol. Args: base_circuit: The state preparation circuit num_shadows: Number of random measurements Returns: List of circuits with randomized measurements appended \"\"\" pass @abstractmethod def reconstruct_classical_shadow ( self , measurement_outcomes : np . ndarray , measurement_bases : np . ndarray ) -> np . ndarray : \"\"\" Reconstruct classical shadow snapshots from measurement data. Args: measurement_outcomes: Binary outcomes (0/1) for each measurement measurement_bases: Which basis was measured for each qubit Returns: Array of shadow snapshots (density matrix representations) \"\"\" pass @abstractmethod def estimate_observable ( self , observable : Observable , shadow_data : np . ndarray | None = None ) -> ShadowEstimate : \"\"\" Estimate expectation value of an observable using shadow data. Args: observable: The observable to estimate shadow_data: Pre-computed shadow snapshots (or use self.shadow_data) Returns: Estimate with confidence interval \"\"\" pass @abstractmethod def estimate_shadow_size_needed ( self , observable : Observable , target_precision : float ) -> int : \"\"\"Estimate the number of shadows required for a desired precision.\"\"\" raise NotImplementedError def estimate_multiple_observables ( self , observables : list [ Observable ] ) -> dict [ str , ShadowEstimate ]: \"\"\" Estimate multiple observables from the same shadow data. This is the key advantage: one shadow dataset, many observables. \"\"\" if self . shadow_data is None : raise ValueError ( \"No shadow data available. Run generate_measurement_circuits first.\" ) results = {} for obs in observables : estimate = self . estimate_observable ( obs ) results [ str ( obs )] = estimate return results def compute_variance_bound ( self , observable : Observable , shadow_size : int ) -> float : \"\"\" Theoretical variance bound for the shadow estimator. Useful for shot allocation and adaptive strategies. \"\"\" # Default implementation (subclasses can override) # For random local Clifford: Var \u2264 4^k / M, where k = support size support_size = sum ( 1 for p in observable . pauli_string if p != \"I\" ) return float ( 4 ** support_size ) / float ( shadow_size ) def compute_confidence_interval ( self , mean : float , variance : float , n_samples : int , confidence : float = 0.95 ) -> tuple [ float , float ]: \"\"\"Compute confidence interval using normal approximation.\"\"\" from scipy import stats std_error = np . sqrt ( variance / n_samples ) z_score = float ( stats . norm . ppf (( 1 + confidence ) / 2 )) ci_lower = mean - z_score * std_error ci_upper = mean + z_score * std_error return ( ci_lower , ci_upper ) compute_confidence_interval ( mean , variance , n_samples , confidence = 0.95 ) \u00b6 Compute confidence interval using normal approximation. Source code in src/quartumse/shadows/core.py 167 168 169 170 171 172 173 174 175 176 177 178 179 def compute_confidence_interval ( self , mean : float , variance : float , n_samples : int , confidence : float = 0.95 ) -> tuple [ float , float ]: \"\"\"Compute confidence interval using normal approximation.\"\"\" from scipy import stats std_error = np . sqrt ( variance / n_samples ) z_score = float ( stats . norm . ppf (( 1 + confidence ) / 2 )) ci_lower = mean - z_score * std_error ci_upper = mean + z_score * std_error return ( ci_lower , ci_upper ) compute_variance_bound ( observable , shadow_size ) \u00b6 Theoretical variance bound for the shadow estimator. Useful for shot allocation and adaptive strategies. Source code in src/quartumse/shadows/core.py 156 157 158 159 160 161 162 163 164 165 def compute_variance_bound ( self , observable : Observable , shadow_size : int ) -> float : \"\"\" Theoretical variance bound for the shadow estimator. Useful for shot allocation and adaptive strategies. \"\"\" # Default implementation (subclasses can override) # For random local Clifford: Var \u2264 4^k / M, where k = support size support_size = sum ( 1 for p in observable . pauli_string if p != \"I\" ) return float ( 4 ** support_size ) / float ( shadow_size ) estimate_multiple_observables ( observables ) \u00b6 Estimate multiple observables from the same shadow data. This is the key advantage: one shadow dataset, many observables. Source code in src/quartumse/shadows/core.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def estimate_multiple_observables ( self , observables : list [ Observable ] ) -> dict [ str , ShadowEstimate ]: \"\"\" Estimate multiple observables from the same shadow data. This is the key advantage: one shadow dataset, many observables. \"\"\" if self . shadow_data is None : raise ValueError ( \"No shadow data available. Run generate_measurement_circuits first.\" ) results = {} for obs in observables : estimate = self . estimate_observable ( obs ) results [ str ( obs )] = estimate return results estimate_observable ( observable , shadow_data = None ) abstractmethod \u00b6 Estimate expectation value of an observable using shadow data. Parameters: observable ( Observable ) \u2013 The observable to estimate shadow_data ( ndarray | None , default: None ) \u2013 Pre-computed shadow snapshots (or use self.shadow_data) Returns: ShadowEstimate \u2013 Estimate with confidence interval Source code in src/quartumse/shadows/core.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @abstractmethod def estimate_observable ( self , observable : Observable , shadow_data : np . ndarray | None = None ) -> ShadowEstimate : \"\"\" Estimate expectation value of an observable using shadow data. Args: observable: The observable to estimate shadow_data: Pre-computed shadow snapshots (or use self.shadow_data) Returns: Estimate with confidence interval \"\"\" pass estimate_shadow_size_needed ( observable , target_precision ) abstractmethod \u00b6 Estimate the number of shadows required for a desired precision. Source code in src/quartumse/shadows/core.py 132 133 134 135 136 @abstractmethod def estimate_shadow_size_needed ( self , observable : Observable , target_precision : float ) -> int : \"\"\"Estimate the number of shadows required for a desired precision.\"\"\" raise NotImplementedError generate_measurement_circuits ( base_circuit , num_shadows ) abstractmethod \u00b6 Generate randomized measurement circuits for shadows protocol. Parameters: base_circuit ( QuantumCircuit ) \u2013 The state preparation circuit num_shadows ( int ) \u2013 Number of random measurements Returns: list [ QuantumCircuit ] \u2013 List of circuits with randomized measurements appended Source code in src/quartumse/shadows/core.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @abstractmethod def generate_measurement_circuits ( self , base_circuit : QuantumCircuit , num_shadows : int ) -> list [ QuantumCircuit ]: \"\"\" Generate randomized measurement circuits for shadows protocol. Args: base_circuit: The state preparation circuit num_shadows: Number of random measurements Returns: List of circuits with randomized measurements appended \"\"\" pass reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) abstractmethod \u00b6 Reconstruct classical shadow snapshots from measurement data. Parameters: measurement_outcomes ( ndarray ) \u2013 Binary outcomes (0/1) for each measurement measurement_bases ( ndarray ) \u2013 Which basis was measured for each qubit Returns: ndarray \u2013 Array of shadow snapshots (density matrix representations) Source code in src/quartumse/shadows/core.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @abstractmethod def reconstruct_classical_shadow ( self , measurement_outcomes : np . ndarray , measurement_bases : np . ndarray ) -> np . ndarray : \"\"\" Reconstruct classical shadow snapshots from measurement data. Args: measurement_outcomes: Binary outcomes (0/1) for each measurement measurement_bases: Which basis was measured for each qubit Returns: Array of shadow snapshots (density matrix representations) \"\"\" pass Estimator \u00b6 Bases: ABC Abstract base class for quantum observable estimators. Provides unified interface for different estimation strategies: - Classical shadows (various versions) - Direct measurement - Grouped Pauli measurement Source code in src/quartumse/estimator/base.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class Estimator ( ABC ): \"\"\" Abstract base class for quantum observable estimators. Provides unified interface for different estimation strategies: - Classical shadows (various versions) - Direct measurement - Grouped Pauli measurement \"\"\" def __init__ ( self , backend : Any , config : Any | None = None ) -> None : self . backend = backend self . config = config @abstractmethod def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , ) -> EstimationResult : \"\"\" Estimate expectation values of observables. Args: circuit: State preparation circuit observables: List of observables to estimate target_precision: Desired precision (optional) Returns: Estimation results with confidence intervals \"\"\" raise NotImplementedError @abstractmethod def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\" Estimate number of shots needed for target precision. Used for cost estimation and shot allocation. \"\"\" raise NotImplementedError estimate ( circuit , observables , target_precision = None ) abstractmethod \u00b6 Estimate expectation values of observables. Parameters: circuit ( QuantumCircuit ) \u2013 State preparation circuit observables ( list [ Observable ] ) \u2013 List of observables to estimate target_precision ( float | None , default: None ) \u2013 Desired precision (optional) Returns: EstimationResult \u2013 Estimation results with confidence intervals Source code in src/quartumse/estimator/base.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @abstractmethod def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , ) -> EstimationResult : \"\"\" Estimate expectation values of observables. Args: circuit: State preparation circuit observables: List of observables to estimate target_precision: Desired precision (optional) Returns: Estimation results with confidence intervals \"\"\" raise NotImplementedError estimate_shots_needed ( observables , target_precision ) abstractmethod \u00b6 Estimate number of shots needed for target precision. Used for cost estimation and shot allocation. Source code in src/quartumse/estimator/base.py 68 69 70 71 72 73 74 75 @abstractmethod def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\" Estimate number of shots needed for target precision. Used for cost estimation and shot allocation. \"\"\" raise NotImplementedError ProvenanceManifest \u00b6 High-level interface for creating and managing provenance manifests. Source code in src/quartumse/reporting/manifest.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class ProvenanceManifest : \"\"\" High-level interface for creating and managing provenance manifests. \"\"\" def __init__ ( self , schema : ManifestSchema ): self . schema = schema @classmethod def create ( cls , experiment_id : str , circuit_fingerprint : CircuitFingerprint , backend_snapshot : BackendSnapshot , ** kwargs : Any , ) -> \"ProvenanceManifest\" : \"\"\"Create a new manifest with required fields.\"\"\" schema = ManifestSchema ( experiment_id = experiment_id , circuit = circuit_fingerprint , backend = backend_snapshot , ** kwargs , ) return cls ( schema ) def to_json ( self , path : str | Path | None = None ) -> str : \"\"\"Export manifest as JSON.\"\"\" json_str = self . schema . model_dump_json ( indent = 2 ) if path : Path ( path ) . write_text ( json_str ) return json_str @classmethod def from_json ( cls , path : str | Path ) -> \"ProvenanceManifest\" : \"\"\"Load manifest from JSON file.\"\"\" json_data = Path ( path ) . read_text () schema = ManifestSchema . model_validate_json ( json_data ) return cls ( schema ) def add_tag ( self , tag : str ) -> None : \"\"\"Add a searchable tag.\"\"\" if tag not in self . schema . tags : self . schema . tags . append ( tag ) def update_results ( self , results : dict [ str , Any ]) -> None : \"\"\"Update the results summary.\"\"\" self . schema . results_summary . update ( results ) def validate ( self , * , require_shot_file : bool = True ) -> bool : \"\"\"Validate the manifest schema and ensure referenced artifacts exist.\"\"\" if require_shot_file : shot_path = Path ( self . schema . shot_data_path ) if not shot_path . exists (): raise FileNotFoundError ( f \"Shot data referenced by manifest is missing: { shot_path } \" ) return True def __repr__ ( self ) -> str : return ( f \"ProvenanceManifest(id= { self . schema . experiment_id } , \" f \"backend= { self . schema . backend . backend_name } , \" f \"created= { self . schema . created_at . isoformat () } )\" ) add_tag ( tag ) \u00b6 Add a searchable tag. Source code in src/quartumse/reporting/manifest.py 250 251 252 253 def add_tag ( self , tag : str ) -> None : \"\"\"Add a searchable tag.\"\"\" if tag not in self . schema . tags : self . schema . tags . append ( tag ) create ( experiment_id , circuit_fingerprint , backend_snapshot , ** kwargs ) classmethod \u00b6 Create a new manifest with required fields. Source code in src/quartumse/reporting/manifest.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 @classmethod def create ( cls , experiment_id : str , circuit_fingerprint : CircuitFingerprint , backend_snapshot : BackendSnapshot , ** kwargs : Any , ) -> \"ProvenanceManifest\" : \"\"\"Create a new manifest with required fields.\"\"\" schema = ManifestSchema ( experiment_id = experiment_id , circuit = circuit_fingerprint , backend = backend_snapshot , ** kwargs , ) return cls ( schema ) from_json ( path ) classmethod \u00b6 Load manifest from JSON file. Source code in src/quartumse/reporting/manifest.py 243 244 245 246 247 248 @classmethod def from_json ( cls , path : str | Path ) -> \"ProvenanceManifest\" : \"\"\"Load manifest from JSON file.\"\"\" json_data = Path ( path ) . read_text () schema = ManifestSchema . model_validate_json ( json_data ) return cls ( schema ) to_json ( path = None ) \u00b6 Export manifest as JSON. Source code in src/quartumse/reporting/manifest.py 234 235 236 237 238 239 240 241 def to_json ( self , path : str | Path | None = None ) -> str : \"\"\"Export manifest as JSON.\"\"\" json_str = self . schema . model_dump_json ( indent = 2 ) if path : Path ( path ) . write_text ( json_str ) return json_str update_results ( results ) \u00b6 Update the results summary. Source code in src/quartumse/reporting/manifest.py 255 256 257 def update_results ( self , results : dict [ str , Any ]) -> None : \"\"\"Update the results summary.\"\"\" self . schema . results_summary . update ( results ) validate ( * , require_shot_file = True ) \u00b6 Validate the manifest schema and ensure referenced artifacts exist. Source code in src/quartumse/reporting/manifest.py 259 260 261 262 263 264 265 266 267 def validate ( self , * , require_shot_file : bool = True ) -> bool : \"\"\"Validate the manifest schema and ensure referenced artifacts exist.\"\"\" if require_shot_file : shot_path = Path ( self . schema . shot_data_path ) if not shot_path . exists (): raise FileNotFoundError ( f \"Shot data referenced by manifest is missing: { shot_path } \" ) return True Report \u00b6 Container for experiment report data. Source code in src/quartumse/reporting/report.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 class Report : \"\"\"Container for experiment report data.\"\"\" def __init__ ( self , manifest : ProvenanceManifest , plots : dict [ str , Any ] | None = None , shot_diagnostics : ShotDataDiagnostics | None = None , ): self . manifest = manifest self . plots = plots or {} self . shot_diagnostics = shot_diagnostics def to_html ( self , output_path : str | Path | None = None ) -> str : \"\"\"Generate HTML report.\"\"\" template = Template ( HTML_TEMPLATE ) metrics_context = normalise_metrics_for_report ( self . manifest . schema . results_summary . get ( \"metrics\" ) if isinstance ( self . manifest . schema . results_summary , dict ) else None ) html = template . render ( manifest = self . manifest . schema , now = datetime . now ( timezone . utc ) . isoformat (), shot_diagnostics = self . shot_diagnostics . to_dict () if self . shot_diagnostics else None , metrics = metrics_context , ) if output_path : Path ( output_path ) . write_text ( html , encoding = \"utf-8\" ) return html def to_pdf ( self , output_path : str | Path ) -> None : \"\"\"Generate PDF report (requires weasyprint).\"\"\" try : from weasyprint import HTML html_content = self . to_html () HTML ( string = html_content ) . write_pdf ( output_path ) except ImportError as err : raise ImportError ( \"PDF generation requires weasyprint. Install with: pip install weasyprint\" ) from err to_html ( output_path = None ) \u00b6 Generate HTML report. Source code in src/quartumse/reporting/report.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def to_html ( self , output_path : str | Path | None = None ) -> str : \"\"\"Generate HTML report.\"\"\" template = Template ( HTML_TEMPLATE ) metrics_context = normalise_metrics_for_report ( self . manifest . schema . results_summary . get ( \"metrics\" ) if isinstance ( self . manifest . schema . results_summary , dict ) else None ) html = template . render ( manifest = self . manifest . schema , now = datetime . now ( timezone . utc ) . isoformat (), shot_diagnostics = self . shot_diagnostics . to_dict () if self . shot_diagnostics else None , metrics = metrics_context , ) if output_path : Path ( output_path ) . write_text ( html , encoding = \"utf-8\" ) return html to_pdf ( output_path ) \u00b6 Generate PDF report (requires weasyprint). Source code in src/quartumse/reporting/report.py 392 393 394 395 396 397 398 399 400 401 402 def to_pdf ( self , output_path : str | Path ) -> None : \"\"\"Generate PDF report (requires weasyprint).\"\"\" try : from weasyprint import HTML html_content = self . to_html () HTML ( string = html_content ) . write_pdf ( output_path ) except ImportError as err : raise ImportError ( \"PDF generation requires weasyprint. Install with: pip install weasyprint\" ) from err ShadowConfig \u00b6 Bases: BaseModel Configuration for classical shadows estimation. Source code in src/quartumse/shadows/config.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class ShadowConfig ( BaseModel ): \"\"\"Configuration for classical shadows estimation.\"\"\" # Core parameters version : ShadowVersion = Field ( default = ShadowVersion . V0_BASELINE , description = \"Shadows algorithm version\" ) shadow_size : int = Field ( default = 1000 , description = \"Number of random measurements (shadow size)\" ) measurement_ensemble : MeasurementEnsemble = Field ( default = MeasurementEnsemble . RANDOM_LOCAL_CLIFFORD ) # v1+ (noise-aware) apply_inverse_channel : bool = Field ( default = False , description = \"Apply noise-aware inverse channel (v1+)\" ) noise_model_path : str | None = Field ( None , description = \"Path to serialized noise model\" ) # v2+ (fermionic) fermionic_mode : bool = Field ( default = False , description = \"Enable fermionic shadows (v2+)\" ) rdm_order : int = Field ( default = 1 , description = \"RDM order for fermionic mode (1 or 2)\" ) # v3+ (adaptive) adaptive : bool = Field ( default = False , description = \"Use adaptive measurement selection (v3+)\" ) target_observables : list [ str ] | None = Field ( None , description = \"Observable strings for adaptive prioritization\" ) derandomization_strategy : str | None = Field ( None , description = \"greedy, importance_sampling, etc.\" ) # v4+ (robust) bayesian_inference : bool = Field ( default = False , description = \"Enable Bayesian robust estimation (v4+)\" ) bootstrap_samples : int = Field ( default = 1000 , description = \"Bootstrap samples for CI (v4+)\" ) confidence_level : float = Field ( default = 0.95 , description = \"Confidence interval level\" ) # General settings random_seed : int | None = Field ( None , description = \"Random seed for reproducibility\" ) parallel_shots : bool = Field ( default = True , description = \"Execute shadow measurements in parallel batches\" ) batch_size : int | None = Field ( None , description = \"Batch size for parallel execution\" ) # Variance reduction median_of_means : bool = Field ( default = False , description = \"Use median-of-means estimator for robustness\" ) num_groups : int = Field ( default = 10 , description = \"Number of groups for median-of-means\" ) # Advanced custom_parameters : dict [ str , Any ] = Field ( default_factory = dict , description = \"Version-specific custom parameters\" ) model_config = ConfigDict ( use_enum_values = False ) def validate_version_compatibility ( self ) -> None : \"\"\"Validate that enabled features match the selected version.\"\"\" # Warning: simplified validation # In production, this would check feature availability pass validate_version_compatibility () \u00b6 Validate that enabled features match the selected version. Source code in src/quartumse/shadows/config.py 88 89 90 91 92 93 def validate_version_compatibility ( self ) -> None : \"\"\"Validate that enabled features match the selected version.\"\"\" # Warning: simplified validation # In production, this would check feature availability pass ShadowEstimator \u00b6 Bases: Estimator Observable estimator using classical shadows. Automatically selects shadow version based on config and orchestrates: 1. Shadow measurement generation 2. Circuit execution 3. Shadow reconstruction 4. Observable estimation 5. Provenance tracking Source code in src/quartumse/estimator/shadow_estimator.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 class ShadowEstimator ( Estimator ): \"\"\" Observable estimator using classical shadows. Automatically selects shadow version based on config and orchestrates: 1. Shadow measurement generation 2. Circuit execution 3. Shadow reconstruction 4. Observable estimation 5. Provenance tracking \"\"\" def __init__ ( self , backend : Backend | str , shadow_config : ShadowConfig | None = None , mitigation_config : MitigationConfig | None = None , data_dir : str | Path | None = None , ): \"\"\" Initialize shadow estimator. Args: backend: Qiskit backend or backend name (e.g., \"aer_simulator\") shadow_config: Classical shadows configuration mitigation_config: Error mitigation configuration data_dir: Directory for storing shot data and manifests \"\"\" # Handle backend self . _backend_descriptor : str | None = None self . _backend_snapshot : BackendSnapshot | None = None if isinstance ( backend , str ): self . _backend_descriptor = backend if \":\" in backend : resolved_backend , snapshot = resolve_backend ( backend ) backend = resolved_backend self . _backend_snapshot = snapshot elif backend == \"aer_simulator\" : backend = AerSimulator () self . _backend_snapshot = create_backend_snapshot ( backend ) else : raise ValueError ( f \"Unknown backend string: { backend } \" ) else : self . _backend_descriptor = getattr ( backend , \"name\" , None ) super () . __init__ ( backend , shadow_config ) self . _runtime_sampler : SamplerPrimitive | None = None self . _runtime_sampler_checked = False self . _use_runtime_sampler = is_ibm_runtime_backend ( self . backend ) self . shadow_config = shadow_config or ShadowConfig . model_validate ({}) self . mitigation_config = mitigation_config or MitigationConfig () self . data_dir = Path ( data_dir ) if data_dir else Path ( \"./data\" ) self . data_dir . mkdir ( parents = True , exist_ok = True ) self . measurement_error_mitigation : MeasurementErrorMitigation | None = None self . _mem_required = ( self . shadow_config . version == ShadowVersion . V1_NOISE_AWARE or self . shadow_config . apply_inverse_channel or ( \"MEM\" in self . mitigation_config . techniques ) ) if self . _mem_required : self . measurement_error_mitigation = MeasurementErrorMitigation ( self . backend ) # Initialize shadow implementation based on version self . shadow_impl : ClassicalShadows = self . _create_shadow_implementation () # Initialize shot data writer self . shot_data_writer = ShotDataWriter ( self . data_dir ) def _get_runtime_sampler ( self ) -> SamplerPrimitive | None : \"\"\"Initialise (if necessary) and return the IBM Runtime sampler.\"\"\" if not self . _use_runtime_sampler : return None if not self . _runtime_sampler_checked : self . _runtime_sampler = create_runtime_sampler ( self . backend ) self . _runtime_sampler_checked = True return self . _runtime_sampler def _create_shadow_implementation ( self ) -> ClassicalShadows : \"\"\"Factory for shadow implementations.\"\"\" version = self . shadow_config . version if version == ShadowVersion . V0_BASELINE : return RandomLocalCliffordShadows ( self . shadow_config ) elif version == ShadowVersion . V1_NOISE_AWARE : if self . measurement_error_mitigation is None : self . measurement_error_mitigation = MeasurementErrorMitigation ( self . backend ) return NoiseAwareRandomLocalCliffordShadows ( self . shadow_config , self . measurement_error_mitigation ) elif version == ShadowVersion . V2_FERMIONIC : # TODO: Implement v2 raise NotImplementedError ( \"Shadows v2 (fermionic) not yet implemented\" ) elif version == ShadowVersion . V3_ADAPTIVE : # TODO: Implement v3 raise NotImplementedError ( \"Shadows v3 (adaptive) not yet implemented\" ) elif version == ShadowVersion . V4_ROBUST : # TODO: Implement v4 raise NotImplementedError ( \"Shadows v4 (robust) not yet implemented\" ) else : raise ValueError ( f \"Unknown shadow version: { version } \" ) def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , save_manifest : bool = True , ) -> EstimationResult : \"\"\" Estimate observables using classical shadows. Workflow: 1. Generate shadow measurement circuits 2. Transpile and execute on backend 3. Reconstruct shadow snapshots 4. Estimate all observables 5. Generate provenance manifest \"\"\" experiment_id = str ( uuid . uuid4 ()) start_time = time . time () # Determine shadow size if target_precision : required_sizes = [ self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) for obs in observables ] shadow_size = max ( required_sizes ) if required_sizes else self . shadow_config . shadow_size if shadow_size <= 0 : raise ValueError ( \"Shadow size estimation produced a non-positive value\" ) self . shadow_config . shadow_size = shadow_size self . shadow_impl . config . shadow_size = shadow_size else : shadow_size = self . shadow_config . shadow_size self . shadow_impl . config . shadow_size = shadow_size # Generate shadow measurement circuits shadow_circuits = self . shadow_impl . generate_measurement_circuits ( circuit , shadow_size ) # Calibrate measurement error mitigation if required if isinstance ( self . shadow_impl , NoiseAwareRandomLocalCliffordShadows ): mem_params = self . mitigation_config . parameters mem_shots = int ( mem_params . get ( \"mem_shots\" , 4096 )) mem_qubits_param = mem_params . get ( \"mem_qubits\" ) if mem_qubits_param is None : mem_qubits = list ( range ( circuit . num_qubits )) elif isinstance ( mem_qubits_param , ( list , tuple )): mem_qubits = [ int ( q ) for q in mem_qubits_param ] else : mem_qubits = [ int ( mem_qubits_param )] mem_force = bool ( mem_params . get ( \"mem_force_calibration\" , False )) run_options = mem_params . get ( \"mem_run_options\" , {}) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path if mem_confusion_path_str and not mem_force : try : self . shadow_impl . mem . load_confusion_matrix ( mem_confusion_path_str ) metadata = self . shadow_impl . mem . get_confusion_metadata () if isinstance ( metadata . get ( \"shots_per_state\" ), ( int , float )): mem_shots = int ( metadata [ \"shots_per_state\" ]) mem_params [ \"mem_shots\" ] = mem_shots if isinstance ( metadata . get ( \"qubits\" ), ( list , tuple )): mem_qubits = [ int ( q ) for q in metadata [ \"qubits\" ]] mem_params [ \"mem_qubits\" ] = mem_qubits except FileNotFoundError : LOGGER . warning ( \"Configured confusion matrix %s not found; recalibrating.\" , mem_confusion_path_str , ) mem_confusion_path_str = None if ( self . shadow_impl . mem . confusion_matrix is None or mem_force or not mem_confusion_path_str ): mem_dir = self . data_dir / \"mem\" mem_dir . mkdir ( parents = True , exist_ok = True ) confusion_matrix_path = mem_dir / f \" { experiment_id } .npz\" saved_confusion_path = self . shadow_impl . mem . calibrate ( mem_qubits , shots = mem_shots , run_options = run_options , output_path = confusion_matrix_path , ) mem_confusion_path = ( saved_confusion_path if saved_confusion_path is not None else confusion_matrix_path ) self . mitigation_config . confusion_matrix_path = str ( mem_confusion_path . resolve ()) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path self . shadow_impl . mem . confusion_matrix_path = Path ( mem_confusion_path_str ) else : self . mitigation_config . confusion_matrix_path = mem_confusion_path_str if \"MEM\" not in self . mitigation_config . techniques : self . mitigation_config . techniques . append ( \"MEM\" ) mem_params [ \"mem_qubits\" ] = mem_qubits mem_params [ \"mem_shots\" ] = mem_shots # Transpile for backend transpiled_circuits = transpile ( shadow_circuits , backend = self . backend ) # Respect backend batching limits max_experiments = None backend_config = None if hasattr ( self . backend , \"configuration\" ): try : backend_config = self . backend . configuration () except Exception : backend_config = None if backend_config is not None : max_experiments = getattr ( backend_config , \"max_experiments\" , None ) if isinstance ( max_experiments , np . integer ): max_experiments = int ( max_experiments ) if not isinstance ( max_experiments , int ) or max_experiments <= 0 : # Use safe default batch size for IBM backends to avoid submission failures max_experiments = 500 print ( f \"Warning: Backend max_experiments unavailable or invalid. \" f \"Using safe default batch size: { max_experiments } \" ) measurement_outcomes_list : list [ np . ndarray ] = [] sampler = self . _get_runtime_sampler () for start_idx in range ( 0 , len ( transpiled_circuits ), max_experiments ): circuit_batch = transpiled_circuits [ start_idx : start_idx + max_experiments ] if sampler is not None : job = sampler . run ( list ( circuit_batch ), shots = 1 ) result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result [ batch_idx ] . data . meas . get_counts () bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) else : job = self . backend . run ( circuit_batch , shots = 1 ) # Each circuit is one shadow result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result . get_counts ( batch_idx ) bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) if len ( measurement_outcomes_list ) != shadow_size : raise RuntimeError ( \"Collected measurement outcomes do not match the requested shadow size.\" ) measurement_outcomes = np . asarray ( measurement_outcomes_list , dtype = int ) measurement_bases = self . shadow_impl . measurement_bases if measurement_bases is None : raise ValueError ( \"Shadow implementation did not record measurement bases.\" ) measurement_bases = np . asarray ( measurement_bases , dtype = int ) self . shadow_impl . measurement_bases = measurement_bases # Save shot data to Parquet shot_data_path = self . shot_data_writer . save_shadow_measurements ( experiment_id = experiment_id , measurement_bases = measurement_bases , measurement_outcomes = measurement_outcomes , num_qubits = circuit . num_qubits , ) # Reconstruct shadows self . shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = self . shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } execution_time = time . time () - start_time # Create provenance manifest if save_manifest : manifest = self . _create_manifest ( experiment_id , circuit , observables , estimates , shadow_size , execution_time , shot_data_path , ) manifest_path = self . data_dir / \"manifests\" / f \" { experiment_id } .json\" manifest_path . parent . mkdir ( parents = True , exist_ok = True ) manifest . to_json ( manifest_path ) else : manifest_path = None return EstimationResult ( observables = estimates , shots_used = shadow_size , execution_time = execution_time , backend_name = self . backend . name , experiment_id = experiment_id , manifest_path = str ( manifest_path ) if manifest_path else None , shot_data_path = str ( shot_data_path . resolve ()), mitigation_confusion_matrix_path = self . mitigation_config . confusion_matrix_path , ) def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\"Estimate shadow size needed for target precision.\"\"\" # Use worst-case observable max_shadow_size = 0 for obs in observables : size = self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) max_shadow_size = max ( max_shadow_size , size ) return max_shadow_size def replay_from_manifest ( self , manifest_path : str | Path , observables : list [ Observable ] | None = None , ) -> EstimationResult : \"\"\" Replay an experiment from a saved manifest and shot data. This allows re-estimation of observables from previously collected shot data without re-executing circuits on the backend. Args: manifest_path: Path to the provenance manifest JSON file observables: Optional new list of observables to estimate. If None, uses observables from the original manifest. Returns: EstimationResult with re-estimated observables \"\"\" manifest_path = Path ( manifest_path ) if not manifest_path . exists (): raise FileNotFoundError ( f \"Manifest not found: { manifest_path } \" ) # Load manifest manifest = ProvenanceManifest . from_json ( manifest_path ) experiment_id = manifest . schema . experiment_id # Load shot data measurement_bases , measurement_outcomes , num_qubits = ( self . shot_data_writer . load_shadow_measurements ( experiment_id ) ) if manifest . schema . shadows is None : raise ValueError ( \"Manifest does not contain classical shadows configuration information.\" ) # Reconstruct shadows with loaded data # Create temporary shadow implementation if needed shadow_payload = manifest . schema . shadows . model_dump () shadow_payload [ \"random_seed\" ] = manifest . schema . random_seed shadow_config = ShadowConfig . model_validate ( shadow_payload ) resolved_confusion_matrix_path : str | None = ( manifest . schema . mitigation . confusion_matrix_path ) if shadow_config . version == ShadowVersion . V0_BASELINE : shadow_impl = RandomLocalCliffordShadows ( shadow_config ) elif shadow_config . version == ShadowVersion . V1_NOISE_AWARE : confusion_matrix_path_str = manifest . schema . mitigation . confusion_matrix_path if not confusion_matrix_path_str : raise FileNotFoundError ( \"Noise-aware manifest does not include a persisted confusion matrix path. \" \"Re-run estimation or provide the saved calibration artifact before replaying.\" ) raw_confusion_path = Path ( confusion_matrix_path_str ) candidate_paths = [ raw_confusion_path ] if not raw_confusion_path . is_absolute (): candidate_paths . append (( manifest_path . parent / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / \"mem\" / raw_confusion_path . name ) . resolve ()) candidate_paths . append ( ( manifest_path . parent / \"mem\" / raw_confusion_path . name ) . resolve () ) confusion_matrix_path : Path | None = None for candidate in candidate_paths : if candidate and candidate . exists (): confusion_matrix_path = candidate break if confusion_matrix_path is None : raise FileNotFoundError ( \"Unable to locate the persisted confusion matrix required for noise-aware replay. \" f \"Looked for { raw_confusion_path } and related paths.\" ) with np . load ( confusion_matrix_path , allow_pickle = False ) as archive : if \"confusion_matrix\" not in archive : raise ValueError ( \"Confusion matrix archive is missing the 'confusion_matrix' dataset.\" ) confusion_matrix = archive [ \"confusion_matrix\" ] mem = MeasurementErrorMitigation ( self . backend ) mem . confusion_matrix = confusion_matrix mem . confusion_matrix_path = confusion_matrix_path . resolve () mem . _calibrated_qubits = tuple ( range ( num_qubits )) shadow_impl = NoiseAwareRandomLocalCliffordShadows ( shadow_config , mem ) resolved_confusion_matrix_path = str ( confusion_matrix_path . resolve ()) else : raise NotImplementedError ( f \"Replay for shadow version { shadow_config . version . value } is not implemented\" ) shadow_impl . measurement_bases = measurement_bases shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Use observables from manifest if not provided if observables is None : observables = [ Observable ( obs_dict [ \"pauli\" ], obs_dict . get ( \"coefficient\" , 1.0 )) for obs_dict in manifest . schema . observables ] # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } return EstimationResult ( observables = estimates , shots_used = manifest . schema . shadows . shadow_size , execution_time = 0.0 , # No execution time for replay backend_name = manifest . schema . backend . backend_name , experiment_id = experiment_id , manifest_path = str ( manifest_path ), shot_data_path = manifest . schema . shot_data_path , mitigation_confusion_matrix_path = resolved_confusion_matrix_path , ) def _create_manifest ( self , experiment_id : str , circuit : QuantumCircuit , observables : list [ Observable ], estimates : dict [ str , dict [ str , object ]], shadow_size : int , execution_time : float , shot_data_path : Path , ) -> ProvenanceManifest : \"\"\"Create provenance manifest for the experiment.\"\"\" import sys import qiskit # Circuit fingerprint try : qasm_str = qasm3 . dumps ( circuit ) except Exception : qasm_str = circuit . qasm () gate_counts : dict [ str , int ] = {} for instruction in circuit . data : gate_name = instruction . operation . name gate_counts [ gate_name ] = gate_counts . get ( gate_name , 0 ) + 1 circuit_hash = hashlib . sha256 ( qasm_str . encode ()) . hexdigest ()[: 16 ] circuit_fp = CircuitFingerprint ( qasm3 = qasm_str , num_qubits = circuit . num_qubits , depth = circuit . depth (), gate_counts = gate_counts , circuit_hash = circuit_hash , ) # Backend snapshot backend_snapshot = self . _backend_snapshot or create_backend_snapshot ( self . backend ) # Shadows config shadows_config = ShadowsConfig . model_validate ( { \"version\" : self . shadow_config . version . value , \"shadow_size\" : shadow_size , \"measurement_ensemble\" : self . shadow_config . measurement_ensemble . value , \"noise_model_path\" : self . shadow_config . noise_model_path , \"inverse_channel_applied\" : self . shadow_config . apply_inverse_channel , \"fermionic_mode\" : self . shadow_config . fermionic_mode , \"rdm_order\" : self . shadow_config . rdm_order , \"adaptive\" : self . shadow_config . adaptive , \"target_observables\" : self . shadow_config . target_observables , \"bayesian_inference\" : self . shadow_config . bayesian_inference , \"bootstrap_samples\" : self . shadow_config . bootstrap_samples , } ) # Resource usage resource_usage = ResourceUsage . model_validate ( { \"total_shots\" : shadow_size , \"execution_time_seconds\" : execution_time , \"queue_time_seconds\" : None , \"estimated_cost_usd\" : None , \"credits_used\" : None , \"classical_compute_seconds\" : None , } ) metadata = {} if self . _backend_descriptor : metadata [ \"backend_descriptor\" ] = self . _backend_descriptor # Create manifest shot_checksum = compute_file_checksum ( shot_data_path ) mitigation_config = self . mitigation_config . model_copy ( deep = True ) confusion_path = mitigation_config . confusion_matrix_path if confusion_path : mitigation_config . confusion_matrix_checksum = compute_file_checksum ( confusion_path ) manifest_schema = ManifestSchema ( experiment_id = experiment_id , experiment_name = None , circuit = circuit_fp , observables = [ { \"pauli\" : obs . pauli_string , \"coefficient\" : obs . coefficient } for obs in observables ], backend = backend_snapshot , mitigation = mitigation_config , shadows = shadows_config , shot_data_path = str ( shot_data_path . resolve ()), shot_data_checksum = shot_checksum , results_summary = estimates , resource_usage = resource_usage , metadata = metadata , random_seed = self . shadow_config . random_seed , quartumse_version = __version__ , qiskit_version = qiskit . __version__ , python_version = f \" { sys . version_info . major } . { sys . version_info . minor } . { sys . version_info . micro } \" , ) return ProvenanceManifest ( manifest_schema ) __init__ ( backend , shadow_config = None , mitigation_config = None , data_dir = None ) \u00b6 Initialize shadow estimator. Parameters: backend ( Backend | str ) \u2013 Qiskit backend or backend name (e.g., \"aer_simulator\") shadow_config ( ShadowConfig | None , default: None ) \u2013 Classical shadows configuration mitigation_config ( MitigationConfig | None , default: None ) \u2013 Error mitigation configuration data_dir ( str | Path | None , default: None ) \u2013 Directory for storing shot data and manifests Source code in src/quartumse/estimator/shadow_estimator.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , backend : Backend | str , shadow_config : ShadowConfig | None = None , mitigation_config : MitigationConfig | None = None , data_dir : str | Path | None = None , ): \"\"\" Initialize shadow estimator. Args: backend: Qiskit backend or backend name (e.g., \"aer_simulator\") shadow_config: Classical shadows configuration mitigation_config: Error mitigation configuration data_dir: Directory for storing shot data and manifests \"\"\" # Handle backend self . _backend_descriptor : str | None = None self . _backend_snapshot : BackendSnapshot | None = None if isinstance ( backend , str ): self . _backend_descriptor = backend if \":\" in backend : resolved_backend , snapshot = resolve_backend ( backend ) backend = resolved_backend self . _backend_snapshot = snapshot elif backend == \"aer_simulator\" : backend = AerSimulator () self . _backend_snapshot = create_backend_snapshot ( backend ) else : raise ValueError ( f \"Unknown backend string: { backend } \" ) else : self . _backend_descriptor = getattr ( backend , \"name\" , None ) super () . __init__ ( backend , shadow_config ) self . _runtime_sampler : SamplerPrimitive | None = None self . _runtime_sampler_checked = False self . _use_runtime_sampler = is_ibm_runtime_backend ( self . backend ) self . shadow_config = shadow_config or ShadowConfig . model_validate ({}) self . mitigation_config = mitigation_config or MitigationConfig () self . data_dir = Path ( data_dir ) if data_dir else Path ( \"./data\" ) self . data_dir . mkdir ( parents = True , exist_ok = True ) self . measurement_error_mitigation : MeasurementErrorMitigation | None = None self . _mem_required = ( self . shadow_config . version == ShadowVersion . V1_NOISE_AWARE or self . shadow_config . apply_inverse_channel or ( \"MEM\" in self . mitigation_config . techniques ) ) if self . _mem_required : self . measurement_error_mitigation = MeasurementErrorMitigation ( self . backend ) # Initialize shadow implementation based on version self . shadow_impl : ClassicalShadows = self . _create_shadow_implementation () # Initialize shot data writer self . shot_data_writer = ShotDataWriter ( self . data_dir ) estimate ( circuit , observables , target_precision = None , save_manifest = True ) \u00b6 Estimate observables using classical shadows. Workflow: 1. Generate shadow measurement circuits 2. Transpile and execute on backend 3. Reconstruct shadow snapshots 4. Estimate all observables 5. Generate provenance manifest Source code in src/quartumse/estimator/shadow_estimator.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , save_manifest : bool = True , ) -> EstimationResult : \"\"\" Estimate observables using classical shadows. Workflow: 1. Generate shadow measurement circuits 2. Transpile and execute on backend 3. Reconstruct shadow snapshots 4. Estimate all observables 5. Generate provenance manifest \"\"\" experiment_id = str ( uuid . uuid4 ()) start_time = time . time () # Determine shadow size if target_precision : required_sizes = [ self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) for obs in observables ] shadow_size = max ( required_sizes ) if required_sizes else self . shadow_config . shadow_size if shadow_size <= 0 : raise ValueError ( \"Shadow size estimation produced a non-positive value\" ) self . shadow_config . shadow_size = shadow_size self . shadow_impl . config . shadow_size = shadow_size else : shadow_size = self . shadow_config . shadow_size self . shadow_impl . config . shadow_size = shadow_size # Generate shadow measurement circuits shadow_circuits = self . shadow_impl . generate_measurement_circuits ( circuit , shadow_size ) # Calibrate measurement error mitigation if required if isinstance ( self . shadow_impl , NoiseAwareRandomLocalCliffordShadows ): mem_params = self . mitigation_config . parameters mem_shots = int ( mem_params . get ( \"mem_shots\" , 4096 )) mem_qubits_param = mem_params . get ( \"mem_qubits\" ) if mem_qubits_param is None : mem_qubits = list ( range ( circuit . num_qubits )) elif isinstance ( mem_qubits_param , ( list , tuple )): mem_qubits = [ int ( q ) for q in mem_qubits_param ] else : mem_qubits = [ int ( mem_qubits_param )] mem_force = bool ( mem_params . get ( \"mem_force_calibration\" , False )) run_options = mem_params . get ( \"mem_run_options\" , {}) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path if mem_confusion_path_str and not mem_force : try : self . shadow_impl . mem . load_confusion_matrix ( mem_confusion_path_str ) metadata = self . shadow_impl . mem . get_confusion_metadata () if isinstance ( metadata . get ( \"shots_per_state\" ), ( int , float )): mem_shots = int ( metadata [ \"shots_per_state\" ]) mem_params [ \"mem_shots\" ] = mem_shots if isinstance ( metadata . get ( \"qubits\" ), ( list , tuple )): mem_qubits = [ int ( q ) for q in metadata [ \"qubits\" ]] mem_params [ \"mem_qubits\" ] = mem_qubits except FileNotFoundError : LOGGER . warning ( \"Configured confusion matrix %s not found; recalibrating.\" , mem_confusion_path_str , ) mem_confusion_path_str = None if ( self . shadow_impl . mem . confusion_matrix is None or mem_force or not mem_confusion_path_str ): mem_dir = self . data_dir / \"mem\" mem_dir . mkdir ( parents = True , exist_ok = True ) confusion_matrix_path = mem_dir / f \" { experiment_id } .npz\" saved_confusion_path = self . shadow_impl . mem . calibrate ( mem_qubits , shots = mem_shots , run_options = run_options , output_path = confusion_matrix_path , ) mem_confusion_path = ( saved_confusion_path if saved_confusion_path is not None else confusion_matrix_path ) self . mitigation_config . confusion_matrix_path = str ( mem_confusion_path . resolve ()) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path self . shadow_impl . mem . confusion_matrix_path = Path ( mem_confusion_path_str ) else : self . mitigation_config . confusion_matrix_path = mem_confusion_path_str if \"MEM\" not in self . mitigation_config . techniques : self . mitigation_config . techniques . append ( \"MEM\" ) mem_params [ \"mem_qubits\" ] = mem_qubits mem_params [ \"mem_shots\" ] = mem_shots # Transpile for backend transpiled_circuits = transpile ( shadow_circuits , backend = self . backend ) # Respect backend batching limits max_experiments = None backend_config = None if hasattr ( self . backend , \"configuration\" ): try : backend_config = self . backend . configuration () except Exception : backend_config = None if backend_config is not None : max_experiments = getattr ( backend_config , \"max_experiments\" , None ) if isinstance ( max_experiments , np . integer ): max_experiments = int ( max_experiments ) if not isinstance ( max_experiments , int ) or max_experiments <= 0 : # Use safe default batch size for IBM backends to avoid submission failures max_experiments = 500 print ( f \"Warning: Backend max_experiments unavailable or invalid. \" f \"Using safe default batch size: { max_experiments } \" ) measurement_outcomes_list : list [ np . ndarray ] = [] sampler = self . _get_runtime_sampler () for start_idx in range ( 0 , len ( transpiled_circuits ), max_experiments ): circuit_batch = transpiled_circuits [ start_idx : start_idx + max_experiments ] if sampler is not None : job = sampler . run ( list ( circuit_batch ), shots = 1 ) result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result [ batch_idx ] . data . meas . get_counts () bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) else : job = self . backend . run ( circuit_batch , shots = 1 ) # Each circuit is one shadow result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result . get_counts ( batch_idx ) bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) if len ( measurement_outcomes_list ) != shadow_size : raise RuntimeError ( \"Collected measurement outcomes do not match the requested shadow size.\" ) measurement_outcomes = np . asarray ( measurement_outcomes_list , dtype = int ) measurement_bases = self . shadow_impl . measurement_bases if measurement_bases is None : raise ValueError ( \"Shadow implementation did not record measurement bases.\" ) measurement_bases = np . asarray ( measurement_bases , dtype = int ) self . shadow_impl . measurement_bases = measurement_bases # Save shot data to Parquet shot_data_path = self . shot_data_writer . save_shadow_measurements ( experiment_id = experiment_id , measurement_bases = measurement_bases , measurement_outcomes = measurement_outcomes , num_qubits = circuit . num_qubits , ) # Reconstruct shadows self . shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = self . shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } execution_time = time . time () - start_time # Create provenance manifest if save_manifest : manifest = self . _create_manifest ( experiment_id , circuit , observables , estimates , shadow_size , execution_time , shot_data_path , ) manifest_path = self . data_dir / \"manifests\" / f \" { experiment_id } .json\" manifest_path . parent . mkdir ( parents = True , exist_ok = True ) manifest . to_json ( manifest_path ) else : manifest_path = None return EstimationResult ( observables = estimates , shots_used = shadow_size , execution_time = execution_time , backend_name = self . backend . name , experiment_id = experiment_id , manifest_path = str ( manifest_path ) if manifest_path else None , shot_data_path = str ( shot_data_path . resolve ()), mitigation_confusion_matrix_path = self . mitigation_config . confusion_matrix_path , ) estimate_shots_needed ( observables , target_precision ) \u00b6 Estimate shadow size needed for target precision. Source code in src/quartumse/estimator/shadow_estimator.py 368 369 370 371 372 373 374 375 376 def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\"Estimate shadow size needed for target precision.\"\"\" # Use worst-case observable max_shadow_size = 0 for obs in observables : size = self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) max_shadow_size = max ( max_shadow_size , size ) return max_shadow_size replay_from_manifest ( manifest_path , observables = None ) \u00b6 Replay an experiment from a saved manifest and shot data. This allows re-estimation of observables from previously collected shot data without re-executing circuits on the backend. Parameters: manifest_path ( str | Path ) \u2013 Path to the provenance manifest JSON file observables ( list [ Observable ] | None , default: None ) \u2013 Optional new list of observables to estimate. If None, uses observables from the original manifest. Returns: EstimationResult \u2013 EstimationResult with re-estimated observables Source code in src/quartumse/estimator/shadow_estimator.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def replay_from_manifest ( self , manifest_path : str | Path , observables : list [ Observable ] | None = None , ) -> EstimationResult : \"\"\" Replay an experiment from a saved manifest and shot data. This allows re-estimation of observables from previously collected shot data without re-executing circuits on the backend. Args: manifest_path: Path to the provenance manifest JSON file observables: Optional new list of observables to estimate. If None, uses observables from the original manifest. Returns: EstimationResult with re-estimated observables \"\"\" manifest_path = Path ( manifest_path ) if not manifest_path . exists (): raise FileNotFoundError ( f \"Manifest not found: { manifest_path } \" ) # Load manifest manifest = ProvenanceManifest . from_json ( manifest_path ) experiment_id = manifest . schema . experiment_id # Load shot data measurement_bases , measurement_outcomes , num_qubits = ( self . shot_data_writer . load_shadow_measurements ( experiment_id ) ) if manifest . schema . shadows is None : raise ValueError ( \"Manifest does not contain classical shadows configuration information.\" ) # Reconstruct shadows with loaded data # Create temporary shadow implementation if needed shadow_payload = manifest . schema . shadows . model_dump () shadow_payload [ \"random_seed\" ] = manifest . schema . random_seed shadow_config = ShadowConfig . model_validate ( shadow_payload ) resolved_confusion_matrix_path : str | None = ( manifest . schema . mitigation . confusion_matrix_path ) if shadow_config . version == ShadowVersion . V0_BASELINE : shadow_impl = RandomLocalCliffordShadows ( shadow_config ) elif shadow_config . version == ShadowVersion . V1_NOISE_AWARE : confusion_matrix_path_str = manifest . schema . mitigation . confusion_matrix_path if not confusion_matrix_path_str : raise FileNotFoundError ( \"Noise-aware manifest does not include a persisted confusion matrix path. \" \"Re-run estimation or provide the saved calibration artifact before replaying.\" ) raw_confusion_path = Path ( confusion_matrix_path_str ) candidate_paths = [ raw_confusion_path ] if not raw_confusion_path . is_absolute (): candidate_paths . append (( manifest_path . parent / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / \"mem\" / raw_confusion_path . name ) . resolve ()) candidate_paths . append ( ( manifest_path . parent / \"mem\" / raw_confusion_path . name ) . resolve () ) confusion_matrix_path : Path | None = None for candidate in candidate_paths : if candidate and candidate . exists (): confusion_matrix_path = candidate break if confusion_matrix_path is None : raise FileNotFoundError ( \"Unable to locate the persisted confusion matrix required for noise-aware replay. \" f \"Looked for { raw_confusion_path } and related paths.\" ) with np . load ( confusion_matrix_path , allow_pickle = False ) as archive : if \"confusion_matrix\" not in archive : raise ValueError ( \"Confusion matrix archive is missing the 'confusion_matrix' dataset.\" ) confusion_matrix = archive [ \"confusion_matrix\" ] mem = MeasurementErrorMitigation ( self . backend ) mem . confusion_matrix = confusion_matrix mem . confusion_matrix_path = confusion_matrix_path . resolve () mem . _calibrated_qubits = tuple ( range ( num_qubits )) shadow_impl = NoiseAwareRandomLocalCliffordShadows ( shadow_config , mem ) resolved_confusion_matrix_path = str ( confusion_matrix_path . resolve ()) else : raise NotImplementedError ( f \"Replay for shadow version { shadow_config . version . value } is not implemented\" ) shadow_impl . measurement_bases = measurement_bases shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Use observables from manifest if not provided if observables is None : observables = [ Observable ( obs_dict [ \"pauli\" ], obs_dict . get ( \"coefficient\" , 1.0 )) for obs_dict in manifest . schema . observables ] # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } return EstimationResult ( observables = estimates , shots_used = manifest . schema . shadows . shadow_size , execution_time = 0.0 , # No execution time for replay backend_name = manifest . schema . backend . backend_name , experiment_id = experiment_id , manifest_path = str ( manifest_path ), shot_data_path = manifest . schema . shot_data_path , mitigation_confusion_matrix_path = resolved_confusion_matrix_path , ) Commands \u00b6 See the CLI reference for command-line usage details.","title":"API"},{"location":"reference/api/#api-reference","text":"QuartumSE - Quantum Measurement Optimization & Observability Platform A vendor-neutral framework for running quantum experiments with: - Classical shadows for shot-efficient observable estimation - Rigorous error mitigation and confidence intervals - Full provenance tracking and reproducibility - Cross-platform backend support (IBM, AWS, and more) License: Apache 2.0","title":"API Reference"},{"location":"reference/api/#quartumse.ClassicalShadows","text":"Bases: ABC Abstract base class for classical shadows implementations. Different versions (v0-v4) subclass this to provide specific algorithms. Source code in src/quartumse/shadows/core.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 class ClassicalShadows ( ABC ): \"\"\" Abstract base class for classical shadows implementations. Different versions (v0-v4) subclass this to provide specific algorithms. \"\"\" def __init__ ( self , config : Any ): self . config = config self . shadow_data : np . ndarray | None = None self . measurement_bases : np . ndarray | None = None self . measurement_outcomes : np . ndarray | None = None @abstractmethod def generate_measurement_circuits ( self , base_circuit : QuantumCircuit , num_shadows : int ) -> list [ QuantumCircuit ]: \"\"\" Generate randomized measurement circuits for shadows protocol. Args: base_circuit: The state preparation circuit num_shadows: Number of random measurements Returns: List of circuits with randomized measurements appended \"\"\" pass @abstractmethod def reconstruct_classical_shadow ( self , measurement_outcomes : np . ndarray , measurement_bases : np . ndarray ) -> np . ndarray : \"\"\" Reconstruct classical shadow snapshots from measurement data. Args: measurement_outcomes: Binary outcomes (0/1) for each measurement measurement_bases: Which basis was measured for each qubit Returns: Array of shadow snapshots (density matrix representations) \"\"\" pass @abstractmethod def estimate_observable ( self , observable : Observable , shadow_data : np . ndarray | None = None ) -> ShadowEstimate : \"\"\" Estimate expectation value of an observable using shadow data. Args: observable: The observable to estimate shadow_data: Pre-computed shadow snapshots (or use self.shadow_data) Returns: Estimate with confidence interval \"\"\" pass @abstractmethod def estimate_shadow_size_needed ( self , observable : Observable , target_precision : float ) -> int : \"\"\"Estimate the number of shadows required for a desired precision.\"\"\" raise NotImplementedError def estimate_multiple_observables ( self , observables : list [ Observable ] ) -> dict [ str , ShadowEstimate ]: \"\"\" Estimate multiple observables from the same shadow data. This is the key advantage: one shadow dataset, many observables. \"\"\" if self . shadow_data is None : raise ValueError ( \"No shadow data available. Run generate_measurement_circuits first.\" ) results = {} for obs in observables : estimate = self . estimate_observable ( obs ) results [ str ( obs )] = estimate return results def compute_variance_bound ( self , observable : Observable , shadow_size : int ) -> float : \"\"\" Theoretical variance bound for the shadow estimator. Useful for shot allocation and adaptive strategies. \"\"\" # Default implementation (subclasses can override) # For random local Clifford: Var \u2264 4^k / M, where k = support size support_size = sum ( 1 for p in observable . pauli_string if p != \"I\" ) return float ( 4 ** support_size ) / float ( shadow_size ) def compute_confidence_interval ( self , mean : float , variance : float , n_samples : int , confidence : float = 0.95 ) -> tuple [ float , float ]: \"\"\"Compute confidence interval using normal approximation.\"\"\" from scipy import stats std_error = np . sqrt ( variance / n_samples ) z_score = float ( stats . norm . ppf (( 1 + confidence ) / 2 )) ci_lower = mean - z_score * std_error ci_upper = mean + z_score * std_error return ( ci_lower , ci_upper )","title":"ClassicalShadows"},{"location":"reference/api/#quartumse.ClassicalShadows.compute_confidence_interval","text":"Compute confidence interval using normal approximation. Source code in src/quartumse/shadows/core.py 167 168 169 170 171 172 173 174 175 176 177 178 179 def compute_confidence_interval ( self , mean : float , variance : float , n_samples : int , confidence : float = 0.95 ) -> tuple [ float , float ]: \"\"\"Compute confidence interval using normal approximation.\"\"\" from scipy import stats std_error = np . sqrt ( variance / n_samples ) z_score = float ( stats . norm . ppf (( 1 + confidence ) / 2 )) ci_lower = mean - z_score * std_error ci_upper = mean + z_score * std_error return ( ci_lower , ci_upper )","title":"compute_confidence_interval"},{"location":"reference/api/#quartumse.ClassicalShadows.compute_variance_bound","text":"Theoretical variance bound for the shadow estimator. Useful for shot allocation and adaptive strategies. Source code in src/quartumse/shadows/core.py 156 157 158 159 160 161 162 163 164 165 def compute_variance_bound ( self , observable : Observable , shadow_size : int ) -> float : \"\"\" Theoretical variance bound for the shadow estimator. Useful for shot allocation and adaptive strategies. \"\"\" # Default implementation (subclasses can override) # For random local Clifford: Var \u2264 4^k / M, where k = support size support_size = sum ( 1 for p in observable . pauli_string if p != \"I\" ) return float ( 4 ** support_size ) / float ( shadow_size )","title":"compute_variance_bound"},{"location":"reference/api/#quartumse.ClassicalShadows.estimate_multiple_observables","text":"Estimate multiple observables from the same shadow data. This is the key advantage: one shadow dataset, many observables. Source code in src/quartumse/shadows/core.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def estimate_multiple_observables ( self , observables : list [ Observable ] ) -> dict [ str , ShadowEstimate ]: \"\"\" Estimate multiple observables from the same shadow data. This is the key advantage: one shadow dataset, many observables. \"\"\" if self . shadow_data is None : raise ValueError ( \"No shadow data available. Run generate_measurement_circuits first.\" ) results = {} for obs in observables : estimate = self . estimate_observable ( obs ) results [ str ( obs )] = estimate return results","title":"estimate_multiple_observables"},{"location":"reference/api/#quartumse.ClassicalShadows.estimate_observable","text":"Estimate expectation value of an observable using shadow data. Parameters: observable ( Observable ) \u2013 The observable to estimate shadow_data ( ndarray | None , default: None ) \u2013 Pre-computed shadow snapshots (or use self.shadow_data) Returns: ShadowEstimate \u2013 Estimate with confidence interval Source code in src/quartumse/shadows/core.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @abstractmethod def estimate_observable ( self , observable : Observable , shadow_data : np . ndarray | None = None ) -> ShadowEstimate : \"\"\" Estimate expectation value of an observable using shadow data. Args: observable: The observable to estimate shadow_data: Pre-computed shadow snapshots (or use self.shadow_data) Returns: Estimate with confidence interval \"\"\" pass","title":"estimate_observable"},{"location":"reference/api/#quartumse.ClassicalShadows.estimate_shadow_size_needed","text":"Estimate the number of shadows required for a desired precision. Source code in src/quartumse/shadows/core.py 132 133 134 135 136 @abstractmethod def estimate_shadow_size_needed ( self , observable : Observable , target_precision : float ) -> int : \"\"\"Estimate the number of shadows required for a desired precision.\"\"\" raise NotImplementedError","title":"estimate_shadow_size_needed"},{"location":"reference/api/#quartumse.ClassicalShadows.generate_measurement_circuits","text":"Generate randomized measurement circuits for shadows protocol. Parameters: base_circuit ( QuantumCircuit ) \u2013 The state preparation circuit num_shadows ( int ) \u2013 Number of random measurements Returns: list [ QuantumCircuit ] \u2013 List of circuits with randomized measurements appended Source code in src/quartumse/shadows/core.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 @abstractmethod def generate_measurement_circuits ( self , base_circuit : QuantumCircuit , num_shadows : int ) -> list [ QuantumCircuit ]: \"\"\" Generate randomized measurement circuits for shadows protocol. Args: base_circuit: The state preparation circuit num_shadows: Number of random measurements Returns: List of circuits with randomized measurements appended \"\"\" pass","title":"generate_measurement_circuits"},{"location":"reference/api/#quartumse.ClassicalShadows.reconstruct_classical_shadow","text":"Reconstruct classical shadow snapshots from measurement data. Parameters: measurement_outcomes ( ndarray ) \u2013 Binary outcomes (0/1) for each measurement measurement_bases ( ndarray ) \u2013 Which basis was measured for each qubit Returns: ndarray \u2013 Array of shadow snapshots (density matrix representations) Source code in src/quartumse/shadows/core.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 @abstractmethod def reconstruct_classical_shadow ( self , measurement_outcomes : np . ndarray , measurement_bases : np . ndarray ) -> np . ndarray : \"\"\" Reconstruct classical shadow snapshots from measurement data. Args: measurement_outcomes: Binary outcomes (0/1) for each measurement measurement_bases: Which basis was measured for each qubit Returns: Array of shadow snapshots (density matrix representations) \"\"\" pass","title":"reconstruct_classical_shadow"},{"location":"reference/api/#quartumse.Estimator","text":"Bases: ABC Abstract base class for quantum observable estimators. Provides unified interface for different estimation strategies: - Classical shadows (various versions) - Direct measurement - Grouped Pauli measurement Source code in src/quartumse/estimator/base.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class Estimator ( ABC ): \"\"\" Abstract base class for quantum observable estimators. Provides unified interface for different estimation strategies: - Classical shadows (various versions) - Direct measurement - Grouped Pauli measurement \"\"\" def __init__ ( self , backend : Any , config : Any | None = None ) -> None : self . backend = backend self . config = config @abstractmethod def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , ) -> EstimationResult : \"\"\" Estimate expectation values of observables. Args: circuit: State preparation circuit observables: List of observables to estimate target_precision: Desired precision (optional) Returns: Estimation results with confidence intervals \"\"\" raise NotImplementedError @abstractmethod def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\" Estimate number of shots needed for target precision. Used for cost estimation and shot allocation. \"\"\" raise NotImplementedError","title":"Estimator"},{"location":"reference/api/#quartumse.Estimator.estimate","text":"Estimate expectation values of observables. Parameters: circuit ( QuantumCircuit ) \u2013 State preparation circuit observables ( list [ Observable ] ) \u2013 List of observables to estimate target_precision ( float | None , default: None ) \u2013 Desired precision (optional) Returns: EstimationResult \u2013 Estimation results with confidence intervals Source code in src/quartumse/estimator/base.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 @abstractmethod def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , ) -> EstimationResult : \"\"\" Estimate expectation values of observables. Args: circuit: State preparation circuit observables: List of observables to estimate target_precision: Desired precision (optional) Returns: Estimation results with confidence intervals \"\"\" raise NotImplementedError","title":"estimate"},{"location":"reference/api/#quartumse.Estimator.estimate_shots_needed","text":"Estimate number of shots needed for target precision. Used for cost estimation and shot allocation. Source code in src/quartumse/estimator/base.py 68 69 70 71 72 73 74 75 @abstractmethod def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\" Estimate number of shots needed for target precision. Used for cost estimation and shot allocation. \"\"\" raise NotImplementedError","title":"estimate_shots_needed"},{"location":"reference/api/#quartumse.ProvenanceManifest","text":"High-level interface for creating and managing provenance manifests. Source code in src/quartumse/reporting/manifest.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 class ProvenanceManifest : \"\"\" High-level interface for creating and managing provenance manifests. \"\"\" def __init__ ( self , schema : ManifestSchema ): self . schema = schema @classmethod def create ( cls , experiment_id : str , circuit_fingerprint : CircuitFingerprint , backend_snapshot : BackendSnapshot , ** kwargs : Any , ) -> \"ProvenanceManifest\" : \"\"\"Create a new manifest with required fields.\"\"\" schema = ManifestSchema ( experiment_id = experiment_id , circuit = circuit_fingerprint , backend = backend_snapshot , ** kwargs , ) return cls ( schema ) def to_json ( self , path : str | Path | None = None ) -> str : \"\"\"Export manifest as JSON.\"\"\" json_str = self . schema . model_dump_json ( indent = 2 ) if path : Path ( path ) . write_text ( json_str ) return json_str @classmethod def from_json ( cls , path : str | Path ) -> \"ProvenanceManifest\" : \"\"\"Load manifest from JSON file.\"\"\" json_data = Path ( path ) . read_text () schema = ManifestSchema . model_validate_json ( json_data ) return cls ( schema ) def add_tag ( self , tag : str ) -> None : \"\"\"Add a searchable tag.\"\"\" if tag not in self . schema . tags : self . schema . tags . append ( tag ) def update_results ( self , results : dict [ str , Any ]) -> None : \"\"\"Update the results summary.\"\"\" self . schema . results_summary . update ( results ) def validate ( self , * , require_shot_file : bool = True ) -> bool : \"\"\"Validate the manifest schema and ensure referenced artifacts exist.\"\"\" if require_shot_file : shot_path = Path ( self . schema . shot_data_path ) if not shot_path . exists (): raise FileNotFoundError ( f \"Shot data referenced by manifest is missing: { shot_path } \" ) return True def __repr__ ( self ) -> str : return ( f \"ProvenanceManifest(id= { self . schema . experiment_id } , \" f \"backend= { self . schema . backend . backend_name } , \" f \"created= { self . schema . created_at . isoformat () } )\" )","title":"ProvenanceManifest"},{"location":"reference/api/#quartumse.ProvenanceManifest.add_tag","text":"Add a searchable tag. Source code in src/quartumse/reporting/manifest.py 250 251 252 253 def add_tag ( self , tag : str ) -> None : \"\"\"Add a searchable tag.\"\"\" if tag not in self . schema . tags : self . schema . tags . append ( tag )","title":"add_tag"},{"location":"reference/api/#quartumse.ProvenanceManifest.create","text":"Create a new manifest with required fields. Source code in src/quartumse/reporting/manifest.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 @classmethod def create ( cls , experiment_id : str , circuit_fingerprint : CircuitFingerprint , backend_snapshot : BackendSnapshot , ** kwargs : Any , ) -> \"ProvenanceManifest\" : \"\"\"Create a new manifest with required fields.\"\"\" schema = ManifestSchema ( experiment_id = experiment_id , circuit = circuit_fingerprint , backend = backend_snapshot , ** kwargs , ) return cls ( schema )","title":"create"},{"location":"reference/api/#quartumse.ProvenanceManifest.from_json","text":"Load manifest from JSON file. Source code in src/quartumse/reporting/manifest.py 243 244 245 246 247 248 @classmethod def from_json ( cls , path : str | Path ) -> \"ProvenanceManifest\" : \"\"\"Load manifest from JSON file.\"\"\" json_data = Path ( path ) . read_text () schema = ManifestSchema . model_validate_json ( json_data ) return cls ( schema )","title":"from_json"},{"location":"reference/api/#quartumse.ProvenanceManifest.to_json","text":"Export manifest as JSON. Source code in src/quartumse/reporting/manifest.py 234 235 236 237 238 239 240 241 def to_json ( self , path : str | Path | None = None ) -> str : \"\"\"Export manifest as JSON.\"\"\" json_str = self . schema . model_dump_json ( indent = 2 ) if path : Path ( path ) . write_text ( json_str ) return json_str","title":"to_json"},{"location":"reference/api/#quartumse.ProvenanceManifest.update_results","text":"Update the results summary. Source code in src/quartumse/reporting/manifest.py 255 256 257 def update_results ( self , results : dict [ str , Any ]) -> None : \"\"\"Update the results summary.\"\"\" self . schema . results_summary . update ( results )","title":"update_results"},{"location":"reference/api/#quartumse.ProvenanceManifest.validate","text":"Validate the manifest schema and ensure referenced artifacts exist. Source code in src/quartumse/reporting/manifest.py 259 260 261 262 263 264 265 266 267 def validate ( self , * , require_shot_file : bool = True ) -> bool : \"\"\"Validate the manifest schema and ensure referenced artifacts exist.\"\"\" if require_shot_file : shot_path = Path ( self . schema . shot_data_path ) if not shot_path . exists (): raise FileNotFoundError ( f \"Shot data referenced by manifest is missing: { shot_path } \" ) return True","title":"validate"},{"location":"reference/api/#quartumse.Report","text":"Container for experiment report data. Source code in src/quartumse/reporting/report.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 class Report : \"\"\"Container for experiment report data.\"\"\" def __init__ ( self , manifest : ProvenanceManifest , plots : dict [ str , Any ] | None = None , shot_diagnostics : ShotDataDiagnostics | None = None , ): self . manifest = manifest self . plots = plots or {} self . shot_diagnostics = shot_diagnostics def to_html ( self , output_path : str | Path | None = None ) -> str : \"\"\"Generate HTML report.\"\"\" template = Template ( HTML_TEMPLATE ) metrics_context = normalise_metrics_for_report ( self . manifest . schema . results_summary . get ( \"metrics\" ) if isinstance ( self . manifest . schema . results_summary , dict ) else None ) html = template . render ( manifest = self . manifest . schema , now = datetime . now ( timezone . utc ) . isoformat (), shot_diagnostics = self . shot_diagnostics . to_dict () if self . shot_diagnostics else None , metrics = metrics_context , ) if output_path : Path ( output_path ) . write_text ( html , encoding = \"utf-8\" ) return html def to_pdf ( self , output_path : str | Path ) -> None : \"\"\"Generate PDF report (requires weasyprint).\"\"\" try : from weasyprint import HTML html_content = self . to_html () HTML ( string = html_content ) . write_pdf ( output_path ) except ImportError as err : raise ImportError ( \"PDF generation requires weasyprint. Install with: pip install weasyprint\" ) from err","title":"Report"},{"location":"reference/api/#quartumse.Report.to_html","text":"Generate HTML report. Source code in src/quartumse/reporting/report.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def to_html ( self , output_path : str | Path | None = None ) -> str : \"\"\"Generate HTML report.\"\"\" template = Template ( HTML_TEMPLATE ) metrics_context = normalise_metrics_for_report ( self . manifest . schema . results_summary . get ( \"metrics\" ) if isinstance ( self . manifest . schema . results_summary , dict ) else None ) html = template . render ( manifest = self . manifest . schema , now = datetime . now ( timezone . utc ) . isoformat (), shot_diagnostics = self . shot_diagnostics . to_dict () if self . shot_diagnostics else None , metrics = metrics_context , ) if output_path : Path ( output_path ) . write_text ( html , encoding = \"utf-8\" ) return html","title":"to_html"},{"location":"reference/api/#quartumse.Report.to_pdf","text":"Generate PDF report (requires weasyprint). Source code in src/quartumse/reporting/report.py 392 393 394 395 396 397 398 399 400 401 402 def to_pdf ( self , output_path : str | Path ) -> None : \"\"\"Generate PDF report (requires weasyprint).\"\"\" try : from weasyprint import HTML html_content = self . to_html () HTML ( string = html_content ) . write_pdf ( output_path ) except ImportError as err : raise ImportError ( \"PDF generation requires weasyprint. Install with: pip install weasyprint\" ) from err","title":"to_pdf"},{"location":"reference/api/#quartumse.ShadowConfig","text":"Bases: BaseModel Configuration for classical shadows estimation. Source code in src/quartumse/shadows/config.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 class ShadowConfig ( BaseModel ): \"\"\"Configuration for classical shadows estimation.\"\"\" # Core parameters version : ShadowVersion = Field ( default = ShadowVersion . V0_BASELINE , description = \"Shadows algorithm version\" ) shadow_size : int = Field ( default = 1000 , description = \"Number of random measurements (shadow size)\" ) measurement_ensemble : MeasurementEnsemble = Field ( default = MeasurementEnsemble . RANDOM_LOCAL_CLIFFORD ) # v1+ (noise-aware) apply_inverse_channel : bool = Field ( default = False , description = \"Apply noise-aware inverse channel (v1+)\" ) noise_model_path : str | None = Field ( None , description = \"Path to serialized noise model\" ) # v2+ (fermionic) fermionic_mode : bool = Field ( default = False , description = \"Enable fermionic shadows (v2+)\" ) rdm_order : int = Field ( default = 1 , description = \"RDM order for fermionic mode (1 or 2)\" ) # v3+ (adaptive) adaptive : bool = Field ( default = False , description = \"Use adaptive measurement selection (v3+)\" ) target_observables : list [ str ] | None = Field ( None , description = \"Observable strings for adaptive prioritization\" ) derandomization_strategy : str | None = Field ( None , description = \"greedy, importance_sampling, etc.\" ) # v4+ (robust) bayesian_inference : bool = Field ( default = False , description = \"Enable Bayesian robust estimation (v4+)\" ) bootstrap_samples : int = Field ( default = 1000 , description = \"Bootstrap samples for CI (v4+)\" ) confidence_level : float = Field ( default = 0.95 , description = \"Confidence interval level\" ) # General settings random_seed : int | None = Field ( None , description = \"Random seed for reproducibility\" ) parallel_shots : bool = Field ( default = True , description = \"Execute shadow measurements in parallel batches\" ) batch_size : int | None = Field ( None , description = \"Batch size for parallel execution\" ) # Variance reduction median_of_means : bool = Field ( default = False , description = \"Use median-of-means estimator for robustness\" ) num_groups : int = Field ( default = 10 , description = \"Number of groups for median-of-means\" ) # Advanced custom_parameters : dict [ str , Any ] = Field ( default_factory = dict , description = \"Version-specific custom parameters\" ) model_config = ConfigDict ( use_enum_values = False ) def validate_version_compatibility ( self ) -> None : \"\"\"Validate that enabled features match the selected version.\"\"\" # Warning: simplified validation # In production, this would check feature availability pass","title":"ShadowConfig"},{"location":"reference/api/#quartumse.ShadowConfig.validate_version_compatibility","text":"Validate that enabled features match the selected version. Source code in src/quartumse/shadows/config.py 88 89 90 91 92 93 def validate_version_compatibility ( self ) -> None : \"\"\"Validate that enabled features match the selected version.\"\"\" # Warning: simplified validation # In production, this would check feature availability pass","title":"validate_version_compatibility"},{"location":"reference/api/#quartumse.ShadowEstimator","text":"Bases: Estimator Observable estimator using classical shadows. Automatically selects shadow version based on config and orchestrates: 1. Shadow measurement generation 2. Circuit execution 3. Shadow reconstruction 4. Observable estimation 5. Provenance tracking Source code in src/quartumse/estimator/shadow_estimator.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 class ShadowEstimator ( Estimator ): \"\"\" Observable estimator using classical shadows. Automatically selects shadow version based on config and orchestrates: 1. Shadow measurement generation 2. Circuit execution 3. Shadow reconstruction 4. Observable estimation 5. Provenance tracking \"\"\" def __init__ ( self , backend : Backend | str , shadow_config : ShadowConfig | None = None , mitigation_config : MitigationConfig | None = None , data_dir : str | Path | None = None , ): \"\"\" Initialize shadow estimator. Args: backend: Qiskit backend or backend name (e.g., \"aer_simulator\") shadow_config: Classical shadows configuration mitigation_config: Error mitigation configuration data_dir: Directory for storing shot data and manifests \"\"\" # Handle backend self . _backend_descriptor : str | None = None self . _backend_snapshot : BackendSnapshot | None = None if isinstance ( backend , str ): self . _backend_descriptor = backend if \":\" in backend : resolved_backend , snapshot = resolve_backend ( backend ) backend = resolved_backend self . _backend_snapshot = snapshot elif backend == \"aer_simulator\" : backend = AerSimulator () self . _backend_snapshot = create_backend_snapshot ( backend ) else : raise ValueError ( f \"Unknown backend string: { backend } \" ) else : self . _backend_descriptor = getattr ( backend , \"name\" , None ) super () . __init__ ( backend , shadow_config ) self . _runtime_sampler : SamplerPrimitive | None = None self . _runtime_sampler_checked = False self . _use_runtime_sampler = is_ibm_runtime_backend ( self . backend ) self . shadow_config = shadow_config or ShadowConfig . model_validate ({}) self . mitigation_config = mitigation_config or MitigationConfig () self . data_dir = Path ( data_dir ) if data_dir else Path ( \"./data\" ) self . data_dir . mkdir ( parents = True , exist_ok = True ) self . measurement_error_mitigation : MeasurementErrorMitigation | None = None self . _mem_required = ( self . shadow_config . version == ShadowVersion . V1_NOISE_AWARE or self . shadow_config . apply_inverse_channel or ( \"MEM\" in self . mitigation_config . techniques ) ) if self . _mem_required : self . measurement_error_mitigation = MeasurementErrorMitigation ( self . backend ) # Initialize shadow implementation based on version self . shadow_impl : ClassicalShadows = self . _create_shadow_implementation () # Initialize shot data writer self . shot_data_writer = ShotDataWriter ( self . data_dir ) def _get_runtime_sampler ( self ) -> SamplerPrimitive | None : \"\"\"Initialise (if necessary) and return the IBM Runtime sampler.\"\"\" if not self . _use_runtime_sampler : return None if not self . _runtime_sampler_checked : self . _runtime_sampler = create_runtime_sampler ( self . backend ) self . _runtime_sampler_checked = True return self . _runtime_sampler def _create_shadow_implementation ( self ) -> ClassicalShadows : \"\"\"Factory for shadow implementations.\"\"\" version = self . shadow_config . version if version == ShadowVersion . V0_BASELINE : return RandomLocalCliffordShadows ( self . shadow_config ) elif version == ShadowVersion . V1_NOISE_AWARE : if self . measurement_error_mitigation is None : self . measurement_error_mitigation = MeasurementErrorMitigation ( self . backend ) return NoiseAwareRandomLocalCliffordShadows ( self . shadow_config , self . measurement_error_mitigation ) elif version == ShadowVersion . V2_FERMIONIC : # TODO: Implement v2 raise NotImplementedError ( \"Shadows v2 (fermionic) not yet implemented\" ) elif version == ShadowVersion . V3_ADAPTIVE : # TODO: Implement v3 raise NotImplementedError ( \"Shadows v3 (adaptive) not yet implemented\" ) elif version == ShadowVersion . V4_ROBUST : # TODO: Implement v4 raise NotImplementedError ( \"Shadows v4 (robust) not yet implemented\" ) else : raise ValueError ( f \"Unknown shadow version: { version } \" ) def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , save_manifest : bool = True , ) -> EstimationResult : \"\"\" Estimate observables using classical shadows. Workflow: 1. Generate shadow measurement circuits 2. Transpile and execute on backend 3. Reconstruct shadow snapshots 4. Estimate all observables 5. Generate provenance manifest \"\"\" experiment_id = str ( uuid . uuid4 ()) start_time = time . time () # Determine shadow size if target_precision : required_sizes = [ self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) for obs in observables ] shadow_size = max ( required_sizes ) if required_sizes else self . shadow_config . shadow_size if shadow_size <= 0 : raise ValueError ( \"Shadow size estimation produced a non-positive value\" ) self . shadow_config . shadow_size = shadow_size self . shadow_impl . config . shadow_size = shadow_size else : shadow_size = self . shadow_config . shadow_size self . shadow_impl . config . shadow_size = shadow_size # Generate shadow measurement circuits shadow_circuits = self . shadow_impl . generate_measurement_circuits ( circuit , shadow_size ) # Calibrate measurement error mitigation if required if isinstance ( self . shadow_impl , NoiseAwareRandomLocalCliffordShadows ): mem_params = self . mitigation_config . parameters mem_shots = int ( mem_params . get ( \"mem_shots\" , 4096 )) mem_qubits_param = mem_params . get ( \"mem_qubits\" ) if mem_qubits_param is None : mem_qubits = list ( range ( circuit . num_qubits )) elif isinstance ( mem_qubits_param , ( list , tuple )): mem_qubits = [ int ( q ) for q in mem_qubits_param ] else : mem_qubits = [ int ( mem_qubits_param )] mem_force = bool ( mem_params . get ( \"mem_force_calibration\" , False )) run_options = mem_params . get ( \"mem_run_options\" , {}) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path if mem_confusion_path_str and not mem_force : try : self . shadow_impl . mem . load_confusion_matrix ( mem_confusion_path_str ) metadata = self . shadow_impl . mem . get_confusion_metadata () if isinstance ( metadata . get ( \"shots_per_state\" ), ( int , float )): mem_shots = int ( metadata [ \"shots_per_state\" ]) mem_params [ \"mem_shots\" ] = mem_shots if isinstance ( metadata . get ( \"qubits\" ), ( list , tuple )): mem_qubits = [ int ( q ) for q in metadata [ \"qubits\" ]] mem_params [ \"mem_qubits\" ] = mem_qubits except FileNotFoundError : LOGGER . warning ( \"Configured confusion matrix %s not found; recalibrating.\" , mem_confusion_path_str , ) mem_confusion_path_str = None if ( self . shadow_impl . mem . confusion_matrix is None or mem_force or not mem_confusion_path_str ): mem_dir = self . data_dir / \"mem\" mem_dir . mkdir ( parents = True , exist_ok = True ) confusion_matrix_path = mem_dir / f \" { experiment_id } .npz\" saved_confusion_path = self . shadow_impl . mem . calibrate ( mem_qubits , shots = mem_shots , run_options = run_options , output_path = confusion_matrix_path , ) mem_confusion_path = ( saved_confusion_path if saved_confusion_path is not None else confusion_matrix_path ) self . mitigation_config . confusion_matrix_path = str ( mem_confusion_path . resolve ()) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path self . shadow_impl . mem . confusion_matrix_path = Path ( mem_confusion_path_str ) else : self . mitigation_config . confusion_matrix_path = mem_confusion_path_str if \"MEM\" not in self . mitigation_config . techniques : self . mitigation_config . techniques . append ( \"MEM\" ) mem_params [ \"mem_qubits\" ] = mem_qubits mem_params [ \"mem_shots\" ] = mem_shots # Transpile for backend transpiled_circuits = transpile ( shadow_circuits , backend = self . backend ) # Respect backend batching limits max_experiments = None backend_config = None if hasattr ( self . backend , \"configuration\" ): try : backend_config = self . backend . configuration () except Exception : backend_config = None if backend_config is not None : max_experiments = getattr ( backend_config , \"max_experiments\" , None ) if isinstance ( max_experiments , np . integer ): max_experiments = int ( max_experiments ) if not isinstance ( max_experiments , int ) or max_experiments <= 0 : # Use safe default batch size for IBM backends to avoid submission failures max_experiments = 500 print ( f \"Warning: Backend max_experiments unavailable or invalid. \" f \"Using safe default batch size: { max_experiments } \" ) measurement_outcomes_list : list [ np . ndarray ] = [] sampler = self . _get_runtime_sampler () for start_idx in range ( 0 , len ( transpiled_circuits ), max_experiments ): circuit_batch = transpiled_circuits [ start_idx : start_idx + max_experiments ] if sampler is not None : job = sampler . run ( list ( circuit_batch ), shots = 1 ) result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result [ batch_idx ] . data . meas . get_counts () bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) else : job = self . backend . run ( circuit_batch , shots = 1 ) # Each circuit is one shadow result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result . get_counts ( batch_idx ) bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) if len ( measurement_outcomes_list ) != shadow_size : raise RuntimeError ( \"Collected measurement outcomes do not match the requested shadow size.\" ) measurement_outcomes = np . asarray ( measurement_outcomes_list , dtype = int ) measurement_bases = self . shadow_impl . measurement_bases if measurement_bases is None : raise ValueError ( \"Shadow implementation did not record measurement bases.\" ) measurement_bases = np . asarray ( measurement_bases , dtype = int ) self . shadow_impl . measurement_bases = measurement_bases # Save shot data to Parquet shot_data_path = self . shot_data_writer . save_shadow_measurements ( experiment_id = experiment_id , measurement_bases = measurement_bases , measurement_outcomes = measurement_outcomes , num_qubits = circuit . num_qubits , ) # Reconstruct shadows self . shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = self . shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } execution_time = time . time () - start_time # Create provenance manifest if save_manifest : manifest = self . _create_manifest ( experiment_id , circuit , observables , estimates , shadow_size , execution_time , shot_data_path , ) manifest_path = self . data_dir / \"manifests\" / f \" { experiment_id } .json\" manifest_path . parent . mkdir ( parents = True , exist_ok = True ) manifest . to_json ( manifest_path ) else : manifest_path = None return EstimationResult ( observables = estimates , shots_used = shadow_size , execution_time = execution_time , backend_name = self . backend . name , experiment_id = experiment_id , manifest_path = str ( manifest_path ) if manifest_path else None , shot_data_path = str ( shot_data_path . resolve ()), mitigation_confusion_matrix_path = self . mitigation_config . confusion_matrix_path , ) def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\"Estimate shadow size needed for target precision.\"\"\" # Use worst-case observable max_shadow_size = 0 for obs in observables : size = self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) max_shadow_size = max ( max_shadow_size , size ) return max_shadow_size def replay_from_manifest ( self , manifest_path : str | Path , observables : list [ Observable ] | None = None , ) -> EstimationResult : \"\"\" Replay an experiment from a saved manifest and shot data. This allows re-estimation of observables from previously collected shot data without re-executing circuits on the backend. Args: manifest_path: Path to the provenance manifest JSON file observables: Optional new list of observables to estimate. If None, uses observables from the original manifest. Returns: EstimationResult with re-estimated observables \"\"\" manifest_path = Path ( manifest_path ) if not manifest_path . exists (): raise FileNotFoundError ( f \"Manifest not found: { manifest_path } \" ) # Load manifest manifest = ProvenanceManifest . from_json ( manifest_path ) experiment_id = manifest . schema . experiment_id # Load shot data measurement_bases , measurement_outcomes , num_qubits = ( self . shot_data_writer . load_shadow_measurements ( experiment_id ) ) if manifest . schema . shadows is None : raise ValueError ( \"Manifest does not contain classical shadows configuration information.\" ) # Reconstruct shadows with loaded data # Create temporary shadow implementation if needed shadow_payload = manifest . schema . shadows . model_dump () shadow_payload [ \"random_seed\" ] = manifest . schema . random_seed shadow_config = ShadowConfig . model_validate ( shadow_payload ) resolved_confusion_matrix_path : str | None = ( manifest . schema . mitigation . confusion_matrix_path ) if shadow_config . version == ShadowVersion . V0_BASELINE : shadow_impl = RandomLocalCliffordShadows ( shadow_config ) elif shadow_config . version == ShadowVersion . V1_NOISE_AWARE : confusion_matrix_path_str = manifest . schema . mitigation . confusion_matrix_path if not confusion_matrix_path_str : raise FileNotFoundError ( \"Noise-aware manifest does not include a persisted confusion matrix path. \" \"Re-run estimation or provide the saved calibration artifact before replaying.\" ) raw_confusion_path = Path ( confusion_matrix_path_str ) candidate_paths = [ raw_confusion_path ] if not raw_confusion_path . is_absolute (): candidate_paths . append (( manifest_path . parent / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / \"mem\" / raw_confusion_path . name ) . resolve ()) candidate_paths . append ( ( manifest_path . parent / \"mem\" / raw_confusion_path . name ) . resolve () ) confusion_matrix_path : Path | None = None for candidate in candidate_paths : if candidate and candidate . exists (): confusion_matrix_path = candidate break if confusion_matrix_path is None : raise FileNotFoundError ( \"Unable to locate the persisted confusion matrix required for noise-aware replay. \" f \"Looked for { raw_confusion_path } and related paths.\" ) with np . load ( confusion_matrix_path , allow_pickle = False ) as archive : if \"confusion_matrix\" not in archive : raise ValueError ( \"Confusion matrix archive is missing the 'confusion_matrix' dataset.\" ) confusion_matrix = archive [ \"confusion_matrix\" ] mem = MeasurementErrorMitigation ( self . backend ) mem . confusion_matrix = confusion_matrix mem . confusion_matrix_path = confusion_matrix_path . resolve () mem . _calibrated_qubits = tuple ( range ( num_qubits )) shadow_impl = NoiseAwareRandomLocalCliffordShadows ( shadow_config , mem ) resolved_confusion_matrix_path = str ( confusion_matrix_path . resolve ()) else : raise NotImplementedError ( f \"Replay for shadow version { shadow_config . version . value } is not implemented\" ) shadow_impl . measurement_bases = measurement_bases shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Use observables from manifest if not provided if observables is None : observables = [ Observable ( obs_dict [ \"pauli\" ], obs_dict . get ( \"coefficient\" , 1.0 )) for obs_dict in manifest . schema . observables ] # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } return EstimationResult ( observables = estimates , shots_used = manifest . schema . shadows . shadow_size , execution_time = 0.0 , # No execution time for replay backend_name = manifest . schema . backend . backend_name , experiment_id = experiment_id , manifest_path = str ( manifest_path ), shot_data_path = manifest . schema . shot_data_path , mitigation_confusion_matrix_path = resolved_confusion_matrix_path , ) def _create_manifest ( self , experiment_id : str , circuit : QuantumCircuit , observables : list [ Observable ], estimates : dict [ str , dict [ str , object ]], shadow_size : int , execution_time : float , shot_data_path : Path , ) -> ProvenanceManifest : \"\"\"Create provenance manifest for the experiment.\"\"\" import sys import qiskit # Circuit fingerprint try : qasm_str = qasm3 . dumps ( circuit ) except Exception : qasm_str = circuit . qasm () gate_counts : dict [ str , int ] = {} for instruction in circuit . data : gate_name = instruction . operation . name gate_counts [ gate_name ] = gate_counts . get ( gate_name , 0 ) + 1 circuit_hash = hashlib . sha256 ( qasm_str . encode ()) . hexdigest ()[: 16 ] circuit_fp = CircuitFingerprint ( qasm3 = qasm_str , num_qubits = circuit . num_qubits , depth = circuit . depth (), gate_counts = gate_counts , circuit_hash = circuit_hash , ) # Backend snapshot backend_snapshot = self . _backend_snapshot or create_backend_snapshot ( self . backend ) # Shadows config shadows_config = ShadowsConfig . model_validate ( { \"version\" : self . shadow_config . version . value , \"shadow_size\" : shadow_size , \"measurement_ensemble\" : self . shadow_config . measurement_ensemble . value , \"noise_model_path\" : self . shadow_config . noise_model_path , \"inverse_channel_applied\" : self . shadow_config . apply_inverse_channel , \"fermionic_mode\" : self . shadow_config . fermionic_mode , \"rdm_order\" : self . shadow_config . rdm_order , \"adaptive\" : self . shadow_config . adaptive , \"target_observables\" : self . shadow_config . target_observables , \"bayesian_inference\" : self . shadow_config . bayesian_inference , \"bootstrap_samples\" : self . shadow_config . bootstrap_samples , } ) # Resource usage resource_usage = ResourceUsage . model_validate ( { \"total_shots\" : shadow_size , \"execution_time_seconds\" : execution_time , \"queue_time_seconds\" : None , \"estimated_cost_usd\" : None , \"credits_used\" : None , \"classical_compute_seconds\" : None , } ) metadata = {} if self . _backend_descriptor : metadata [ \"backend_descriptor\" ] = self . _backend_descriptor # Create manifest shot_checksum = compute_file_checksum ( shot_data_path ) mitigation_config = self . mitigation_config . model_copy ( deep = True ) confusion_path = mitigation_config . confusion_matrix_path if confusion_path : mitigation_config . confusion_matrix_checksum = compute_file_checksum ( confusion_path ) manifest_schema = ManifestSchema ( experiment_id = experiment_id , experiment_name = None , circuit = circuit_fp , observables = [ { \"pauli\" : obs . pauli_string , \"coefficient\" : obs . coefficient } for obs in observables ], backend = backend_snapshot , mitigation = mitigation_config , shadows = shadows_config , shot_data_path = str ( shot_data_path . resolve ()), shot_data_checksum = shot_checksum , results_summary = estimates , resource_usage = resource_usage , metadata = metadata , random_seed = self . shadow_config . random_seed , quartumse_version = __version__ , qiskit_version = qiskit . __version__ , python_version = f \" { sys . version_info . major } . { sys . version_info . minor } . { sys . version_info . micro } \" , ) return ProvenanceManifest ( manifest_schema )","title":"ShadowEstimator"},{"location":"reference/api/#quartumse.ShadowEstimator.__init__","text":"Initialize shadow estimator. Parameters: backend ( Backend | str ) \u2013 Qiskit backend or backend name (e.g., \"aer_simulator\") shadow_config ( ShadowConfig | None , default: None ) \u2013 Classical shadows configuration mitigation_config ( MitigationConfig | None , default: None ) \u2013 Error mitigation configuration data_dir ( str | Path | None , default: None ) \u2013 Directory for storing shot data and manifests Source code in src/quartumse/estimator/shadow_estimator.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , backend : Backend | str , shadow_config : ShadowConfig | None = None , mitigation_config : MitigationConfig | None = None , data_dir : str | Path | None = None , ): \"\"\" Initialize shadow estimator. Args: backend: Qiskit backend or backend name (e.g., \"aer_simulator\") shadow_config: Classical shadows configuration mitigation_config: Error mitigation configuration data_dir: Directory for storing shot data and manifests \"\"\" # Handle backend self . _backend_descriptor : str | None = None self . _backend_snapshot : BackendSnapshot | None = None if isinstance ( backend , str ): self . _backend_descriptor = backend if \":\" in backend : resolved_backend , snapshot = resolve_backend ( backend ) backend = resolved_backend self . _backend_snapshot = snapshot elif backend == \"aer_simulator\" : backend = AerSimulator () self . _backend_snapshot = create_backend_snapshot ( backend ) else : raise ValueError ( f \"Unknown backend string: { backend } \" ) else : self . _backend_descriptor = getattr ( backend , \"name\" , None ) super () . __init__ ( backend , shadow_config ) self . _runtime_sampler : SamplerPrimitive | None = None self . _runtime_sampler_checked = False self . _use_runtime_sampler = is_ibm_runtime_backend ( self . backend ) self . shadow_config = shadow_config or ShadowConfig . model_validate ({}) self . mitigation_config = mitigation_config or MitigationConfig () self . data_dir = Path ( data_dir ) if data_dir else Path ( \"./data\" ) self . data_dir . mkdir ( parents = True , exist_ok = True ) self . measurement_error_mitigation : MeasurementErrorMitigation | None = None self . _mem_required = ( self . shadow_config . version == ShadowVersion . V1_NOISE_AWARE or self . shadow_config . apply_inverse_channel or ( \"MEM\" in self . mitigation_config . techniques ) ) if self . _mem_required : self . measurement_error_mitigation = MeasurementErrorMitigation ( self . backend ) # Initialize shadow implementation based on version self . shadow_impl : ClassicalShadows = self . _create_shadow_implementation () # Initialize shot data writer self . shot_data_writer = ShotDataWriter ( self . data_dir )","title":"__init__"},{"location":"reference/api/#quartumse.ShadowEstimator.estimate","text":"Estimate observables using classical shadows. Workflow: 1. Generate shadow measurement circuits 2. Transpile and execute on backend 3. Reconstruct shadow snapshots 4. Estimate all observables 5. Generate provenance manifest Source code in src/quartumse/estimator/shadow_estimator.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 def estimate ( self , circuit : QuantumCircuit , observables : list [ Observable ], target_precision : float | None = None , save_manifest : bool = True , ) -> EstimationResult : \"\"\" Estimate observables using classical shadows. Workflow: 1. Generate shadow measurement circuits 2. Transpile and execute on backend 3. Reconstruct shadow snapshots 4. Estimate all observables 5. Generate provenance manifest \"\"\" experiment_id = str ( uuid . uuid4 ()) start_time = time . time () # Determine shadow size if target_precision : required_sizes = [ self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) for obs in observables ] shadow_size = max ( required_sizes ) if required_sizes else self . shadow_config . shadow_size if shadow_size <= 0 : raise ValueError ( \"Shadow size estimation produced a non-positive value\" ) self . shadow_config . shadow_size = shadow_size self . shadow_impl . config . shadow_size = shadow_size else : shadow_size = self . shadow_config . shadow_size self . shadow_impl . config . shadow_size = shadow_size # Generate shadow measurement circuits shadow_circuits = self . shadow_impl . generate_measurement_circuits ( circuit , shadow_size ) # Calibrate measurement error mitigation if required if isinstance ( self . shadow_impl , NoiseAwareRandomLocalCliffordShadows ): mem_params = self . mitigation_config . parameters mem_shots = int ( mem_params . get ( \"mem_shots\" , 4096 )) mem_qubits_param = mem_params . get ( \"mem_qubits\" ) if mem_qubits_param is None : mem_qubits = list ( range ( circuit . num_qubits )) elif isinstance ( mem_qubits_param , ( list , tuple )): mem_qubits = [ int ( q ) for q in mem_qubits_param ] else : mem_qubits = [ int ( mem_qubits_param )] mem_force = bool ( mem_params . get ( \"mem_force_calibration\" , False )) run_options = mem_params . get ( \"mem_run_options\" , {}) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path if mem_confusion_path_str and not mem_force : try : self . shadow_impl . mem . load_confusion_matrix ( mem_confusion_path_str ) metadata = self . shadow_impl . mem . get_confusion_metadata () if isinstance ( metadata . get ( \"shots_per_state\" ), ( int , float )): mem_shots = int ( metadata [ \"shots_per_state\" ]) mem_params [ \"mem_shots\" ] = mem_shots if isinstance ( metadata . get ( \"qubits\" ), ( list , tuple )): mem_qubits = [ int ( q ) for q in metadata [ \"qubits\" ]] mem_params [ \"mem_qubits\" ] = mem_qubits except FileNotFoundError : LOGGER . warning ( \"Configured confusion matrix %s not found; recalibrating.\" , mem_confusion_path_str , ) mem_confusion_path_str = None if ( self . shadow_impl . mem . confusion_matrix is None or mem_force or not mem_confusion_path_str ): mem_dir = self . data_dir / \"mem\" mem_dir . mkdir ( parents = True , exist_ok = True ) confusion_matrix_path = mem_dir / f \" { experiment_id } .npz\" saved_confusion_path = self . shadow_impl . mem . calibrate ( mem_qubits , shots = mem_shots , run_options = run_options , output_path = confusion_matrix_path , ) mem_confusion_path = ( saved_confusion_path if saved_confusion_path is not None else confusion_matrix_path ) self . mitigation_config . confusion_matrix_path = str ( mem_confusion_path . resolve ()) mem_confusion_path_str = self . mitigation_config . confusion_matrix_path self . shadow_impl . mem . confusion_matrix_path = Path ( mem_confusion_path_str ) else : self . mitigation_config . confusion_matrix_path = mem_confusion_path_str if \"MEM\" not in self . mitigation_config . techniques : self . mitigation_config . techniques . append ( \"MEM\" ) mem_params [ \"mem_qubits\" ] = mem_qubits mem_params [ \"mem_shots\" ] = mem_shots # Transpile for backend transpiled_circuits = transpile ( shadow_circuits , backend = self . backend ) # Respect backend batching limits max_experiments = None backend_config = None if hasattr ( self . backend , \"configuration\" ): try : backend_config = self . backend . configuration () except Exception : backend_config = None if backend_config is not None : max_experiments = getattr ( backend_config , \"max_experiments\" , None ) if isinstance ( max_experiments , np . integer ): max_experiments = int ( max_experiments ) if not isinstance ( max_experiments , int ) or max_experiments <= 0 : # Use safe default batch size for IBM backends to avoid submission failures max_experiments = 500 print ( f \"Warning: Backend max_experiments unavailable or invalid. \" f \"Using safe default batch size: { max_experiments } \" ) measurement_outcomes_list : list [ np . ndarray ] = [] sampler = self . _get_runtime_sampler () for start_idx in range ( 0 , len ( transpiled_circuits ), max_experiments ): circuit_batch = transpiled_circuits [ start_idx : start_idx + max_experiments ] if sampler is not None : job = sampler . run ( list ( circuit_batch ), shots = 1 ) result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result [ batch_idx ] . data . meas . get_counts () bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) else : job = self . backend . run ( circuit_batch , shots = 1 ) # Each circuit is one shadow result = job . result () for batch_idx , _ in enumerate ( circuit_batch ): counts = result . get_counts ( batch_idx ) bitstring = list ( counts . keys ())[ 0 ] . replace ( \" \" , \"\" ) outcomes = np . array ([ int ( b ) for b in bitstring [:: - 1 ]], dtype = int ) measurement_outcomes_list . append ( outcomes ) if len ( measurement_outcomes_list ) != shadow_size : raise RuntimeError ( \"Collected measurement outcomes do not match the requested shadow size.\" ) measurement_outcomes = np . asarray ( measurement_outcomes_list , dtype = int ) measurement_bases = self . shadow_impl . measurement_bases if measurement_bases is None : raise ValueError ( \"Shadow implementation did not record measurement bases.\" ) measurement_bases = np . asarray ( measurement_bases , dtype = int ) self . shadow_impl . measurement_bases = measurement_bases # Save shot data to Parquet shot_data_path = self . shot_data_writer . save_shadow_measurements ( experiment_id = experiment_id , measurement_bases = measurement_bases , measurement_outcomes = measurement_outcomes , num_qubits = circuit . num_qubits , ) # Reconstruct shadows self . shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = self . shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } execution_time = time . time () - start_time # Create provenance manifest if save_manifest : manifest = self . _create_manifest ( experiment_id , circuit , observables , estimates , shadow_size , execution_time , shot_data_path , ) manifest_path = self . data_dir / \"manifests\" / f \" { experiment_id } .json\" manifest_path . parent . mkdir ( parents = True , exist_ok = True ) manifest . to_json ( manifest_path ) else : manifest_path = None return EstimationResult ( observables = estimates , shots_used = shadow_size , execution_time = execution_time , backend_name = self . backend . name , experiment_id = experiment_id , manifest_path = str ( manifest_path ) if manifest_path else None , shot_data_path = str ( shot_data_path . resolve ()), mitigation_confusion_matrix_path = self . mitigation_config . confusion_matrix_path , )","title":"estimate"},{"location":"reference/api/#quartumse.ShadowEstimator.estimate_shots_needed","text":"Estimate shadow size needed for target precision. Source code in src/quartumse/estimator/shadow_estimator.py 368 369 370 371 372 373 374 375 376 def estimate_shots_needed ( self , observables : list [ Observable ], target_precision : float ) -> int : \"\"\"Estimate shadow size needed for target precision.\"\"\" # Use worst-case observable max_shadow_size = 0 for obs in observables : size = self . shadow_impl . estimate_shadow_size_needed ( obs , target_precision ) max_shadow_size = max ( max_shadow_size , size ) return max_shadow_size","title":"estimate_shots_needed"},{"location":"reference/api/#quartumse.ShadowEstimator.replay_from_manifest","text":"Replay an experiment from a saved manifest and shot data. This allows re-estimation of observables from previously collected shot data without re-executing circuits on the backend. Parameters: manifest_path ( str | Path ) \u2013 Path to the provenance manifest JSON file observables ( list [ Observable ] | None , default: None ) \u2013 Optional new list of observables to estimate. If None, uses observables from the original manifest. Returns: EstimationResult \u2013 EstimationResult with re-estimated observables Source code in src/quartumse/estimator/shadow_estimator.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 def replay_from_manifest ( self , manifest_path : str | Path , observables : list [ Observable ] | None = None , ) -> EstimationResult : \"\"\" Replay an experiment from a saved manifest and shot data. This allows re-estimation of observables from previously collected shot data without re-executing circuits on the backend. Args: manifest_path: Path to the provenance manifest JSON file observables: Optional new list of observables to estimate. If None, uses observables from the original manifest. Returns: EstimationResult with re-estimated observables \"\"\" manifest_path = Path ( manifest_path ) if not manifest_path . exists (): raise FileNotFoundError ( f \"Manifest not found: { manifest_path } \" ) # Load manifest manifest = ProvenanceManifest . from_json ( manifest_path ) experiment_id = manifest . schema . experiment_id # Load shot data measurement_bases , measurement_outcomes , num_qubits = ( self . shot_data_writer . load_shadow_measurements ( experiment_id ) ) if manifest . schema . shadows is None : raise ValueError ( \"Manifest does not contain classical shadows configuration information.\" ) # Reconstruct shadows with loaded data # Create temporary shadow implementation if needed shadow_payload = manifest . schema . shadows . model_dump () shadow_payload [ \"random_seed\" ] = manifest . schema . random_seed shadow_config = ShadowConfig . model_validate ( shadow_payload ) resolved_confusion_matrix_path : str | None = ( manifest . schema . mitigation . confusion_matrix_path ) if shadow_config . version == ShadowVersion . V0_BASELINE : shadow_impl = RandomLocalCliffordShadows ( shadow_config ) elif shadow_config . version == ShadowVersion . V1_NOISE_AWARE : confusion_matrix_path_str = manifest . schema . mitigation . confusion_matrix_path if not confusion_matrix_path_str : raise FileNotFoundError ( \"Noise-aware manifest does not include a persisted confusion matrix path. \" \"Re-run estimation or provide the saved calibration artifact before replaying.\" ) raw_confusion_path = Path ( confusion_matrix_path_str ) candidate_paths = [ raw_confusion_path ] if not raw_confusion_path . is_absolute (): candidate_paths . append (( manifest_path . parent / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / raw_confusion_path ) . resolve ()) candidate_paths . append (( self . data_dir / \"mem\" / raw_confusion_path . name ) . resolve ()) candidate_paths . append ( ( manifest_path . parent / \"mem\" / raw_confusion_path . name ) . resolve () ) confusion_matrix_path : Path | None = None for candidate in candidate_paths : if candidate and candidate . exists (): confusion_matrix_path = candidate break if confusion_matrix_path is None : raise FileNotFoundError ( \"Unable to locate the persisted confusion matrix required for noise-aware replay. \" f \"Looked for { raw_confusion_path } and related paths.\" ) with np . load ( confusion_matrix_path , allow_pickle = False ) as archive : if \"confusion_matrix\" not in archive : raise ValueError ( \"Confusion matrix archive is missing the 'confusion_matrix' dataset.\" ) confusion_matrix = archive [ \"confusion_matrix\" ] mem = MeasurementErrorMitigation ( self . backend ) mem . confusion_matrix = confusion_matrix mem . confusion_matrix_path = confusion_matrix_path . resolve () mem . _calibrated_qubits = tuple ( range ( num_qubits )) shadow_impl = NoiseAwareRandomLocalCliffordShadows ( shadow_config , mem ) resolved_confusion_matrix_path = str ( confusion_matrix_path . resolve ()) else : raise NotImplementedError ( f \"Replay for shadow version { shadow_config . version . value } is not implemented\" ) shadow_impl . measurement_bases = measurement_bases shadow_impl . reconstruct_classical_shadow ( measurement_outcomes , measurement_bases ) # Use observables from manifest if not provided if observables is None : observables = [ Observable ( obs_dict [ \"pauli\" ], obs_dict . get ( \"coefficient\" , 1.0 )) for obs_dict in manifest . schema . observables ] # Estimate all observables estimates : dict [ str , dict [ str , object ]] = {} for obs in observables : estimate = shadow_impl . estimate_observable ( obs ) estimates [ str ( obs )] = { \"expectation_value\" : estimate . expectation_value , \"variance\" : estimate . variance , \"ci_95\" : estimate . confidence_interval , \"ci_width\" : estimate . ci_width , } return EstimationResult ( observables = estimates , shots_used = manifest . schema . shadows . shadow_size , execution_time = 0.0 , # No execution time for replay backend_name = manifest . schema . backend . backend_name , experiment_id = experiment_id , manifest_path = str ( manifest_path ), shot_data_path = manifest . schema . shot_data_path , mitigation_confusion_matrix_path = resolved_confusion_matrix_path , )","title":"replay_from_manifest"},{"location":"reference/api/#commands","text":"See the CLI reference for command-line usage details.","title":"Commands"},{"location":"strategy/phase1_reference_runs/","text":"Phase 1 Reference Simulation Procedure \u00b6 This document defines the procedure used during Phase 1 to generate and maintain reusable reference datasets for QuartumSE experiments. The goal is to capture a high-fidelity simulator baseline that downstream experiments can replay without resampling, ensuring that shot budgets are preserved for hardware executions. Overview \u00b6 Reference runs are executed on the Qiskit aer_simulator backend using the classical-shadows estimator. Artifacts are stored under data/ : Artifact Location Provenance manifests data/manifests/ Persisted shadow measurement parquet data/shots/ Reference manifest index data/manifests/reference_index.json Each manifest contains a metadata.reference_dataset block and is tagged with reference-dataset plus scenario-specific tags (e.g., phase1 , ghz ). The index file is used by experiment scripts to look up a reference run before generating new data. Simulator configuration \u00b6 Phase 1 reference runs use the following simulator settings: Backend : aer_simulator (default configuration, single shot queue) Shadow version : Baseline v0 for noise-free references and v1 when MEM calibration data is required. Random seed : Fixed to 42 to guarantee deterministic measurement bases. Measurement ensemble : Local random Clifford rotations (default for ShadowConfig ). Output precision : 64-bit floating point expectations with 95% confidence intervals generated by the estimator. Shot counts \u00b6 Scenario Measurement shots Calibration shots Notes GHZ reference ( v0 ) 4,096 0 Three-qubit GHZ prepared via CNOT ladder. GHZ reference ( v1 + MEM) 4,096 4,096 MEM shots allocated as 512 per basis state on three qubits. Tip: When a configuration includes MEM calibration the calibration shots are recorded inside the manifest metadata and will be skipped when an existing reference run is replayed. Naming and metadata \u00b6 Reference datasets are keyed by a slug that uniquely identifies the scenario. Recommended convention: {phase}-{circuit}-{variant}-n{num_qubits}-s{measurement_shots} Example: phase1-ghz-v0-n3-s4096 . When a reference run completes, the manifest metadata is populated with: \"reference_dataset\" : { \"slug\" : \"phase1-ghz-v0-n3-s4096\" , \"phase\" : \"phase1\" , \"experiment\" : \"ghz-reference\" , \"run_name\" : \"ghz-3q-baseline\" , \"variant\" : \"v0\" , \"backend_descriptor\" : \"aer_simulator\" , \"shadow_size\" : 4096 , \"num_qubits\" : 3 , \"observable_count\" : 5 , \"calibration_shots\" : 0 , \"registered_at\" : \"<UTC timestamp>\" , \"last_used_at\" : \"<UTC timestamp>\" , \"tags\" : [ \"phase1\" , \"reference\" , \"ghz\" ] } The manifest tags field is also extended with the reference-dataset marker and all scenario tags, allowing simple filtering. Execution steps \u00b6 Prepare configuration \u2013 Either use the default YAML configuration embedded in the template CLI or author a custom config file describing the runs (see below). Invoke the template CLI \u2013 Run python experiments/reference/run_phase1_reference.py with optional overrides such as --config custom.yml or --backend aer_simulator . Replay when available \u2013 The CLI (through ReferenceDatasetRegistry ) checks reference_index.json and manifest metadata. If an entry with the requested slug exists, it is replayed via the estimator without queuing new shots. Review outputs \u2013 The CLI prints the manifest and shot data paths for each run. Additional summary files can be generated later using quartumse report against the saved manifests. Configuration schema \u00b6 Custom configurations are authored as YAML or JSON with the shape: experiment_name : ghz-reference phase : phase1 default_tags : [ phase1 , reference ] runs : - name : ghz-3q-baseline reference_slug : phase1-ghz-v0-n3-s4096 circuit : ghz num_qubits : 3 variant : v0 shadow_size : 4096 backend : aer_simulator tags : [ ghz ] observables : - pauli : ZII - pauli : IZI - pauli : IIZ - pauli : ZZI - pauli : ZZZ - name : ghz-3q-mem reference_slug : phase1-ghz-v1-n3-s4096 circuit : ghz num_qubits : 3 variant : v1 shadow_size : 4096 mem_shots : 512 backend : aer_simulator tags : [ ghz , mem ] observables : *same_as_above Each run entry is processed independently. Omitting mem_shots defaults to the CLI argument (512). The ReferenceDatasetRegistry will add bookkeeping fields such as registered_at and last_used_at automatically. Manifest index maintenance \u00b6 The registry updates reference_index.json on every successful run. Stale entries are pruned automatically if the manifest file is removed. When editing manifests manually, keep the reference_dataset.slug field in sync with the filename or update the index by rerunning the CLI with --force to regenerate the dataset. Following this procedure guarantees that all Phase 1 simulators share the same reference baselines and that manifests carry enough metadata for downstream automation to reason about provenance and reuse.","title":"Phase 1 Reference Simulation Procedure"},{"location":"strategy/phase1_reference_runs/#phase-1-reference-simulation-procedure","text":"This document defines the procedure used during Phase 1 to generate and maintain reusable reference datasets for QuartumSE experiments. The goal is to capture a high-fidelity simulator baseline that downstream experiments can replay without resampling, ensuring that shot budgets are preserved for hardware executions.","title":"Phase 1 Reference Simulation Procedure"},{"location":"strategy/phase1_reference_runs/#overview","text":"Reference runs are executed on the Qiskit aer_simulator backend using the classical-shadows estimator. Artifacts are stored under data/ : Artifact Location Provenance manifests data/manifests/ Persisted shadow measurement parquet data/shots/ Reference manifest index data/manifests/reference_index.json Each manifest contains a metadata.reference_dataset block and is tagged with reference-dataset plus scenario-specific tags (e.g., phase1 , ghz ). The index file is used by experiment scripts to look up a reference run before generating new data.","title":"Overview"},{"location":"strategy/phase1_reference_runs/#simulator-configuration","text":"Phase 1 reference runs use the following simulator settings: Backend : aer_simulator (default configuration, single shot queue) Shadow version : Baseline v0 for noise-free references and v1 when MEM calibration data is required. Random seed : Fixed to 42 to guarantee deterministic measurement bases. Measurement ensemble : Local random Clifford rotations (default for ShadowConfig ). Output precision : 64-bit floating point expectations with 95% confidence intervals generated by the estimator.","title":"Simulator configuration"},{"location":"strategy/phase1_reference_runs/#shot-counts","text":"Scenario Measurement shots Calibration shots Notes GHZ reference ( v0 ) 4,096 0 Three-qubit GHZ prepared via CNOT ladder. GHZ reference ( v1 + MEM) 4,096 4,096 MEM shots allocated as 512 per basis state on three qubits. Tip: When a configuration includes MEM calibration the calibration shots are recorded inside the manifest metadata and will be skipped when an existing reference run is replayed.","title":"Shot counts"},{"location":"strategy/phase1_reference_runs/#naming-and-metadata","text":"Reference datasets are keyed by a slug that uniquely identifies the scenario. Recommended convention: {phase}-{circuit}-{variant}-n{num_qubits}-s{measurement_shots} Example: phase1-ghz-v0-n3-s4096 . When a reference run completes, the manifest metadata is populated with: \"reference_dataset\" : { \"slug\" : \"phase1-ghz-v0-n3-s4096\" , \"phase\" : \"phase1\" , \"experiment\" : \"ghz-reference\" , \"run_name\" : \"ghz-3q-baseline\" , \"variant\" : \"v0\" , \"backend_descriptor\" : \"aer_simulator\" , \"shadow_size\" : 4096 , \"num_qubits\" : 3 , \"observable_count\" : 5 , \"calibration_shots\" : 0 , \"registered_at\" : \"<UTC timestamp>\" , \"last_used_at\" : \"<UTC timestamp>\" , \"tags\" : [ \"phase1\" , \"reference\" , \"ghz\" ] } The manifest tags field is also extended with the reference-dataset marker and all scenario tags, allowing simple filtering.","title":"Naming and metadata"},{"location":"strategy/phase1_reference_runs/#execution-steps","text":"Prepare configuration \u2013 Either use the default YAML configuration embedded in the template CLI or author a custom config file describing the runs (see below). Invoke the template CLI \u2013 Run python experiments/reference/run_phase1_reference.py with optional overrides such as --config custom.yml or --backend aer_simulator . Replay when available \u2013 The CLI (through ReferenceDatasetRegistry ) checks reference_index.json and manifest metadata. If an entry with the requested slug exists, it is replayed via the estimator without queuing new shots. Review outputs \u2013 The CLI prints the manifest and shot data paths for each run. Additional summary files can be generated later using quartumse report against the saved manifests.","title":"Execution steps"},{"location":"strategy/phase1_reference_runs/#configuration-schema","text":"Custom configurations are authored as YAML or JSON with the shape: experiment_name : ghz-reference phase : phase1 default_tags : [ phase1 , reference ] runs : - name : ghz-3q-baseline reference_slug : phase1-ghz-v0-n3-s4096 circuit : ghz num_qubits : 3 variant : v0 shadow_size : 4096 backend : aer_simulator tags : [ ghz ] observables : - pauli : ZII - pauli : IZI - pauli : IIZ - pauli : ZZI - pauli : ZZZ - name : ghz-3q-mem reference_slug : phase1-ghz-v1-n3-s4096 circuit : ghz num_qubits : 3 variant : v1 shadow_size : 4096 mem_shots : 512 backend : aer_simulator tags : [ ghz , mem ] observables : *same_as_above Each run entry is processed independently. Omitting mem_shots defaults to the CLI argument (512). The ReferenceDatasetRegistry will add bookkeeping fields such as registered_at and last_used_at automatically.","title":"Configuration schema"},{"location":"strategy/phase1_reference_runs/#manifest-index-maintenance","text":"The registry updates reference_index.json on every successful run. Stale entries are pruned automatically if the manifest file is removed. When editing manifests manually, keep the reference_dataset.slug field in sync with the filename or update the index by rerunning the CLI with --force to regenerate the dataset. Following this procedure guarantees that all Phase 1 simulators share the same reference baselines and that manifests carry enough metadata for downstream automation to reason about provenance and reuse.","title":"Manifest index maintenance"},{"location":"strategy/phase1_task_checklist/","text":"Phase 1 Operational Task Checklist \u00b6 This checklist aggregates the outstanding Phase 1 tasks called out across the roadmap, experiment plans, and validation guides so the execution team can run them without cross-referencing multiple documents. Use it alongside the detailed procedures in experiments/shadows and the runtime runbook when scheduling IBM Quantum jobs. Shared infrastructure & preparation \u00b6 [x] Establish a reusable readout calibration workflow (circuit templates, manifest storage, reuse cadence) that precedes every hardware run. [x] Draft the runtime budgeting checklist (shot counts, batching, queue timing) to stay within the 10-minute IBM Quantum free-tier window. [x] Implement shared analysis utilities for shot-saving ratio (SSR), confidence-interval (CI) coverage, and variance tracking so experiments share the same metrics code. [x] Document how to generate and store high-statistics reference datasets (simulators or large-shot baselines) whenever analytical ground truth is unavailable. Readout calibration cadence and artifacts \u00b6 Invoke quartumse calibrate-readout --backend <provider:name> --qubit <i> ... before each hardware session to refresh confusion matrices. The CLI will reuse an existing archive unless --force is set or --max-age-hours expires. Calibrations live under validation_data/calibrations/<backend>/q<indices>/confusion_matrix.npz with a sibling .manifest.json capturing metadata ( backend_descriptor , shots_per_state , reuse flag, etc.). The manager in experiments/shadows/common_utils.py uses the same layout so GHZ, Bell, Clifford, Ising, and H\u2082 scripts automatically share matrices. Surface the cached path in manifests via MitigationConfig.confusion_matrix_path ; the CLI output and experiment metadata record this location directly for provenance linking. Recommended cadence: refresh for new backend calibrations, topology changes, or when the cached artifact exceeds its --max-age-hours threshold (default is unlimited reuse). Force a regeneration whenever qubit mappings change or significant readout drift is observed. Shadows workstream (S) \u00b6 Extended GHZ (S-T01/S-T02 bridge) \u00b6 [ ] Simulate connectivity-aware GHZ(4\u20135) preparation circuits for target backends. [ ] Integrate MEM-calibrated measurement routines and demonstrate mitigated fidelity \u2265 0.5. [ ] Expand observable estimation to full stabilizer + Mermin terms and compare against grouped measurement baselines. [ ] Run \u226510 hardware trials to study CI coverage/heavy tails; apply robust estimators if needed. [ ] Archive procedures, raw logs, processed metrics, and discussion notes in the experiment subfolders. Parallel Bell pairs (S\u2011BELL) \u00b6 [ ] Build 4-qubit (optionally 6/8-qubit) disjoint Bell-pair circuits and verify pairwise entanglement. [ ] Measure $ZZ$, $XX$, and CHSH combinations with MEM such that mitigated $S>2$. [ ] Quantify SSR gains for simultaneous subsystem observables versus per-pair grouped runs. [ ] File methodologies, datasets, and analyses under the dedicated directories. Random Clifford benchmarking (S\u2011CLIFF) \u00b6 [ ] Generate depth-limited random Clifford circuits (\u22655 qubits) and capture stabilizer references from simulation. [ ] Estimate \u226550 Pauli observables with shadows and grouped baselines, reporting MAE distributions and CI coverage. [ ] Execute direct fidelity estimation (DFE) and compare shot requirements to shadows. [ ] Store scripts, calibration notes, stats summaries, and interpretation artifacts. Ising chain (S\u2011ISING) \u00b6 [ ] Assemble first-order Trotter circuits for the 6-qubit transverse-field Ising model and validate expected observables in simulation. [ ] Collect hardware data comparing grouped vs. shadow measurements for equal shot budgets (track energy error/variance). [ ] Extract auxiliary observables (magnetization, correlators, energy variance) from the same shadows datasets. [ ] Document procedures, execution logs, analyses, and interpretation materials. H\u2082 energy (S\u2011CHEM) \u00b6 [ ] Implement the 4-qubit H\u2082 ansatz and benchmark ideal expectations for validation. [ ] Run shadow+MEM versus grouped-measurement comparisons, targeting 0.02\u20130.05 Ha accuracy and \u226530% uncertainty reduction. [ ] Monitor and correct estimator bias from locally weighted sampling if uncertainties are exceeded. [ ] Capture methodology narratives, raw/processed data, and publication-ready discussion notes. Cross-experiment reporting \u00b6 [ ] Aggregate the high-impact findings (Hamiltonian efficiency gains, entanglement recovery, multi-observable accuracy) into a consolidated Phase 1 report. [ ] Ensure each experiment\u2019s discussion notes document success criteria, SSR achievements, and limitations for manuscript prep. Validation gating tasks \u00b6 [ ] Complete the extended IBM hardware validation campaign (SSR \u2265 1.1\u00d7; manifests saved under validation_data/ ). [ ] Run the hardware validation post-checks: manifests present, calibration snapshot archived, v0 vs v1 MAE comparison, and results summarised in the Phase 1 status log section below. [ ] Verify validation CI coverage \u2265 80% and document Phase 1 completion once criteria are met. Cross-workstream starters (C/O/B/M) \u00b6 [ ] Execute C/O/B/M starter experiments on simulator and generate first data drops (manifests + shot data for Phase 1 closure). [ ] Confirm patent theme shortlist drafting continues ahead of the Phase 2 gate review. Note: Advanced shadow variants (v2 Fermionic, v3 Adaptive, v4 Robust) are Phase 2+ scope and intentionally excluded from this checklist to keep Phase 1 focused on v0/v1 hardware validation. How to use this checklist \u00b6 Copy relevant tasks into your sprint tracker, linking back to their detailed procedure files (design docs in experiments/shadows/**/ and the hardware validation design note). Before each IBM Quantum run, execute quartumse runtime-status --json --backend <backend> and record queue depth/runtime budget in the runbook. After completing an experiment, attach manifests, calibration data, and summary notebooks to the appropriate results/ and discussion/ folders and check the corresponding box here. When preparing a public release or milestone summary, update CHANGELOG.md with the scope of work that shipped before tagging. Update this document whenever a task is completed or a new Phase 1 dependency is identified. Use the log below to capture concise status updates instead of scattering notes across ad-hoc documents. Phase 1 status log \u00b6 Date Update 2025-10-22 IBM ibm_torino smoke test complete. GHZ v0/v1 + MEM validated, SSR \u2265 1.2\u00d7 on simulator, CI coverage 100%.","title":"Phase 1 Checklist"},{"location":"strategy/phase1_task_checklist/#phase-1-operational-task-checklist","text":"This checklist aggregates the outstanding Phase 1 tasks called out across the roadmap, experiment plans, and validation guides so the execution team can run them without cross-referencing multiple documents. Use it alongside the detailed procedures in experiments/shadows and the runtime runbook when scheduling IBM Quantum jobs.","title":"Phase 1 Operational Task Checklist"},{"location":"strategy/phase1_task_checklist/#shared-infrastructure-preparation","text":"[x] Establish a reusable readout calibration workflow (circuit templates, manifest storage, reuse cadence) that precedes every hardware run. [x] Draft the runtime budgeting checklist (shot counts, batching, queue timing) to stay within the 10-minute IBM Quantum free-tier window. [x] Implement shared analysis utilities for shot-saving ratio (SSR), confidence-interval (CI) coverage, and variance tracking so experiments share the same metrics code. [x] Document how to generate and store high-statistics reference datasets (simulators or large-shot baselines) whenever analytical ground truth is unavailable.","title":"Shared infrastructure &amp; preparation"},{"location":"strategy/phase1_task_checklist/#readout-calibration-cadence-and-artifacts","text":"Invoke quartumse calibrate-readout --backend <provider:name> --qubit <i> ... before each hardware session to refresh confusion matrices. The CLI will reuse an existing archive unless --force is set or --max-age-hours expires. Calibrations live under validation_data/calibrations/<backend>/q<indices>/confusion_matrix.npz with a sibling .manifest.json capturing metadata ( backend_descriptor , shots_per_state , reuse flag, etc.). The manager in experiments/shadows/common_utils.py uses the same layout so GHZ, Bell, Clifford, Ising, and H\u2082 scripts automatically share matrices. Surface the cached path in manifests via MitigationConfig.confusion_matrix_path ; the CLI output and experiment metadata record this location directly for provenance linking. Recommended cadence: refresh for new backend calibrations, topology changes, or when the cached artifact exceeds its --max-age-hours threshold (default is unlimited reuse). Force a regeneration whenever qubit mappings change or significant readout drift is observed.","title":"Readout calibration cadence and artifacts"},{"location":"strategy/phase1_task_checklist/#shadows-workstream-s","text":"","title":"Shadows workstream (S)"},{"location":"strategy/phase1_task_checklist/#extended-ghz-s-t01s-t02-bridge","text":"[ ] Simulate connectivity-aware GHZ(4\u20135) preparation circuits for target backends. [ ] Integrate MEM-calibrated measurement routines and demonstrate mitigated fidelity \u2265 0.5. [ ] Expand observable estimation to full stabilizer + Mermin terms and compare against grouped measurement baselines. [ ] Run \u226510 hardware trials to study CI coverage/heavy tails; apply robust estimators if needed. [ ] Archive procedures, raw logs, processed metrics, and discussion notes in the experiment subfolders.","title":"Extended GHZ (S-T01/S-T02 bridge)"},{"location":"strategy/phase1_task_checklist/#parallel-bell-pairs-sbell","text":"[ ] Build 4-qubit (optionally 6/8-qubit) disjoint Bell-pair circuits and verify pairwise entanglement. [ ] Measure $ZZ$, $XX$, and CHSH combinations with MEM such that mitigated $S>2$. [ ] Quantify SSR gains for simultaneous subsystem observables versus per-pair grouped runs. [ ] File methodologies, datasets, and analyses under the dedicated directories.","title":"Parallel Bell pairs (S\u2011BELL)"},{"location":"strategy/phase1_task_checklist/#random-clifford-benchmarking-scliff","text":"[ ] Generate depth-limited random Clifford circuits (\u22655 qubits) and capture stabilizer references from simulation. [ ] Estimate \u226550 Pauli observables with shadows and grouped baselines, reporting MAE distributions and CI coverage. [ ] Execute direct fidelity estimation (DFE) and compare shot requirements to shadows. [ ] Store scripts, calibration notes, stats summaries, and interpretation artifacts.","title":"Random Clifford benchmarking (S\u2011CLIFF)"},{"location":"strategy/phase1_task_checklist/#ising-chain-sising","text":"[ ] Assemble first-order Trotter circuits for the 6-qubit transverse-field Ising model and validate expected observables in simulation. [ ] Collect hardware data comparing grouped vs. shadow measurements for equal shot budgets (track energy error/variance). [ ] Extract auxiliary observables (magnetization, correlators, energy variance) from the same shadows datasets. [ ] Document procedures, execution logs, analyses, and interpretation materials.","title":"Ising chain (S\u2011ISING)"},{"location":"strategy/phase1_task_checklist/#h2-energy-schem","text":"[ ] Implement the 4-qubit H\u2082 ansatz and benchmark ideal expectations for validation. [ ] Run shadow+MEM versus grouped-measurement comparisons, targeting 0.02\u20130.05 Ha accuracy and \u226530% uncertainty reduction. [ ] Monitor and correct estimator bias from locally weighted sampling if uncertainties are exceeded. [ ] Capture methodology narratives, raw/processed data, and publication-ready discussion notes.","title":"H\u2082 energy (S\u2011CHEM)"},{"location":"strategy/phase1_task_checklist/#cross-experiment-reporting","text":"[ ] Aggregate the high-impact findings (Hamiltonian efficiency gains, entanglement recovery, multi-observable accuracy) into a consolidated Phase 1 report. [ ] Ensure each experiment\u2019s discussion notes document success criteria, SSR achievements, and limitations for manuscript prep.","title":"Cross-experiment reporting"},{"location":"strategy/phase1_task_checklist/#validation-gating-tasks","text":"[ ] Complete the extended IBM hardware validation campaign (SSR \u2265 1.1\u00d7; manifests saved under validation_data/ ). [ ] Run the hardware validation post-checks: manifests present, calibration snapshot archived, v0 vs v1 MAE comparison, and results summarised in the Phase 1 status log section below. [ ] Verify validation CI coverage \u2265 80% and document Phase 1 completion once criteria are met.","title":"Validation gating tasks"},{"location":"strategy/phase1_task_checklist/#cross-workstream-starters-cobm","text":"[ ] Execute C/O/B/M starter experiments on simulator and generate first data drops (manifests + shot data for Phase 1 closure). [ ] Confirm patent theme shortlist drafting continues ahead of the Phase 2 gate review. Note: Advanced shadow variants (v2 Fermionic, v3 Adaptive, v4 Robust) are Phase 2+ scope and intentionally excluded from this checklist to keep Phase 1 focused on v0/v1 hardware validation.","title":"Cross-workstream starters (C/O/B/M)"},{"location":"strategy/phase1_task_checklist/#how-to-use-this-checklist","text":"Copy relevant tasks into your sprint tracker, linking back to their detailed procedure files (design docs in experiments/shadows/**/ and the hardware validation design note). Before each IBM Quantum run, execute quartumse runtime-status --json --backend <backend> and record queue depth/runtime budget in the runbook. After completing an experiment, attach manifests, calibration data, and summary notebooks to the appropriate results/ and discussion/ folders and check the corresponding box here. When preparing a public release or milestone summary, update CHANGELOG.md with the scope of work that shipped before tagging. Update this document whenever a task is completed or a new Phase 1 dependency is identified. Use the log below to capture concise status updates instead of scattering notes across ad-hoc documents.","title":"How to use this checklist"},{"location":"strategy/phase1_task_checklist/#phase-1-status-log","text":"Date Update 2025-10-22 IBM ibm_torino smoke test complete. GHZ v0/v1 + MEM validated, SSR \u2265 1.2\u00d7 on simulator, CI coverage 100%.","title":"Phase 1 status log"},{"location":"strategy/project_bible/","text":"QuartumSE Project Bible \u2014 Strategic Blueprint (2025\u20132028) \u00b6 Vision & Positioning \u00b6 Product One-Liner: \u201cA vendor-neutral way to run quantum jobs with fewer shots and trusted error bars, plus a provenance report you can cite.\u201d Vision: Establish QuartumSE as the default quantum measurement and observability layer for all quantum computing teams. QuartumSE will provide a universal platform that maximizes the useful information gained per experiment (reducing cost per result ) and instill confidence via rigorous error estimates. It will offer a consistent way to run quantum jobs with minimal shots while delivering reliable error bars and detailed provenance. Think of QuartumSE as a neutral monitoring standard enabling robust cross-platform performance metrics and comparisons. By focusing on measurement quality and transparency, QuartumSE also plans to lay the groundwork for advanced capabilities like pulse-level optimizations and real-time error correction support (the upcoming AutoPulse and qLDPC modules). Target Users & Key Use Cases \u00b6 QuartumSE is being designed for quantum R&D practitioners across industry and academia: Algorithm Researchers: Reduce the number of shots (and thus cost) needed to achieve target error margins, while producing 95% confidence intervals for expectation values. Hardware Teams: Benchmark fairly across backends. QuartumSE aims to enable cross-platform comparisons with consistent mitigation and provenance for apples-to-apples cost-per-accuracy metrics. Enterprise R&D: Generate auditable, reproducible, compliance-ready reports. Provenance manifests will track every circuit, calibration, and configuration automatically. Differentiators & Unique Value Proposition \u00b6 Vendor-Neutral Platform: One SDK will work seamlessly with IBM Quantum, AWS Braket, and beyond. No lock-in. Cost-for-Accuracy Metrics: QuartumSE will introduce RMSE@$ and Shot-Savings Ratio (SSR) to quantify cost-efficiency. It answers: how many dollars to reach a given precision? \u201cMeasure Once, Ask Later\u201d: Classical shadows that will allow one set of randomized measurements to estimate multiple observables offline \u2013 maximizing insight per shot. Provenance & Auditability: Every run will produce a JSON Provenance Manifest and a PDF/HTML report with circuits, calibrations, mitigations, and cost. Local-First Design: All shot data and reports stored locally by default, with optional secure cloud sync. Suitable for sensitive or air\u2011gapped R&D. Future-Proof Modularity: At a later stage, QuartumSE plans to develop AutoPulse (pulse-level optimization) and qLDPC (error-correction integration) modules to ensure longevity beyond the NISQ era. Planned Technical Architecture \u00b6 Core Components: - Python SDK: QuartumSE.Estimator , QuartumSE.Shadows , QuartumSE.Report \u2014 one-call estimation with confidence intervals and provenance generation. - Mitigation & Shadows Engines: Automated orchestration of ZNE, MEM, PEC, and randomized compiling for accuracy-per-cost optimization. - Data Layer: Local DuckDB/Parquet storage + calibration snapshots; all runs logged to Provenance Manifest . - Connectors: Multi-cloud backends (IBM Qiskit Runtime, AWS Braket, extensible to IonQ/Rigetti). - Server & CLI: FastAPI REST service and Typer CLI for multi-user or CI/CD integration. - Extensibility: Plug-in modules for pulse optimization (AutoPulse) and real-time error correction (qLDPC). Competitive Landscape & Positioning \u00b6 QuartumSE would bridge a gap that vendor SDKs and point solutions leave open: Category Example Competitors How QuartumSE Differentiates Vendor SDKs IBM Qiskit, AWS Braket Cross-platform; unified reporting; cost-per-accuracy metrics Mitigation Libraries Mitiq, Qermit Full orchestration + provenance; end-to-end workflow Commercial Tools Q-CTRL Fire Opal, Keysight True\u2011Q Open, vendor\u2011neutral, audit\u2011ready results Workflow Platforms Zapata Orquestra, Covalent QuartumSE plugs in as a measurement optimizer; local\u2011first operation No other tool combines cross\u2011provider measurement optimization , multi\u2011observable reuse , and auditable cost\u2011for\u2011accuracy tracking in one open framework. Long-Term Roadmap Highlights \u00b6 (See detailed milestones in roadmap.md ) Year Focus Key Goals 2025\u201326 MVP & Design Partners IBM integration, SSR \u22651.3\u00d7, provenance reports, AWS Braket connector 2026\u201327 Public Beta & Expansion AutoPulse & qLDPC prototypes, SSR \u22652\u00d7, pilot customers 2027\u201328 Scale & Standardization 50+ orgs using QuartumSE; Provenance Manifest adopted as industry standard Vision Beyond NISQ \u00b6 QuartumSE is designed to evolve with the field: from today\u2019s noisy processors to tomorrow\u2019s error\u2011corrected systems. It will remain relevant by: - Integrating low\u2011level pulse optimization and logical\u2011qubit error\u2011correction data; - Tracking cross\u2011hardware cost/performance benchmarks; - Defining open standards for quantum experiment reporting; - Offering an enterprise\u2011ready observability layer for quantum runtime workflows. Conclusion \u00b6 QuartumSE is building the foundation for trust and efficiency in quantum computing . By measuring smarter, reporting transparently, and staying vendor\u2011neutral, QuartumSE is positioning itself to become the default measurement and observability standard of the quantum era \u2014 the open, reliable infrastructure every quantum team will depend on.","title":"Project Bible"},{"location":"strategy/project_bible/#quartumse-project-bible-strategic-blueprint-20252028","text":"","title":"QuartumSE Project Bible \u2014 Strategic Blueprint (2025\u20132028)"},{"location":"strategy/project_bible/#vision-positioning","text":"Product One-Liner: \u201cA vendor-neutral way to run quantum jobs with fewer shots and trusted error bars, plus a provenance report you can cite.\u201d Vision: Establish QuartumSE as the default quantum measurement and observability layer for all quantum computing teams. QuartumSE will provide a universal platform that maximizes the useful information gained per experiment (reducing cost per result ) and instill confidence via rigorous error estimates. It will offer a consistent way to run quantum jobs with minimal shots while delivering reliable error bars and detailed provenance. Think of QuartumSE as a neutral monitoring standard enabling robust cross-platform performance metrics and comparisons. By focusing on measurement quality and transparency, QuartumSE also plans to lay the groundwork for advanced capabilities like pulse-level optimizations and real-time error correction support (the upcoming AutoPulse and qLDPC modules).","title":"Vision &amp; Positioning"},{"location":"strategy/project_bible/#target-users-key-use-cases","text":"QuartumSE is being designed for quantum R&D practitioners across industry and academia: Algorithm Researchers: Reduce the number of shots (and thus cost) needed to achieve target error margins, while producing 95% confidence intervals for expectation values. Hardware Teams: Benchmark fairly across backends. QuartumSE aims to enable cross-platform comparisons with consistent mitigation and provenance for apples-to-apples cost-per-accuracy metrics. Enterprise R&D: Generate auditable, reproducible, compliance-ready reports. Provenance manifests will track every circuit, calibration, and configuration automatically.","title":"Target Users &amp; Key Use Cases"},{"location":"strategy/project_bible/#differentiators-unique-value-proposition","text":"Vendor-Neutral Platform: One SDK will work seamlessly with IBM Quantum, AWS Braket, and beyond. No lock-in. Cost-for-Accuracy Metrics: QuartumSE will introduce RMSE@$ and Shot-Savings Ratio (SSR) to quantify cost-efficiency. It answers: how many dollars to reach a given precision? \u201cMeasure Once, Ask Later\u201d: Classical shadows that will allow one set of randomized measurements to estimate multiple observables offline \u2013 maximizing insight per shot. Provenance & Auditability: Every run will produce a JSON Provenance Manifest and a PDF/HTML report with circuits, calibrations, mitigations, and cost. Local-First Design: All shot data and reports stored locally by default, with optional secure cloud sync. Suitable for sensitive or air\u2011gapped R&D. Future-Proof Modularity: At a later stage, QuartumSE plans to develop AutoPulse (pulse-level optimization) and qLDPC (error-correction integration) modules to ensure longevity beyond the NISQ era.","title":"Differentiators &amp; Unique Value Proposition"},{"location":"strategy/project_bible/#planned-technical-architecture","text":"Core Components: - Python SDK: QuartumSE.Estimator , QuartumSE.Shadows , QuartumSE.Report \u2014 one-call estimation with confidence intervals and provenance generation. - Mitigation & Shadows Engines: Automated orchestration of ZNE, MEM, PEC, and randomized compiling for accuracy-per-cost optimization. - Data Layer: Local DuckDB/Parquet storage + calibration snapshots; all runs logged to Provenance Manifest . - Connectors: Multi-cloud backends (IBM Qiskit Runtime, AWS Braket, extensible to IonQ/Rigetti). - Server & CLI: FastAPI REST service and Typer CLI for multi-user or CI/CD integration. - Extensibility: Plug-in modules for pulse optimization (AutoPulse) and real-time error correction (qLDPC).","title":"Planned Technical Architecture"},{"location":"strategy/project_bible/#competitive-landscape-positioning","text":"QuartumSE would bridge a gap that vendor SDKs and point solutions leave open: Category Example Competitors How QuartumSE Differentiates Vendor SDKs IBM Qiskit, AWS Braket Cross-platform; unified reporting; cost-per-accuracy metrics Mitigation Libraries Mitiq, Qermit Full orchestration + provenance; end-to-end workflow Commercial Tools Q-CTRL Fire Opal, Keysight True\u2011Q Open, vendor\u2011neutral, audit\u2011ready results Workflow Platforms Zapata Orquestra, Covalent QuartumSE plugs in as a measurement optimizer; local\u2011first operation No other tool combines cross\u2011provider measurement optimization , multi\u2011observable reuse , and auditable cost\u2011for\u2011accuracy tracking in one open framework.","title":"Competitive Landscape &amp; Positioning"},{"location":"strategy/project_bible/#long-term-roadmap-highlights","text":"(See detailed milestones in roadmap.md ) Year Focus Key Goals 2025\u201326 MVP & Design Partners IBM integration, SSR \u22651.3\u00d7, provenance reports, AWS Braket connector 2026\u201327 Public Beta & Expansion AutoPulse & qLDPC prototypes, SSR \u22652\u00d7, pilot customers 2027\u201328 Scale & Standardization 50+ orgs using QuartumSE; Provenance Manifest adopted as industry standard","title":"Long-Term Roadmap Highlights"},{"location":"strategy/project_bible/#vision-beyond-nisq","text":"QuartumSE is designed to evolve with the field: from today\u2019s noisy processors to tomorrow\u2019s error\u2011corrected systems. It will remain relevant by: - Integrating low\u2011level pulse optimization and logical\u2011qubit error\u2011correction data; - Tracking cross\u2011hardware cost/performance benchmarks; - Defining open standards for quantum experiment reporting; - Offering an enterprise\u2011ready observability layer for quantum runtime workflows.","title":"Vision Beyond NISQ"},{"location":"strategy/project_bible/#conclusion","text":"QuartumSE is building the foundation for trust and efficiency in quantum computing . By measuring smarter, reporting transparently, and staying vendor\u2011neutral, QuartumSE is positioning itself to become the default measurement and observability standard of the quantum era \u2014 the open, reliable infrastructure every quantum team will depend on.","title":"Conclusion"},{"location":"strategy/roadmap/","text":"QuartumSE R&D\u2011Centric Roadmap (Updated, 2025\u20132026) \u00b6 Last updated: 2025-10-24 Phase snapshot (Oct 2025) \u00b6 \u2705 Phase 1 scaffolding, provenance pipeline, and CI harness are live. \u2705 S\u2011T01 GHZ baseline + S\u2011T02 noise-aware runs validated on IBM ibm_torino (smoke test Oct 22, 2025). \u2705 IBM Runtime CLI ( quartumse runtime-status ) operational with webhook notifications. \u26a0\ufe0f Extended IBM hardware validation (target SSR \u2265 1.1 across repeated runs) scheduled for Nov 2025. \u26a0\ufe0f Cross-workstream starter experiments (C/O/B/M) need first data drops before Phase 1 closes. \ud83d\udcdd Patent theme shortlist drafting in progress ahead of the Phase 2 gate review. \ud83d\udccb See phase1_task_checklist.md for the consolidated execution checklist that enumerates every outstanding task before the Phase 1 gate review. Principle: Front\u2011load research & hardware iteration . Build on IBM Quantum free\u2011tier devices until we have an attractive, validated, and patentable measurement stack. Only then open Early Access for design partners. This roadmap folds in: (i) a sophisticated classical shadows program, (ii) concrete experiments & tests mapped to each phase, and (iii) clear publication/patent gates before external onboarding. Glossary (metrics & terms) \u00b6 SSR (Shot\u2011Savings Ratio): shot\u2011count (baseline) \u00f7 shot\u2011count (QuartumSE) at equal error tolerance. RMSE@$: cost\u2011for\u2011accuracy \u2014 dollars (or credits/time) to reach a target RMSE on an observable/metric. CI coverage: frequency a 95% CI contains ground truth (simulation) or gold standard (hardware cross\u2011checks). Provenance Manifest: JSON artifact capturing circuits, calibrations, mitigations, backend, seeds, versions. MEM / M3: measurement error mitigation (confusion matrices); ZNE: zero\u2011noise extrapolation. PEC: probabilistic error cancellation; RC: randomized compiling. Program Structure at a Glance \u00b6 Workstream S (Shadows): Classical Shadows Engine v0\u2192v4 (baseline \u2192 noise\u2011aware \u2192 fermionic \u2192 adaptive/derandomized \u2192 robust Bayesian/bootstrapped). Workstream C (Chemistry/VQE): Shadow\u2011VQE for small molecules (H\u2082, LiH, BeH\u2082). Workstream O (Optimization/QAOA): Shot\u2011frugal QAOA on MAX\u2011CUT & MIS toy instances. Workstream B (Benchmarking): RB/XEB/Quantum\u2011Volume + Shadow\u2011Benchmarking (fidelity/entropy/purity via shadows). Workstream M (Metrology): Variational entangled probes (GHZ/W states) for phase\u2011sensing toy tasks. Workstream P (Provenance & Reporting): Manifest schema, CI pipelines, PDF/HTML reports, reproducibility notebooks. Each phase below enumerates Experiments & Tests with IDs that recur across phases for iteration & scaling. Operational cadence checkpoints \u00b6 Monthly (first business day): Run quartumse runtime-status --json --backend ibm:ibmq_brisbane --instance ibm-q/open/main and log runtime minutes, queue caps, and fallback readiness in OPS_RUNTIME_RUNBOOK.md . Schedule a recurring calendar reminder for the ops lead. Weekly (Mondays): Trigger the runtime status CLI with Slack webhook enabled to post queue depth/quota snapshots into the project notifications channel. Use the summary to reprioritise hardware jobs if the queue is saturated. Phase 1 \u2014 Foundation & R&D Sprints (Now \u2192 Nov 2025) \u00b6 Focus: Ship scaffolding and start real algorithmic experiments immediately (sim + small IBM jobs). Objectives \u00b6 Solidify repository, CI/CD, SDK skeleton, provenance/reporting. Implement Shadows v0 (random local Clifford) + v1 (noise\u2011aware inverse\u2011channel + MEM). Stand up baseline C , O , B , M toy pipelines against Aer simulator and at least one IBM free\u2011tier backend. Deliver a tractable test suite and benchmarking harness (pytest + notebooks). Deliverables \u00b6 SDK modules: Estimator , Shadows , Report ; Provenance Manifest v1 ; quickstart notebook. Mitigation core: MEM (M3) (production) and ZNE scaffolding; PEC/RC hooks planned post-Phase 1. Shadows v0\u2013v1 reference implementation with CI. Test harness: datasets, seeds, fixtures; storage: Parquet/DuckDB; PDF/HTML report. Internal whiteboard spec for patent themes (see Phase 2 gate). Experiments & Tests (P1) \u00b6 S\u2011T01 (Shadows\u2011Core): Random local Clifford shadows on GHZ(3\u20135), Bell pairs; estimate \u27e8Z\u1d62\u27e9, \u27e8Z\u1d62Z\u2c7c\u27e9, purity. Targets: CI coverage \u2265 0.9; SSR \u2265 1.2 on sim, \u2265 1.1 on IBM. S\u2011T02 (Noise\u2011Aware): Calibrate per\u2011qubit inverse channel; compare with/without MEM; compute variance reduction. C\u2011T01 (H\u2082@STO\u20113G): Hardware\u2011efficient VQE (depth \u2264 2) + Shadows readout of Hamiltonian terms; energy error \u2264 50 mHa (sim), \u2264 80 mHa (IBM). O\u2011T01 (MAX\u2011CUT\u20115): QAOA p\u2208{1,2} on 5\u2011node ring; shot\u2011frugal optimizer; compare cost estimate variance with/without Shadows proxy. B\u2011T01 (RB/XEB): 1\u20133 qubit RB; XEB on depth\u2011limited random circuits; log into Manifest; compare to IBM backend calibration metadata. M\u2011T01 (GHZ\u2011Phase): Prepare GHZ(3\u20134), encode small Z\u2011phase, estimate via optimal readout; CI coverage \u2265 0.8 on sim; explore ZNE for readout bias. Exit / Success Criteria \u00b6 End\u2011to\u2011end run from notebook \u2192 manifest \u2192 report on Aer + at least one IBM free\u2011tier backend. SSR \u2265 1.2\u00d7 on Shadows\u2011Core (sim) and \u2265 1.1\u00d7 (IBM). CI coverage \u2265 80%, zero critical issues, reproducible seeds & manifests. Patent themes shortlist (top\u20113) + experiment data to support novelty. Phase 2 \u2014 Hardware\u2011First Iteration & Patent Drafts (Nov \u2192 Dec 2025) \u00b6 Focus: Iterate on hardware . Elevate shadows & domain demos; lock initial patent filings; prep first papers. Objectives \u00b6 Implement Shadows v2 (Fermionic) for 2\u2011RDM estimation; integrate with VQE readout. Prototype Shadows v3 (Adaptive/Derandomized) : choose measurement ensembles to minimize estimator variance given target observable set. Harden error mitigation combinations ( MEM + RC + ZNE ) with ablation studies. Run structured hardware campaigns (blocked time windows) to control drift. Deliverables \u00b6 IBM hardware campaign #1 dataset + full manifests + PDF/HTML reports. Provisional patent draft(s) for: Variance\u2011Aware Adaptive Classical Shadows (VACS) ; Shadow\u2011VQE readout integration; Shadow\u2011Benchmarking workflow. Two arXiv preprints : (i) Shadows engine on IBM, (ii) Shadow\u2011VQE for H\u2082/LiH small\u2011basis. Updated SDK APIs (stabilize experimental flags), plus \u201creplay from manifest\u201d tooling. Experiments & Tests (P2) \u00b6 S\u2011T03 (Fermionic\u2011Shadows): Direct 2\u2011RDM from shadows; H\u2082/LiH energies within 40\u201360 mHa on IBM at \u2264 baseline shots; SSR \u2265 1.3\u00d7 (IBM). S\u2011T04 (Adaptive/Derand): Greedy/importance\u2011sampled basis selection vs plain random; measure variance \u2193 \u2265 25% for fixed shots (IBM). C\u2011T02 (LiH@Minimal): VQE with Shadow\u2011readout vs grouped\u2011Pauli readout; RMSE@$ \u2193 by \u2265 30% at matched error bars. O\u2011T02 (MAX\u2011CUT\u20116/7): Depth\u2011aware layout + RC; shot\u2011allocation per\u2011iteration; track optimizer steps saved vs fixed\u2011shot budget. B\u2011T02 (Shadow\u2011Benchmarking): Estimate linear entropy, multi\u2011qubit purities, and fidelity to GHZ using the same shadows dataset; compare to direct methods; sample\u2011efficiency \u2265 2\u00d7 . M\u2011T02 (Variational\u2011Metrology): Variational state+measurement co\u2011optimization for phase sensing; demonstrate > classical shot\u2011noise scaling on sim, and robust advantage trend on IBM within CI. Exit / Success Criteria (Gate to P3) \u00b6 SSR \u2265 1.3\u00d7 on IBM for at least one domain test (Shadows\u2011Core or Fermionic\u2011Shadows). Draft provisional patent(s) filed ; arXiv preprints ready. CI artifacts: reproducible notebooks, manifests, reports for all P2 tests. Phase 3 \u2014 Internal Validation & Publication/Patent Gate (Jan \u2192 Mar 2026) \u00b6 Focus: Consolidate results; conduct controlled comparisons; submit publications; finalize patents. No external users yet. Objectives \u00b6 Build automated benchmark suite : GHZ, VQE(H\u2082, LiH, BeH\u2082), QAOA(MAX\u2011CUT\u2011k), Shadow\u2011Benchmarking panels. Statistical validation: SSR , RMSE@$ , CI coverage , reproducibility (< 2% drift under re\u2011runs). Implement Shadows v4 (Robust/Bayesian) : bootstrap CI, variance debiasing, heteroscedastic weighting by device cal data. Prepare journal submissions (PRX Quantum/npjQI/Quantum) and non\u2011provisional patent filings . Deliverables \u00b6 Benchmark suite (pytest + CLI) with per\u2011test Manifest templates and reporting. Internal whitepaper and slide deck with full ablation matrices. Code\u2011frozen R&D branch tagged for archival reproducibility (DOI/Zenodo). Experiments & Tests (P3) \u00b6 S\u2011T05 (Robust\u2011Shadows): Bootstrap CI coverage \u2265 0.9 on sim and \u2265 0.85 on IBM across GHZ and small\u2011chemistry states. C\u2011T03 (BeH\u2082@Minimal): Shadow\u2011VQE energy within 80\u2013100 mHa on IBM; RMSE@$ \u2193 \u2265 35% vs grouped\u2011Pauli baseline. O\u2011T03 (MAX\u2011CUT\u20117) and O\u2011T04 (MIS\u20116): Evaluate solution quality vs shots; show optimizer steps \u2193 \u2265 20% using shot\u2011frugal and variance\u2011aware estimates. B\u2011T03 (Cross\u2011Provider Sim): Aer vs IBM reproducibility; manifest \u201creplay\u201d round\u2011trip equality. M\u2011T03 (Sensor\u2011Tuning): Variational probe robustness to readout noise; CI width \u2193 15\u201325% after robust shadows weighting. Exit / Success Criteria (Gate to Early Access) \u00b6 SSR \u2265 1.5\u00d7 achieved on internal benchmarks; RMSE@$ consistently better than baselines. At least one paper accepted (or under strong revise\u2011&\u2011resubmit) and patents filed . Provenance & replay validated; CI green across full suite. Repository made public : Audit Git history for secrets, make repo public to enable external contributions. CI matrix expanded : Restore full cross-platform testing (12 jobs: 3 OSes \u00d7 4 Python versions). See docs/ops/ci_expansion_guide.md . Only once the above gates are cleared do we begin external onboarding. Phase 4 \u2014 Early Access (Design Partners) & Multi\u2011Provider Expansion (Apr \u2192 Jun 2026) \u00b6 Focus: Limited Early Access after patents/papers. Add AWS Braket connector. Gather external evidence on real workloads. Objectives \u00b6 Onboard 2\u20133 design partners (academia/industry) with NDAs referencing filed IP. Implement AWS Braket connector ; cross\u2011provider parity and consistency tests. Partner\u2011coauthored case studies; feedback loop into APIs & docs. Deliverables \u00b6 Partner playbooks; onboarding notebooks; Slack/Discord channels. Cross\u2011provider tests: same circuits on IBM vs AWS; delta analysis reported. Case study draft(s) + testimonial(s). Experiments & Tests (P4) \u00b6 B\u2011T04 (Cross\u2011Provider Parity): Within 10% agreement on observables post\u2011mitigation across IBM/AWS for GHZ and VQE(H\u2082) tasks. C\u2011T04 (Partner\u2011Chemistry): Run partner\u2011provided small chemistry model; maintain SSR \u2265 1.5\u00d7 . O\u2011T05 (Partner\u2011Optimization): QAOA on a partner toy instance; capture wall\u2011clock + cost deltas in RMSE@$. S\u2011T06 (Partner\u2011Shadows): Validate adaptive shadows on partner circuits; document any domain\u2011specific gains. Exit / Success Criteria \u00b6 \u22653 partners actively running; parity across providers; partner satisfaction survey \u2265 8/10. External replication of SSR \u2265 1.5\u00d7 ; stable APIs for public beta drafting. Phase 5 \u2014 Public Beta & Pilot Conversion (Jul \u2192 Sep 2026) \u00b6 Focus: Stabilize and open up. Convert Early Access into pilots. Prepare commercial posture. Objectives \u00b6 Public Beta (v1.0) on PyPI + GitHub; full docs; examples; tutorials. Secure 2\u20133 pilot customers/LOIs; webinar/demo using published results. Verify SSR \u2265 2.0\u00d7 in multi\u2011provider benchmarks; publish follow\u2011ups. Deliverables \u00b6 v1.0 release; docs portal; community channels; issue triage. Pilot SOW templates; pricing experiments around RMSE@$ value metric. Public benchmark report with manifests for community reproduction. Experiments & Tests (P5) \u00b6 B\u2011T05 (Public Benchmarks): Community\u2011reproducible GHZ/VQE/QAOA panels with manifests and reference CI. C\u2011T05 (Chemistry\u2011Scale\u2011Up): Largest feasible molecule instance on accessible hardware; publish shot & cost curves. S\u2011T07 (Shadows\u2011Ablation Public): Public ablation notebook isolating contributions from v0\u2192v4 components. KPIs \u00b6 \u22655 orgs using (3 design partners + \u22652 pilots); \u22652 paying/committed customers. Verified SSR \u2265 2.0\u00d7 on multi\u2011provider suite; community replications reported. Algorithm & Test Matrix (IDs referenced above) \u00b6 ID Category Technique Circuits / Instances Backends Primary Metrics Evidence Artifacts S\u2011T01 Shadows v0 (random local Clifford) Bell, GHZ(3\u20135) Aer, IBM free\u2011tier SSR, CI coverage Manifest, notebook, PDF S\u2011T02 Shadows v1 (noise\u2011aware + MEM) As above Aer, IBM Var. reduction, bias Manifest, ablation table S\u2011T03 Shadows v2 (fermionic) H\u2082/LiH 2\u2011RDM Aer, IBM Energy error, SSR Manifest, data parquet S\u2011T04 Shadows v3 (adaptive/derand) Target Pauli sets Aer, IBM Variance \u2193 Manifest, policy snapshot S\u2011T05 Shadows v4 (robust/Bayesian) GHZ + chemistry Aer, IBM CI coverage, width Manifest, bootstrap logs C\u2011T01 Chemistry VQE + Shadow readout H\u2082@STO\u20113G Aer, IBM Energy error, RMSE@$ Manifest, report C\u2011T02 Chemistry Shadow\u2011VQE vs grouped LiH@minimal Aer, IBM RMSE@$ \u2193 Notebook, plot C\u2011T03 Chemistry Scale\u2011up BeH\u2082@minimal Aer, IBM Energy error, SSR Manifest, report C\u2011T04 Chemistry Partner task Partner circuit IBM/AWS SSR \u22651.5\u00d7 Case study C\u2011T05 Chemistry Public benchmark Largest feasible IBM/AWS Shot & cost curves Public repo O\u2011T01 Optimization QAOA p\u22642 MAX\u2011CUT\u20115 Aer, IBM Cost var., steps Manifest, runtime logs O\u2011T02 Optimization Shot\u2011frugal + RC MAX\u2011CUT\u20116/7 IBM Steps \u2193, RMSE@$ Report O\u2011T03 Optimization MIS\u20116 MIS\u20116 IBM Quality vs shots Manifest O\u2011T04 Optimization MAX\u2011CUT\u20117 MAX\u2011CUT\u20117 IBM Steps \u2193 Logs O\u2011T05 Optimization Partner task Partner graph IBM/AWS RMSE@$ Case study B\u2011T01 Benchmark RB/XEB 1\u20133q RB; XEB IBM Gate error trends Manifest, plots B\u2011T02 Benchmark Shadow\u2011Benchmarking Purity, entropy, GHZ\u2011Fid IBM Sample\u2011efficiency Report B\u2011T03 Benchmark Reproducibility Aer vs IBM Aer/IBM Replay fidelity Replay manifests B\u2011T04 Benchmark Cross\u2011provider IBM vs AWS IBM/AWS Parity Report B\u2011T05 Benchmark Public suite Community panels All Replications Public repo M\u2011T01 Metrology GHZ phase toy GHZ(3\u20134) Aer/IBM CI cov. Manifest M\u2011T02 Metrology Variational GHZ/W Aer/IBM Advantage trend Logs M\u2011T03 Metrology Robust probes GHZ k\u2011qubit Aer/IBM CI width \u2193 Report Governance, Gating & Quality \u00b6 Gates: P2\u2192P3 requires IBM SSR \u2265 1.3\u00d7 ; P3\u2192P4 requires SSR \u2265 1.5\u00d7 + patent(s) filed + \u22651 paper accepted/near\u2011accept; P4\u2192P5 requires cross\u2011provider parity and partner replication of SSR. Reproducibility: Every result ships with a Manifest , raw parquet shot data, and a replay notebook. Risk controls: Time\u2011boxed hardware campaigns; ablations to isolate mitigation effects; fail\u2011closed CI (no release if tests or coverage gates fail). Timeline (target) \u00b6 Phase Focus Key Milestone Target Date P1 Foundation & R&D sprints First IBM runs + Shadows v1 Nov 2025 P2 Hardware\u2011first iteration IBM campaign #1 + provisional patents + preprints Dec 2025 P3 Internal validation SSR \u22651.5\u00d7 + journal submits + non\u2011provisionals Mar 2026 P4 Early Access & expansion 2\u20133 partners + AWS parity Jun 2026 P5 Public Beta v1.0 + pilots Sep 2026 Notes for Engineering \u00b6 Keep device\u2011agnostic connectors ; IBM first, AWS next. Prefer layout\u2011aware, shallow circuits ; re\u2011seed experiments; record cal snapshots. Implement manifest replay (offline) as a first\u2011class command; integrate with CI. Publish small, frequent preprints; convert to journals as results mature.","title":"Roadmap"},{"location":"strategy/roadmap/#quartumse-rdcentric-roadmap-updated-20252026","text":"Last updated: 2025-10-24","title":"QuartumSE R&amp;D\u2011Centric Roadmap (Updated, 2025\u20132026)"},{"location":"strategy/roadmap/#phase-snapshot-oct-2025","text":"\u2705 Phase 1 scaffolding, provenance pipeline, and CI harness are live. \u2705 S\u2011T01 GHZ baseline + S\u2011T02 noise-aware runs validated on IBM ibm_torino (smoke test Oct 22, 2025). \u2705 IBM Runtime CLI ( quartumse runtime-status ) operational with webhook notifications. \u26a0\ufe0f Extended IBM hardware validation (target SSR \u2265 1.1 across repeated runs) scheduled for Nov 2025. \u26a0\ufe0f Cross-workstream starter experiments (C/O/B/M) need first data drops before Phase 1 closes. \ud83d\udcdd Patent theme shortlist drafting in progress ahead of the Phase 2 gate review. \ud83d\udccb See phase1_task_checklist.md for the consolidated execution checklist that enumerates every outstanding task before the Phase 1 gate review. Principle: Front\u2011load research & hardware iteration . Build on IBM Quantum free\u2011tier devices until we have an attractive, validated, and patentable measurement stack. Only then open Early Access for design partners. This roadmap folds in: (i) a sophisticated classical shadows program, (ii) concrete experiments & tests mapped to each phase, and (iii) clear publication/patent gates before external onboarding.","title":"Phase snapshot (Oct 2025)"},{"location":"strategy/roadmap/#glossary-metrics-terms","text":"SSR (Shot\u2011Savings Ratio): shot\u2011count (baseline) \u00f7 shot\u2011count (QuartumSE) at equal error tolerance. RMSE@$: cost\u2011for\u2011accuracy \u2014 dollars (or credits/time) to reach a target RMSE on an observable/metric. CI coverage: frequency a 95% CI contains ground truth (simulation) or gold standard (hardware cross\u2011checks). Provenance Manifest: JSON artifact capturing circuits, calibrations, mitigations, backend, seeds, versions. MEM / M3: measurement error mitigation (confusion matrices); ZNE: zero\u2011noise extrapolation. PEC: probabilistic error cancellation; RC: randomized compiling.","title":"Glossary (metrics &amp; terms)"},{"location":"strategy/roadmap/#program-structure-at-a-glance","text":"Workstream S (Shadows): Classical Shadows Engine v0\u2192v4 (baseline \u2192 noise\u2011aware \u2192 fermionic \u2192 adaptive/derandomized \u2192 robust Bayesian/bootstrapped). Workstream C (Chemistry/VQE): Shadow\u2011VQE for small molecules (H\u2082, LiH, BeH\u2082). Workstream O (Optimization/QAOA): Shot\u2011frugal QAOA on MAX\u2011CUT & MIS toy instances. Workstream B (Benchmarking): RB/XEB/Quantum\u2011Volume + Shadow\u2011Benchmarking (fidelity/entropy/purity via shadows). Workstream M (Metrology): Variational entangled probes (GHZ/W states) for phase\u2011sensing toy tasks. Workstream P (Provenance & Reporting): Manifest schema, CI pipelines, PDF/HTML reports, reproducibility notebooks. Each phase below enumerates Experiments & Tests with IDs that recur across phases for iteration & scaling.","title":"Program Structure at a Glance"},{"location":"strategy/roadmap/#operational-cadence-checkpoints","text":"Monthly (first business day): Run quartumse runtime-status --json --backend ibm:ibmq_brisbane --instance ibm-q/open/main and log runtime minutes, queue caps, and fallback readiness in OPS_RUNTIME_RUNBOOK.md . Schedule a recurring calendar reminder for the ops lead. Weekly (Mondays): Trigger the runtime status CLI with Slack webhook enabled to post queue depth/quota snapshots into the project notifications channel. Use the summary to reprioritise hardware jobs if the queue is saturated.","title":"Operational cadence checkpoints"},{"location":"strategy/roadmap/#phase-1-foundation-rd-sprints-now-nov-2025","text":"Focus: Ship scaffolding and start real algorithmic experiments immediately (sim + small IBM jobs).","title":"Phase 1 \u2014 Foundation &amp; R&amp;D Sprints (Now \u2192 Nov 2025)"},{"location":"strategy/roadmap/#objectives","text":"Solidify repository, CI/CD, SDK skeleton, provenance/reporting. Implement Shadows v0 (random local Clifford) + v1 (noise\u2011aware inverse\u2011channel + MEM). Stand up baseline C , O , B , M toy pipelines against Aer simulator and at least one IBM free\u2011tier backend. Deliver a tractable test suite and benchmarking harness (pytest + notebooks).","title":"Objectives"},{"location":"strategy/roadmap/#deliverables","text":"SDK modules: Estimator , Shadows , Report ; Provenance Manifest v1 ; quickstart notebook. Mitigation core: MEM (M3) (production) and ZNE scaffolding; PEC/RC hooks planned post-Phase 1. Shadows v0\u2013v1 reference implementation with CI. Test harness: datasets, seeds, fixtures; storage: Parquet/DuckDB; PDF/HTML report. Internal whiteboard spec for patent themes (see Phase 2 gate).","title":"Deliverables"},{"location":"strategy/roadmap/#experiments-tests-p1","text":"S\u2011T01 (Shadows\u2011Core): Random local Clifford shadows on GHZ(3\u20135), Bell pairs; estimate \u27e8Z\u1d62\u27e9, \u27e8Z\u1d62Z\u2c7c\u27e9, purity. Targets: CI coverage \u2265 0.9; SSR \u2265 1.2 on sim, \u2265 1.1 on IBM. S\u2011T02 (Noise\u2011Aware): Calibrate per\u2011qubit inverse channel; compare with/without MEM; compute variance reduction. C\u2011T01 (H\u2082@STO\u20113G): Hardware\u2011efficient VQE (depth \u2264 2) + Shadows readout of Hamiltonian terms; energy error \u2264 50 mHa (sim), \u2264 80 mHa (IBM). O\u2011T01 (MAX\u2011CUT\u20115): QAOA p\u2208{1,2} on 5\u2011node ring; shot\u2011frugal optimizer; compare cost estimate variance with/without Shadows proxy. B\u2011T01 (RB/XEB): 1\u20133 qubit RB; XEB on depth\u2011limited random circuits; log into Manifest; compare to IBM backend calibration metadata. M\u2011T01 (GHZ\u2011Phase): Prepare GHZ(3\u20134), encode small Z\u2011phase, estimate via optimal readout; CI coverage \u2265 0.8 on sim; explore ZNE for readout bias.","title":"Experiments &amp; Tests (P1)"},{"location":"strategy/roadmap/#exit-success-criteria","text":"End\u2011to\u2011end run from notebook \u2192 manifest \u2192 report on Aer + at least one IBM free\u2011tier backend. SSR \u2265 1.2\u00d7 on Shadows\u2011Core (sim) and \u2265 1.1\u00d7 (IBM). CI coverage \u2265 80%, zero critical issues, reproducible seeds & manifests. Patent themes shortlist (top\u20113) + experiment data to support novelty.","title":"Exit / Success Criteria"},{"location":"strategy/roadmap/#phase-2-hardwarefirst-iteration-patent-drafts-nov-dec-2025","text":"Focus: Iterate on hardware . Elevate shadows & domain demos; lock initial patent filings; prep first papers.","title":"Phase 2 \u2014 Hardware\u2011First Iteration &amp; Patent Drafts (Nov \u2192 Dec 2025)"},{"location":"strategy/roadmap/#objectives_1","text":"Implement Shadows v2 (Fermionic) for 2\u2011RDM estimation; integrate with VQE readout. Prototype Shadows v3 (Adaptive/Derandomized) : choose measurement ensembles to minimize estimator variance given target observable set. Harden error mitigation combinations ( MEM + RC + ZNE ) with ablation studies. Run structured hardware campaigns (blocked time windows) to control drift.","title":"Objectives"},{"location":"strategy/roadmap/#deliverables_1","text":"IBM hardware campaign #1 dataset + full manifests + PDF/HTML reports. Provisional patent draft(s) for: Variance\u2011Aware Adaptive Classical Shadows (VACS) ; Shadow\u2011VQE readout integration; Shadow\u2011Benchmarking workflow. Two arXiv preprints : (i) Shadows engine on IBM, (ii) Shadow\u2011VQE for H\u2082/LiH small\u2011basis. Updated SDK APIs (stabilize experimental flags), plus \u201creplay from manifest\u201d tooling.","title":"Deliverables"},{"location":"strategy/roadmap/#experiments-tests-p2","text":"S\u2011T03 (Fermionic\u2011Shadows): Direct 2\u2011RDM from shadows; H\u2082/LiH energies within 40\u201360 mHa on IBM at \u2264 baseline shots; SSR \u2265 1.3\u00d7 (IBM). S\u2011T04 (Adaptive/Derand): Greedy/importance\u2011sampled basis selection vs plain random; measure variance \u2193 \u2265 25% for fixed shots (IBM). C\u2011T02 (LiH@Minimal): VQE with Shadow\u2011readout vs grouped\u2011Pauli readout; RMSE@$ \u2193 by \u2265 30% at matched error bars. O\u2011T02 (MAX\u2011CUT\u20116/7): Depth\u2011aware layout + RC; shot\u2011allocation per\u2011iteration; track optimizer steps saved vs fixed\u2011shot budget. B\u2011T02 (Shadow\u2011Benchmarking): Estimate linear entropy, multi\u2011qubit purities, and fidelity to GHZ using the same shadows dataset; compare to direct methods; sample\u2011efficiency \u2265 2\u00d7 . M\u2011T02 (Variational\u2011Metrology): Variational state+measurement co\u2011optimization for phase sensing; demonstrate > classical shot\u2011noise scaling on sim, and robust advantage trend on IBM within CI.","title":"Experiments &amp; Tests (P2)"},{"location":"strategy/roadmap/#exit-success-criteria-gate-to-p3","text":"SSR \u2265 1.3\u00d7 on IBM for at least one domain test (Shadows\u2011Core or Fermionic\u2011Shadows). Draft provisional patent(s) filed ; arXiv preprints ready. CI artifacts: reproducible notebooks, manifests, reports for all P2 tests.","title":"Exit / Success Criteria (Gate to P3)"},{"location":"strategy/roadmap/#phase-3-internal-validation-publicationpatent-gate-jan-mar-2026","text":"Focus: Consolidate results; conduct controlled comparisons; submit publications; finalize patents. No external users yet.","title":"Phase 3 \u2014 Internal Validation &amp; Publication/Patent Gate (Jan \u2192 Mar 2026)"},{"location":"strategy/roadmap/#objectives_2","text":"Build automated benchmark suite : GHZ, VQE(H\u2082, LiH, BeH\u2082), QAOA(MAX\u2011CUT\u2011k), Shadow\u2011Benchmarking panels. Statistical validation: SSR , RMSE@$ , CI coverage , reproducibility (< 2% drift under re\u2011runs). Implement Shadows v4 (Robust/Bayesian) : bootstrap CI, variance debiasing, heteroscedastic weighting by device cal data. Prepare journal submissions (PRX Quantum/npjQI/Quantum) and non\u2011provisional patent filings .","title":"Objectives"},{"location":"strategy/roadmap/#deliverables_2","text":"Benchmark suite (pytest + CLI) with per\u2011test Manifest templates and reporting. Internal whitepaper and slide deck with full ablation matrices. Code\u2011frozen R&D branch tagged for archival reproducibility (DOI/Zenodo).","title":"Deliverables"},{"location":"strategy/roadmap/#experiments-tests-p3","text":"S\u2011T05 (Robust\u2011Shadows): Bootstrap CI coverage \u2265 0.9 on sim and \u2265 0.85 on IBM across GHZ and small\u2011chemistry states. C\u2011T03 (BeH\u2082@Minimal): Shadow\u2011VQE energy within 80\u2013100 mHa on IBM; RMSE@$ \u2193 \u2265 35% vs grouped\u2011Pauli baseline. O\u2011T03 (MAX\u2011CUT\u20117) and O\u2011T04 (MIS\u20116): Evaluate solution quality vs shots; show optimizer steps \u2193 \u2265 20% using shot\u2011frugal and variance\u2011aware estimates. B\u2011T03 (Cross\u2011Provider Sim): Aer vs IBM reproducibility; manifest \u201creplay\u201d round\u2011trip equality. M\u2011T03 (Sensor\u2011Tuning): Variational probe robustness to readout noise; CI width \u2193 15\u201325% after robust shadows weighting.","title":"Experiments &amp; Tests (P3)"},{"location":"strategy/roadmap/#exit-success-criteria-gate-to-early-access","text":"SSR \u2265 1.5\u00d7 achieved on internal benchmarks; RMSE@$ consistently better than baselines. At least one paper accepted (or under strong revise\u2011&\u2011resubmit) and patents filed . Provenance & replay validated; CI green across full suite. Repository made public : Audit Git history for secrets, make repo public to enable external contributions. CI matrix expanded : Restore full cross-platform testing (12 jobs: 3 OSes \u00d7 4 Python versions). See docs/ops/ci_expansion_guide.md . Only once the above gates are cleared do we begin external onboarding.","title":"Exit / Success Criteria (Gate to Early Access)"},{"location":"strategy/roadmap/#phase-4-early-access-design-partners-multiprovider-expansion-apr-jun-2026","text":"Focus: Limited Early Access after patents/papers. Add AWS Braket connector. Gather external evidence on real workloads.","title":"Phase 4 \u2014 Early Access (Design Partners) &amp; Multi\u2011Provider Expansion (Apr \u2192 Jun 2026)"},{"location":"strategy/roadmap/#objectives_3","text":"Onboard 2\u20133 design partners (academia/industry) with NDAs referencing filed IP. Implement AWS Braket connector ; cross\u2011provider parity and consistency tests. Partner\u2011coauthored case studies; feedback loop into APIs & docs.","title":"Objectives"},{"location":"strategy/roadmap/#deliverables_3","text":"Partner playbooks; onboarding notebooks; Slack/Discord channels. Cross\u2011provider tests: same circuits on IBM vs AWS; delta analysis reported. Case study draft(s) + testimonial(s).","title":"Deliverables"},{"location":"strategy/roadmap/#experiments-tests-p4","text":"B\u2011T04 (Cross\u2011Provider Parity): Within 10% agreement on observables post\u2011mitigation across IBM/AWS for GHZ and VQE(H\u2082) tasks. C\u2011T04 (Partner\u2011Chemistry): Run partner\u2011provided small chemistry model; maintain SSR \u2265 1.5\u00d7 . O\u2011T05 (Partner\u2011Optimization): QAOA on a partner toy instance; capture wall\u2011clock + cost deltas in RMSE@$. S\u2011T06 (Partner\u2011Shadows): Validate adaptive shadows on partner circuits; document any domain\u2011specific gains.","title":"Experiments &amp; Tests (P4)"},{"location":"strategy/roadmap/#exit-success-criteria_1","text":"\u22653 partners actively running; parity across providers; partner satisfaction survey \u2265 8/10. External replication of SSR \u2265 1.5\u00d7 ; stable APIs for public beta drafting.","title":"Exit / Success Criteria"},{"location":"strategy/roadmap/#phase-5-public-beta-pilot-conversion-jul-sep-2026","text":"Focus: Stabilize and open up. Convert Early Access into pilots. Prepare commercial posture.","title":"Phase 5 \u2014 Public Beta &amp; Pilot Conversion (Jul \u2192 Sep 2026)"},{"location":"strategy/roadmap/#objectives_4","text":"Public Beta (v1.0) on PyPI + GitHub; full docs; examples; tutorials. Secure 2\u20133 pilot customers/LOIs; webinar/demo using published results. Verify SSR \u2265 2.0\u00d7 in multi\u2011provider benchmarks; publish follow\u2011ups.","title":"Objectives"},{"location":"strategy/roadmap/#deliverables_4","text":"v1.0 release; docs portal; community channels; issue triage. Pilot SOW templates; pricing experiments around RMSE@$ value metric. Public benchmark report with manifests for community reproduction.","title":"Deliverables"},{"location":"strategy/roadmap/#experiments-tests-p5","text":"B\u2011T05 (Public Benchmarks): Community\u2011reproducible GHZ/VQE/QAOA panels with manifests and reference CI. C\u2011T05 (Chemistry\u2011Scale\u2011Up): Largest feasible molecule instance on accessible hardware; publish shot & cost curves. S\u2011T07 (Shadows\u2011Ablation Public): Public ablation notebook isolating contributions from v0\u2192v4 components.","title":"Experiments &amp; Tests (P5)"},{"location":"strategy/roadmap/#kpis","text":"\u22655 orgs using (3 design partners + \u22652 pilots); \u22652 paying/committed customers. Verified SSR \u2265 2.0\u00d7 on multi\u2011provider suite; community replications reported.","title":"KPIs"},{"location":"strategy/roadmap/#algorithm-test-matrix-ids-referenced-above","text":"ID Category Technique Circuits / Instances Backends Primary Metrics Evidence Artifacts S\u2011T01 Shadows v0 (random local Clifford) Bell, GHZ(3\u20135) Aer, IBM free\u2011tier SSR, CI coverage Manifest, notebook, PDF S\u2011T02 Shadows v1 (noise\u2011aware + MEM) As above Aer, IBM Var. reduction, bias Manifest, ablation table S\u2011T03 Shadows v2 (fermionic) H\u2082/LiH 2\u2011RDM Aer, IBM Energy error, SSR Manifest, data parquet S\u2011T04 Shadows v3 (adaptive/derand) Target Pauli sets Aer, IBM Variance \u2193 Manifest, policy snapshot S\u2011T05 Shadows v4 (robust/Bayesian) GHZ + chemistry Aer, IBM CI coverage, width Manifest, bootstrap logs C\u2011T01 Chemistry VQE + Shadow readout H\u2082@STO\u20113G Aer, IBM Energy error, RMSE@$ Manifest, report C\u2011T02 Chemistry Shadow\u2011VQE vs grouped LiH@minimal Aer, IBM RMSE@$ \u2193 Notebook, plot C\u2011T03 Chemistry Scale\u2011up BeH\u2082@minimal Aer, IBM Energy error, SSR Manifest, report C\u2011T04 Chemistry Partner task Partner circuit IBM/AWS SSR \u22651.5\u00d7 Case study C\u2011T05 Chemistry Public benchmark Largest feasible IBM/AWS Shot & cost curves Public repo O\u2011T01 Optimization QAOA p\u22642 MAX\u2011CUT\u20115 Aer, IBM Cost var., steps Manifest, runtime logs O\u2011T02 Optimization Shot\u2011frugal + RC MAX\u2011CUT\u20116/7 IBM Steps \u2193, RMSE@$ Report O\u2011T03 Optimization MIS\u20116 MIS\u20116 IBM Quality vs shots Manifest O\u2011T04 Optimization MAX\u2011CUT\u20117 MAX\u2011CUT\u20117 IBM Steps \u2193 Logs O\u2011T05 Optimization Partner task Partner graph IBM/AWS RMSE@$ Case study B\u2011T01 Benchmark RB/XEB 1\u20133q RB; XEB IBM Gate error trends Manifest, plots B\u2011T02 Benchmark Shadow\u2011Benchmarking Purity, entropy, GHZ\u2011Fid IBM Sample\u2011efficiency Report B\u2011T03 Benchmark Reproducibility Aer vs IBM Aer/IBM Replay fidelity Replay manifests B\u2011T04 Benchmark Cross\u2011provider IBM vs AWS IBM/AWS Parity Report B\u2011T05 Benchmark Public suite Community panels All Replications Public repo M\u2011T01 Metrology GHZ phase toy GHZ(3\u20134) Aer/IBM CI cov. Manifest M\u2011T02 Metrology Variational GHZ/W Aer/IBM Advantage trend Logs M\u2011T03 Metrology Robust probes GHZ k\u2011qubit Aer/IBM CI width \u2193 Report","title":"Algorithm &amp; Test Matrix (IDs referenced above)"},{"location":"strategy/roadmap/#governance-gating-quality","text":"Gates: P2\u2192P3 requires IBM SSR \u2265 1.3\u00d7 ; P3\u2192P4 requires SSR \u2265 1.5\u00d7 + patent(s) filed + \u22651 paper accepted/near\u2011accept; P4\u2192P5 requires cross\u2011provider parity and partner replication of SSR. Reproducibility: Every result ships with a Manifest , raw parquet shot data, and a replay notebook. Risk controls: Time\u2011boxed hardware campaigns; ablations to isolate mitigation effects; fail\u2011closed CI (no release if tests or coverage gates fail).","title":"Governance, Gating &amp; Quality"},{"location":"strategy/roadmap/#timeline-target","text":"Phase Focus Key Milestone Target Date P1 Foundation & R&D sprints First IBM runs + Shadows v1 Nov 2025 P2 Hardware\u2011first iteration IBM campaign #1 + provisional patents + preprints Dec 2025 P3 Internal validation SSR \u22651.5\u00d7 + journal submits + non\u2011provisionals Mar 2026 P4 Early Access & expansion 2\u20133 partners + AWS parity Jun 2026 P5 Public Beta v1.0 + pilots Sep 2026","title":"Timeline (target)"},{"location":"strategy/roadmap/#notes-for-engineering","text":"Keep device\u2011agnostic connectors ; IBM first, AWS next. Prefer layout\u2011aware, shallow circuits ; re\u2011seed experiments; record cal snapshots. Implement manifest replay (offline) as a first\u2011class command; integrate with CI. Publish small, frequent preprints; convert to journals as results mature.","title":"Notes for Engineering"},{"location":"strategy/runtime_budgeting_checklist/","text":"Runtime Budgeting Checklist Template \u00b6 Use this checklist before launching any IBM Quantum workload. Populate it with quartumse runtime-status --json output (which now includes a budgeting section) and the planned experiment slate. Suggested Workflow \u00b6 Capture runtime status: quartumse runtime-status \\ --backend ibm:ibmq_jakarta \\ --json \\ --shots-per-second 8 .3 \\ --batch-seconds 600 \\ --calibration-shots 1024 \\ > runtime_status.json Feed the JSON into experiments.shadows.common_utils.load_budgeting_summary (see helper below) to obtain standardized allocation notes. Transcribe the summary into the YAML template and record any manual adjustments or fallback decisions. YAML Checklist Template \u00b6 runtime_budget_review : collected_at : <ISO8601 timestamp from payload.collected_at> backend : <payload.queue.backend_name> queue : pending_jobs : <payload.queue.pending_jobs> operational : <payload.queue.operational> status_msg : <payload.queue.status_msg> runtime_quota : plan : <payload.quota.plan> limit_seconds : <payload.quota.limit_seconds> remaining_seconds : <payload.quota.remaining_seconds> refresh_date : <payload.quota.refresh_date> budgeting : assumptions : <payload.budgeting.assumptions> timing : <payload.budgeting.timing> shot_capacity : <payload.budgeting.shot_capacity> fallbacks : <payload.budgeting.fallbacks> shot_allocation : total_measurement_shots : <summary.total_measurement_shots> total_calibration_shots : <summary.total_calibration_shots> per_experiment : - name : <experiment label> allocated_shots : <derived via allocate_shots> notes : <batch ordering / grouping> batching_strategy : target_window_seconds : <payload.budgeting.assumptions.batch_seconds> estimated_batches : <payload.budgeting.timing.estimated_batches> queue_checkpoint : <time to re-query runtime-status> batching_notes : - [ ] Submitted circuits grouped to respect measurement shots envelope - [ ] Calibration reuse verified (if measurement shots ~= payload.budgeting.shot_capacity.estimated_batch_shots) fallback_plan : - trigger : <condition from payload.budgeting.fallbacks> action : <planned mitigation> owner : <on-call engineer> - trigger : \"Runtime quota under 10%\" action : \"Trim measurement shots and re-run allocate_shots for high-priority experiments only\" Markdown Checklist Variant \u00b6 For teams preferring Markdown checklists, adapt the YAML above into the following structure: - [ ] Runtime status captured ( `runtime_status.json` attached) - [ ] Queue depth reviewed (pending: <payload.queue.pending_jobs>) - [ ] Remaining seconds mapped to measurement shots (<payload.budgeting.shot_capacity.measurement_shots_available>) - [ ] Shots allocated via `allocate_shots(total_shots, n_experiments)` - [ ] Batching window (<payload.budgeting.assumptions.batch_seconds> s) confirmed - [ ] Fallback scenarios acknowledged: - <condition>: <action> Keep the JSON artifact with the filled checklist so downstream analyses can reference the same budgeting envelope.","title":"Runtime Budgeting Checklist Template"},{"location":"strategy/runtime_budgeting_checklist/#runtime-budgeting-checklist-template","text":"Use this checklist before launching any IBM Quantum workload. Populate it with quartumse runtime-status --json output (which now includes a budgeting section) and the planned experiment slate.","title":"Runtime Budgeting Checklist Template"},{"location":"strategy/runtime_budgeting_checklist/#suggested-workflow","text":"Capture runtime status: quartumse runtime-status \\ --backend ibm:ibmq_jakarta \\ --json \\ --shots-per-second 8 .3 \\ --batch-seconds 600 \\ --calibration-shots 1024 \\ > runtime_status.json Feed the JSON into experiments.shadows.common_utils.load_budgeting_summary (see helper below) to obtain standardized allocation notes. Transcribe the summary into the YAML template and record any manual adjustments or fallback decisions.","title":"Suggested Workflow"},{"location":"strategy/runtime_budgeting_checklist/#yaml-checklist-template","text":"runtime_budget_review : collected_at : <ISO8601 timestamp from payload.collected_at> backend : <payload.queue.backend_name> queue : pending_jobs : <payload.queue.pending_jobs> operational : <payload.queue.operational> status_msg : <payload.queue.status_msg> runtime_quota : plan : <payload.quota.plan> limit_seconds : <payload.quota.limit_seconds> remaining_seconds : <payload.quota.remaining_seconds> refresh_date : <payload.quota.refresh_date> budgeting : assumptions : <payload.budgeting.assumptions> timing : <payload.budgeting.timing> shot_capacity : <payload.budgeting.shot_capacity> fallbacks : <payload.budgeting.fallbacks> shot_allocation : total_measurement_shots : <summary.total_measurement_shots> total_calibration_shots : <summary.total_calibration_shots> per_experiment : - name : <experiment label> allocated_shots : <derived via allocate_shots> notes : <batch ordering / grouping> batching_strategy : target_window_seconds : <payload.budgeting.assumptions.batch_seconds> estimated_batches : <payload.budgeting.timing.estimated_batches> queue_checkpoint : <time to re-query runtime-status> batching_notes : - [ ] Submitted circuits grouped to respect measurement shots envelope - [ ] Calibration reuse verified (if measurement shots ~= payload.budgeting.shot_capacity.estimated_batch_shots) fallback_plan : - trigger : <condition from payload.budgeting.fallbacks> action : <planned mitigation> owner : <on-call engineer> - trigger : \"Runtime quota under 10%\" action : \"Trim measurement shots and re-run allocate_shots for high-priority experiments only\"","title":"YAML Checklist Template"},{"location":"strategy/runtime_budgeting_checklist/#markdown-checklist-variant","text":"For teams preferring Markdown checklists, adapt the YAML above into the following structure: - [ ] Runtime status captured ( `runtime_status.json` attached) - [ ] Queue depth reviewed (pending: <payload.queue.pending_jobs>) - [ ] Remaining seconds mapped to measurement shots (<payload.budgeting.shot_capacity.measurement_shots_available>) - [ ] Shots allocated via `allocate_shots(total_shots, n_experiments)` - [ ] Batching window (<payload.budgeting.assumptions.batch_seconds> s) confirmed - [ ] Fallback scenarios acknowledged: - <condition>: <action> Keep the JSON artifact with the filled checklist so downstream analyses can reference the same budgeting envelope.","title":"Markdown Checklist Variant"},{"location":"tutorials/hardware_quickstart/","text":"Hardware Quickstart (IBM Runtime) \u00b6 Prereqs: pip install quartumse[mitigation] qiskit-ibm-runtime , IBM Quantum account, export QISKIT_IBM_TOKEN=... . 1) Verify credentials and quota \u00b6 quartumse runtime-status --backend ibm:ibmq_qasm_simulator 2) Run S-T01 on IBM simulator \u00b6 python experiments/shadows/S_T01_ghz_baseline.py --backend ibm:ibmq_qasm_simulator 3) Enable MEM (v1) and compare \u00b6 python experiments/shadows/S_T01_ghz_baseline.py --backend ibm:ibmq_qasm_simulator --variant st02 4) Outputs \u00b6 Manifests under data/manifests/ , shots under data/shots/ . See the Manifest Schema .","title":"Hardware Quickstart"},{"location":"tutorials/hardware_quickstart/#hardware-quickstart-ibm-runtime","text":"Prereqs: pip install quartumse[mitigation] qiskit-ibm-runtime , IBM Quantum account, export QISKIT_IBM_TOKEN=... .","title":"Hardware Quickstart (IBM Runtime)"},{"location":"tutorials/hardware_quickstart/#1-verify-credentials-and-quota","text":"quartumse runtime-status --backend ibm:ibmq_qasm_simulator","title":"1) Verify credentials and quota"},{"location":"tutorials/hardware_quickstart/#2-run-s-t01-on-ibm-simulator","text":"python experiments/shadows/S_T01_ghz_baseline.py --backend ibm:ibmq_qasm_simulator","title":"2) Run S-T01 on IBM simulator"},{"location":"tutorials/hardware_quickstart/#3-enable-mem-v1-and-compare","text":"python experiments/shadows/S_T01_ghz_baseline.py --backend ibm:ibmq_qasm_simulator --variant st02","title":"3) Enable MEM (v1) and compare"},{"location":"tutorials/hardware_quickstart/#4-outputs","text":"Manifests under data/manifests/ , shots under data/shots/ . See the Manifest Schema .","title":"4) Outputs"},{"location":"tutorials/quickstart/","text":"Quickstart \u00b6 This guide walks through setting up a local development environment, running core verification checks, and executing your first experiment. It merges the previous INSTALL_GUIDE.md and SETUP.md content into a single reference. Prerequisites \u00b6 Python 3.10+ (3.11 recommended) Git for cloning the repository Virtual environment tooling such as venv , conda , or pipenv (Optional) Jupyter for running the demo notebooks 1. Clone the repository & create an environment \u00b6 Unix/macOS: # Clone repository git clone https://github.com/quartumse/quartumse.git cd quartumse # Create virtual environment (replace with your preferred workflow) python -m venv .venv # Activate the environment source .venv/bin/activate Windows (PowerShell): # Clone repository git clone https :// github . com / quartumse / quartumse . git cd quartumse # Create virtual environment python -m venv . venv # Activate the environment . venv \\ Scripts \\ activate Windows (Command Prompt): rem Clone repository git clone https://github.com/quartumse/quartumse.git cd quartumse rem Create virtual environment python -m venv .venv rem Activate the environment .venv\\Scripts\\activate.bat If you use Conda or another environment manager, create an equivalent environment targeting Python 3.10\u20133.12. 2. Install QuartumSE \u00b6 Choose the extra set that matches your workflow: Unix/macOS: # Core SDK only pip install -e . # Core SDK + development tooling (pytest, black, ruff, mypy, jupyter) pip install -e \".[dev]\" # With optional mitigation / chemistry dependencies (Python < 3.13) pip install -e \".[dev,mitigation,chemistry]\" Windows: # Core SDK only pip install -e . # Core SDK + development tooling (pytest, black, ruff, mypy, jupyter) pip install -e \".[dev]\" # With optional mitigation / chemistry dependencies (Python < 3.13) pip install -e \".[dev,mitigation,chemistry]\" Upgrading pip before installation often avoids wheel build issues: Unix/macOS: python -m pip install --upgrade pip Windows: python -m pip install - -upgrade pip 3. Verify the installation \u00b6 Run the basic smoke checks to make sure the package imports and the test suite passes on simulators: Unix/macOS: # Confirm the CLI can be invoked quartumse --help # Quick import verification python -c \"from quartumse import ShadowEstimator; print('QuartumSE ready')\" # Run unit tests (skipping slow hardware checks) pytest tests -m \"not slow and not hardware\" -v Windows: # Confirm the CLI can be invoked quartumse - -help # Quick import verification python -c \"from quartumse import ShadowEstimator; print('QuartumSE ready')\" # Run unit tests (skipping slow hardware checks) pytest tests -m \"not slow and not hardware\" -v Install pre-commit hooks to keep formatting and linting consistent: Unix/macOS: pre-commit install Windows: pre-commit install 4. Launch the quickstart notebook (optional) \u00b6 Unix/macOS: # Ensure Jupyter is installed (included in the [dev] extras) pip install jupyter # Start Jupyter Notebook or Lab jupyter notebook # or jupyter lab Windows: # Ensure Jupyter is installed (included in the [dev] extras) pip install jupyter # Start Jupyter Notebook or Lab jupyter notebook # or jupyter lab Open notebooks/quickstart_shot_persistence.ipynb and choose Run All . The notebook walks through: Preparing a 3-qubit GHZ state. Estimating multiple observables with classical shadows. Inspecting the saved Parquet shot data and JSON provenance manifest. Replaying the experiment to calculate new observables without re-running the circuit. Troubleshooting tips: ModuleNotFoundError: quartumse \u2192 ensure the environment is activated and pip install -e . succeeded. Browser does not open automatically \u2192 copy the Jupyter server URL from the terminal into your browser. Kernel crashes mid-run \u2192 restart the kernel and choose Run All again. 5. Run the S\u2011T01 GHZ baseline experiment \u00b6 The CLI script demonstrates the same workflow outside notebooks and produces provenance artifacts in data/ : Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator Key outputs: Observable estimates with 95% confidence intervals. Shot-savings ratio (SSR) versus direct measurement baselines. Manifest & shot files saved under data/manifests/ and data/shots/ . Supply an IBM Quantum backend descriptor (e.g., ibm:ibmq_qasm_simulator ) to run against managed hardware or the cloud simulator once credentials are configured. 6. Next steps \u00b6 Review Testing for guidance on slow, integration, and hardware test markers. Explore the Manifest Schema reference to understand the manifest and Parquet schemas. Consult the Runtime runbook when planning IBM hardware executions. Study the strategic context in the Project Bible and Roadmap .","title":"Quickstart"},{"location":"tutorials/quickstart/#quickstart","text":"This guide walks through setting up a local development environment, running core verification checks, and executing your first experiment. It merges the previous INSTALL_GUIDE.md and SETUP.md content into a single reference.","title":"Quickstart"},{"location":"tutorials/quickstart/#prerequisites","text":"Python 3.10+ (3.11 recommended) Git for cloning the repository Virtual environment tooling such as venv , conda , or pipenv (Optional) Jupyter for running the demo notebooks","title":"Prerequisites"},{"location":"tutorials/quickstart/#1-clone-the-repository-create-an-environment","text":"Unix/macOS: # Clone repository git clone https://github.com/quartumse/quartumse.git cd quartumse # Create virtual environment (replace with your preferred workflow) python -m venv .venv # Activate the environment source .venv/bin/activate Windows (PowerShell): # Clone repository git clone https :// github . com / quartumse / quartumse . git cd quartumse # Create virtual environment python -m venv . venv # Activate the environment . venv \\ Scripts \\ activate Windows (Command Prompt): rem Clone repository git clone https://github.com/quartumse/quartumse.git cd quartumse rem Create virtual environment python -m venv .venv rem Activate the environment .venv\\Scripts\\activate.bat If you use Conda or another environment manager, create an equivalent environment targeting Python 3.10\u20133.12.","title":"1. Clone the repository &amp; create an environment"},{"location":"tutorials/quickstart/#2-install-quartumse","text":"Choose the extra set that matches your workflow: Unix/macOS: # Core SDK only pip install -e . # Core SDK + development tooling (pytest, black, ruff, mypy, jupyter) pip install -e \".[dev]\" # With optional mitigation / chemistry dependencies (Python < 3.13) pip install -e \".[dev,mitigation,chemistry]\" Windows: # Core SDK only pip install -e . # Core SDK + development tooling (pytest, black, ruff, mypy, jupyter) pip install -e \".[dev]\" # With optional mitigation / chemistry dependencies (Python < 3.13) pip install -e \".[dev,mitigation,chemistry]\" Upgrading pip before installation often avoids wheel build issues: Unix/macOS: python -m pip install --upgrade pip Windows: python -m pip install - -upgrade pip","title":"2. Install QuartumSE"},{"location":"tutorials/quickstart/#3-verify-the-installation","text":"Run the basic smoke checks to make sure the package imports and the test suite passes on simulators: Unix/macOS: # Confirm the CLI can be invoked quartumse --help # Quick import verification python -c \"from quartumse import ShadowEstimator; print('QuartumSE ready')\" # Run unit tests (skipping slow hardware checks) pytest tests -m \"not slow and not hardware\" -v Windows: # Confirm the CLI can be invoked quartumse - -help # Quick import verification python -c \"from quartumse import ShadowEstimator; print('QuartumSE ready')\" # Run unit tests (skipping slow hardware checks) pytest tests -m \"not slow and not hardware\" -v Install pre-commit hooks to keep formatting and linting consistent: Unix/macOS: pre-commit install Windows: pre-commit install","title":"3. Verify the installation"},{"location":"tutorials/quickstart/#4-launch-the-quickstart-notebook-optional","text":"Unix/macOS: # Ensure Jupyter is installed (included in the [dev] extras) pip install jupyter # Start Jupyter Notebook or Lab jupyter notebook # or jupyter lab Windows: # Ensure Jupyter is installed (included in the [dev] extras) pip install jupyter # Start Jupyter Notebook or Lab jupyter notebook # or jupyter lab Open notebooks/quickstart_shot_persistence.ipynb and choose Run All . The notebook walks through: Preparing a 3-qubit GHZ state. Estimating multiple observables with classical shadows. Inspecting the saved Parquet shot data and JSON provenance manifest. Replaying the experiment to calculate new observables without re-running the circuit. Troubleshooting tips: ModuleNotFoundError: quartumse \u2192 ensure the environment is activated and pip install -e . succeeded. Browser does not open automatically \u2192 copy the Jupyter server URL from the terminal into your browser. Kernel crashes mid-run \u2192 restart the kernel and choose Run All again.","title":"4. Launch the quickstart notebook (optional)"},{"location":"tutorials/quickstart/#5-run-the-st01-ghz-baseline-experiment","text":"The CLI script demonstrates the same workflow outside notebooks and produces provenance artifacts in data/ : Unix/macOS: python experiments/shadows/S_T01_ghz_baseline.py --backend aer_simulator Windows: python experiments / shadows / S_T01_ghz_baseline . py - -backend aer_simulator Key outputs: Observable estimates with 95% confidence intervals. Shot-savings ratio (SSR) versus direct measurement baselines. Manifest & shot files saved under data/manifests/ and data/shots/ . Supply an IBM Quantum backend descriptor (e.g., ibm:ibmq_qasm_simulator ) to run against managed hardware or the cloud simulator once credentials are configured.","title":"5. Run the S\u2011T01 GHZ baseline experiment"},{"location":"tutorials/quickstart/#6-next-steps","text":"Review Testing for guidance on slow, integration, and hardware test markers. Explore the Manifest Schema reference to understand the manifest and Parquet schemas. Consult the Runtime runbook when planning IBM hardware executions. Study the strategic context in the Project Bible and Roadmap .","title":"6. Next steps"}]}